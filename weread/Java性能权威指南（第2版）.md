## Java性能权威指南（第2版）

 **斯科特·奥克斯**


### 前言

* 这是典型的应用程序发展历程（JVM本身也是应用程序）：在项目初期，很容易找到架构上的改进点（或代码缺陷），改进后会带来巨大的性能提升。在成熟的应用程序中，很少能找到这样的性能改进点。

* 平台的进化带来了一个值得注意的性能关注点：在两个程序之间用JSON交换信息，毫无疑问比使用高度优化的专有协议更加简洁和高效。为开发人员节省时间确实可以提升生产力，但是确保在生产力提升的同时性能也提升（至少打平）才是真正的目标。


### 第1章 导论

* 这些知识分为两大部分。第一部分关于Java虚拟机（Java Virtual Machine，JVM）的性能优化，即如何通过JVM的配置方式影响程序的性能。使用其他语言的资深开发人员可能认为JVM优化有点令人厌烦，但实际上，JVM优化类似于C++程序员在编译阶段测试和挑选编译器参数，或者PHP程序员在php.ini文件中设置合适的变量。
第二部分关于理解Java平台的特性如何影响性能。注意，这里使用了平台这个词：有些平台特性（比如线程和同步）是Java语言的一部分，而有些平台特性（比如字符串处理）是Java标准API的一部分。尽管Java语言和Java API有着显著的区别，但在本书中，两者会被同等对待。这两方面在本书中都会讲到。


### 1.2 平台和约定

* 除了少数例外，JVM接收两种标志：布尔标志和附带参数的标志。
布尔标志使用的语法是：-XX:+FlagName表示开启，-XX:-FlagName表示关闭。
附带参数的标志使用的语法是：-XX:FlagName=something，表示设置FlagName的值为something。其中，something指表示任意值的符号。例如，-XX:NewRatio=N表示NewRatio标志可以设成任意值N（N的含义将是讨论的重点）。

* 所以，如果4核机器开启超线程，似乎就可以同时执行8个线程的指令（尽管从技术上讲，每个CPU周期只能执行4条指令）。这对于操作系统，也就是Java和其他应用程序来说，机器可以表现得像拥有8个CPU一样。但从性能的角度来讲，这些CPU并不等价。如果运行一个CPU密集型任务，它只会使用1个核心；运行两个CPU密集型任务才会用到第2个核心；以此类推，直到第4个核心。也就是说，独立运行4个CPU密集型任务可以得到4倍的吞吐量。
如果添加第5个任务，它只能在任何其他任务暂停时运行。该任务的运行时间平均占总时间的20%到40%。额外的任务都面临这一挑战。因此，添加第5个任务只能提高约30%的性能。结论就是，使用超线程技术得到的8个CPU能使机器性能达到单核的五六倍。
你将在后文的几个章节中看到这个例子。垃圾回收很大程度上是CPU密集型任务，所以第5章介绍了超线程如何影响垃圾回收算法的并行化。第9章大致讨论了如何充分利用Java的多线程能力，你也会看到扩展超线程核心的例子。

* 默认情况下，Docker容器可以自由使用机器上的所有资源，包括所有可用的CPU和内存。如果使用Docker仅仅是为了在机器上快速部署单一应用程序，并且机器只运行这一个Docker容器，那没问题。但其实经常需要在机器上运行多个Docker容器，还要限制每个容器的资源。鉴于我们的4核机器有16 GB内存，我们可能希望运行两个Docker容器，每个可获取2个核心和8 GB内存。

* 在Java 8的早期版本中，JVM无法得知任何容器的限制：当检测机器剩余内存以确定默认的堆大小时，它会检测机器的所有内存，但我们更希望看到的是允许Docker容器使用的内存。同样，检查有多少CPU可以用来优化垃圾回收器时，它会检查机器上所有的CPU，而不是只检查分配给Docker容器的CPU。结果是，JVM运行状态不理想：启用的线程过多，设置的堆过大。线程过多会导致性能下降，但真正的问题出在内存上：堆的最大大小比分配给Docker容器的内存还大，当堆增大到这个大小时，Docker容器（以及JVM）会停止运行。


### 1.3 全面的性能

* 归根结底，程序的性能取决于程序怎么写。如果使用循环遍历数组中的所有元素，那么JVM可以优化数组的边界检查方式，让循环运行得更快，还可以展开循环操作以提供额外的加速。如果使用循环是为了查找某个特定的元素，那么世界上还没有可行的优化方式，能让基于数组的代码和使用哈希映射（hash map）一样快。
好的算法对于提升性能是至关重要的。

* 我们最终会输掉这场战争
一个反常（令人沮丧）的地方是，每个应用程序的性能都会随着时间下降，准确地说是随着应用程序新版本的发布而下降。这种性能差异常常被忽略，因为硬件的改进可以保证新应用程序的运行速度。

* 高德纳被认为是最早创造过早优化这个词的人。开发人员经常使用这个词来宣称，代码的性能重要不重要，得运行了才知道。也许你从不知道，完整的原话是这么说的：“大部分时间里，比如在97%的时间里，我们不应该对细枝末节进行优化。过早优化是万恶之源。”3

* 如果你在开发不依赖外部资源的独立Java应用程序，那么该应用程序本身的性能几乎是最重要的。一旦外部资源（例如数据库）添加进来，那两者的性能就都很重要了。在分布式环境中，如果有Java REST服务器、负载均衡器、数据库和后端企业信息系统，那么Java服务器的性能可能是最不重要的部分。


* 另外，不要忽视最初的分析。如果数据库是瓶颈（提示：它的确是），对访问数据库的Java应用程序进行优化不会提升整体性能。实际上，这有可能适得其反。一个通用的准则是，给已经过载的系统增加负载，系统的性能会变差。如果在Java应用程序中做了一些更改让它更高效，这也会给过载的数据库增加负载，所以实际上整体性能是下降的。若你就此得出“不应使用某项JVM改进”的结论，那就危险了。


### 2.1 测试真实的应用程序

* 在编写多线程微基准测试时要相当警惕。当多个线程同时执行一小段代码时，发生同步瓶颈（和其他线程问题）的可能性相当大。多线程微基准测试的结果往往会导致需要花费大量时间在优化同步瓶颈上，而不是在解决更紧迫的性能需求上，但同步瓶颈很少出现在实际代码中。

* 到目前为止，我们关注的问题可以通过认真编写微基准测试来解决。代码被纳入到一个更大的系统中后，其他因素也会影响代码运行的最终结果。编译器利用代码的性能分析反馈（profile feedback）决定编译方法时的最佳优化方式。性能分析反馈基于方法被频繁调用、调用栈的深度、参数的实际类型（包括子类）等因素，而这些都是由代码的实际运行环境决定的。


* 尽管有很多缺陷，但微基准测试还是很受欢迎，以至于OpenJDK有一个专门用来开发微基准测试的核心框架：Java微基准测试工具（jmh，Java Microbenchmark Harness）。jmh被JDK开发人员用来构建针对JDK本身的回归测试，同时为开发人员提供了通用的基准测试框架。

* Java的一个性能特点是，代码执行得越多，性能就越好（这个话题会在第4章讲到）。因此，微基准测试必须包含一个预热期，让编译器有机会生成最佳代码。
本章稍后会深入讨论预热期的优点。微基准测试必须有预热期，否则它测量的就是编译性能，而不是代码性能。

* 需要测试完整应用程序的另一个原因是资源分配。在理想的情况下，有足够的时间优化应用程序中的每一行代码。在现实世界里，交付日期迫在眉睫，仅仅优化复杂运行环境的一部分可能不会有立竿见影的效果。

* 在这个例子中，优化数据业务处理所花费的时间并没有被完全浪费。一旦优化了其他的系统瓶颈，性能收益就会显现。准确地说，这是优先级的问题。在没测试整个应用程序的时候，不可能知道优化哪部分的性能会有效果。


* 尽管如此，介基准测试仍不完美。如果使用这种基准测试来比较两台应用程序服务器的性能，那么开发人员很容易误入歧途。


### 2.2 理解吞吐量、批处理时间和响应时间

* 对于静态编译语言，这种测试很简单：编写应用程序，并测量其执行时间。不过，Java世界里的即时编译会带来额外的问题（这个过程在第4章详述）。实际上，这个过程需要几秒到几分钟（甚至更长）的时间将代码充分优化并以最佳性能运行。基于这个（和其他）原因，Java性能研究很注重预热期，其性能的测量通常在相关代码运行足够长的时间、被编译和优化之后。

* 在客户端/服务器测试中，客户端的配置很重要，因为需要保证客户端向服务器发送数据的速度足够快。速度不够快的原因可能是客户端的机器没有足够的CPU周期来运行预期数量的线程，或者客户端在发送新的请求之前需要大量时间来处理请求。在这些情况下，测试实际上是在测量客户端的性能，而不是服务器的性能，这通常不是最初的目标。

* 吞吐量测试常常也会报告请求的平均响应时间。这是一条有趣的信息，只有在所报告的吞吐量相同时，平均响应时间的变化才表示性能有问题。举例来说，以0.5秒的响应时间承载500 OPS的服务器，比以0.3秒的响应时间承载400 OPS的服务器性能好。吞吐量测试几乎都是在适当的预热期之后进行的，因为所需测量的吞吐量并非一成不变。

* 出现异常值的原因有很多，GC引入的暂停时间2会让Java程序更容易出现异常值。在性能测试中，通常关注第90百分位响应时间（或者第95百分位响应时间和第99百分位响应时间。在这里，第90百分位响应时间只是举例说明）。如果你只能测量一个数字，那么最好选择基于百分位响应时间的测试，因为减小这个数字将惠及大多数用户。不过，最好同时测量平均响应时间和至少一个百分位响应时间，这样你就不会错过异常值过大的情况了。

* 在这个例子中，有25个客户端（-c 25）向股票Servlet发送请求（请求股票代码SDO），每个请求的周期时间是1秒（-W 1000）。-r 300/300/60表示这个基准测试有5分钟（300秒）的预热期，接下来是5分钟的测量期和1分钟的减速期。测试结束后，fhb报告了该测试的OPS和各种响应时间。（因为这个例子涉及思考时间，所以响应时间就成了更重要的指标，OPS基本上会是常量。）


### 2.3 理解可变性

* 第三个原则，即理解可变性，是指理解测试结果如何随时间变化。即使处理完全相同的数据集，应用程序每次运行也都会产生不同的结果。这是因为，后台进程会影响应用程序的运行，而且网络在运行时或多或少会出现拥堵。好的基准测试不会每次都使用完全相同的数据集，而是会在测试过程中构建随机行为以模拟真实世界。这就产生了一个问题：当比较两次运行的结果时，差异是由性能造成的，还是由随机变化的测试造成的？这个问题可以通过多次运行测试后取平均值来解决。如果正在测试的代码有修改，那么可以再多运行几次测试，再次对结果取平均值，然后比较这两个平均值。这听起来很简单。不幸的是，事情并没有那么简单，因为很难判断什么时候是真正的性能倒退，什么时候是随机变化。在性能优化这一重要领域，科学引领了方向，艺术也在发挥作用。当比较基准测试结果的平均值时，我们不能绝对地肯定这些平均值的差异是由于真正的性能问题还是随机变化而导致的。最好的办法是假设“平均值是相同的”，然后判断这句话正确的概率。如果这句话错误的概率很高，我们就可以放心地认为平均值的差异是由于真正的性能问题（哪怕我们永远不能100%确定）。

* 像这样因代码改进而进行的测试叫作回归测试（regression testing）。在回归测试中，原本的代码叫作基线（baseline），新的代码叫作样本（specimen）。

* 统计数字及其语义表述正确表述T检验结果的方法是这样的：样本和基线有差异的概率为57%，差异的最佳估值是25%。一般这样表述结果：结果提升25%的置信度是57%。虽然这与前一种说法并不完全相同，也会让统计学家抓狂，但这种说法简洁且易于接受，也不是很离谱。概率分析总会涉及不确定性，准确的表述会让这种不确定性更容易理解。然而，特别是在一个根本问题已被透彻理解的领域，必然会出现语义上的简化。

* 回归测试很重要，但它并不是非黑即白的二元选择。如果不做一些统计分析来理解这些数字的含义，就不能对这一系列数字（或它们的平均值）进行比较判断。但是，概率本身的含义是，即使做了这种分析，也不能得出一个完全确定的答案。性能优化工程师的工作就是查看数据、理解概率，并根据所有可用的数据决定应该先优化哪个问题。


### 2.4 早测试、常测试

* 在测试运行之前，必须通过自动化确保机器处于合适的状态。必须检查并确定没有额外的进程在运行，并且操作系统配置正确。只有当每次的运行环境都一样时，性能测试才是可重复的。自动化必须注意这一点。

* 这些数据可用于分析任何未发现的性能倒退。如果CPU使用率增加了，就需要查阅性能分析信息，看看是什么耗费了更多时间。如果花在GC上的时间增加了，就需要查阅堆性能分析信息，看看是什么占用了更多内存。如果CPU使用率和GC时间都减少了，那么可能是某个地方的资源竞争降低了性能，比如，栈数据有助于找出特定同步瓶颈的原因（见第9章），JFR记录有助于找出应用程序延迟的原因，数据库日志有助于找出导致数据库资源竞争加剧的原因。在查找性能倒退的原因时，需要像侦探一样，可用的数据越多，能推断出的线索就越多。正如第1章所讨论的那样，性能倒退不一定是JVM导致的。测量一切，确保分析结果的正确性。

* 因此，如果不在预期的负载下和预期的硬件上测试，就永远无法完全知道特定生产环境的性能。通过在较小的硬件上运行较小的测试，可以得出近似值以便做出推断。再加上，在现实世界中，复制用于测试的生产环境是相当困难或相当昂贵的。所以，推断只是简单的预测，就算是在最好的情况下，预测也可能是错的。大规模系统不仅仅是各个部分的总和，并且没有什么能替代在目标平台上执行充分的负载测试。


### 2.5 基准测试示例

* 设置代码与低级别测试
后面章节的一些jmh测试会报告特定的操作只相差几纳秒。这些测试并不是在测试单个操作后报告纳秒级时间，而是进行大量操作后报告统计方差内的平均值。jmh会为我们管理这些。
请务必注意：如果每次调用都会执行setup代码，那么jmh很难执行平均调用分析。因为这个（和其他）原因，建议尽量少使用Level.Invocation注解，最好只在测试方法本身耗时较长的情况下使用。

* 至于在实际情况中需要多少次迭代、多少次派生试验、多长的执行时间，才能获取足够的数据让结果这样清晰，并没有硬性规定。如果你比较的两个技术之间差别不大，那么你会需要更多的迭代和试验。另外，如果它们真的很相似，那么最好去看看其他对性能影响更大的方面。这又是一个艺术影响科学的例子。某些时候，你必须自己决定边界在哪里。

* 通常让Java代码变得更好、更快的方法是写出更好的算法，但这个实现与任何Java调优实践或者Java编码实践无关。

* 这个类可以加入不同实现的历史Bean（立即初始化、延迟初始化等）。它会选择性地缓存从后端数据库（或者模拟的实体管理器）中检索到的数据。这些是处理企业级应用程序性能问题的常见选择（特别是在中间层缓存数据，有时可以给应用程序服务器带来巨大的性能优势）。书中的例子都考虑了这些需要权衡的因素。


### 第3章 Java性能工具箱

* 数以百计的工具可以提供Java应用程序的运行信息，但是研究所有的工具是不切实际的。最重要的工具大都是Java开发工具包（JDK）自带的。尽管还有其他的开源工具和商用工具，但为方便起见，本章主要关注JDK工具。


### 3.1 操作系统工具和分析

* 如果我在Linux桌面系统上运行vmstat 1，会得到如下的几行输出（每秒一行）

* 但是操作系统不会猜测你下一步要做什么，它默认会执行任何可以执行的应用程序，而不是让CPU空闲。

* 如果应用程序优化之后每个请求只需要400毫秒，那么总体的CPU使用率会降低至40%。这是唯一降低CPU使用率有意义的情况——流入系统的负载量固定且应用程序不受限于外部资源。同时，这些优化可以让系统承担更多的负载，最终提高CPU使用率。在微观层面，这种优化仍然只是在短时间（执行请求的400毫秒）内让CPU使用率达到100%——只是CPU峰值的维持时间太短，不足以让大多数工具显示为100%使用率。

* 在这个具体的例子中，应该增加线程池的大小。但是，不要仅仅因为有空闲的CPU可用，就认为应该增加线程池的大小以完成更多的工作。程序无法获得CPU周期，还有我们之前提到过的两个原因——锁或外部资源的瓶颈。在确定行动方案之前，了解程序为什么没有获得CPU很重要。（有关这一主题的更多详细信息，请参见第9章。）


* 如果要运行的线程数量多于可用的CPU，性能就会开始下降。总体来说，为确保性能，你需要让Windows系统的处理器队列长度等于0，或者让Unix系统的运行队列长度小于等于CPU数。这不是一条硬性规定，系统进程和其他进程会定期地暂时提高这个值，这不会对性能产生重大影响。但是如果运行队列在相当长的时间内过长，那就说明机器已经过载，你需要想办法减少机器当前的工作量（通过将任务移至其他机器或者优化代码）。

* 有些系统的基本I/O监视要比其他系统的好。以下是Linux系统上的iostat的部分输出

* 监控磁盘使用率的第二个目的——即使预计应用程序不会进行大量I/O操作——是监控系统是否在进行内存交换。计算机有固定的物理内存，但是它可以用更大的虚拟内存来运行一系列应用程序。应用程序会保留超出其需要的内存量，且通常只在一部分内存上操作。在这两种情况下，操作系统会将没有用到的内存部分保留在磁盘上，只在需要的时候将之转入物理内存。

* 系统工具也可以报告系统是否在交换内存。比如，vmstat的输出中有两列数据（si代表换入，so代表换出），可以警示我们系统是否正在进行内存交换。磁盘活动也说明内存交换可能正在发生。请密切注意这些信息，因为系统在交换内存时——将数据分页从主内存移入磁盘或者进行相反的过程——往往性能相当差。必须配置系统，使其永远不会发生内存交换。

* 幸好，许多开源工具和商用工具可以监控网络带宽。在Unix系统中，受欢迎的命令行工具是nicstat，它可以展示每个网络接口的流量概况，包括接口的使用程度

* 类似typeperf和netstat的工具会报告读写数据的总量，但是要计算网络使用率，你必须在获取带宽后用脚本进行计算。一定要记住，带宽的单位是位/秒（bps），而工具报告的一般是字节/秒（Bps）。1000兆位网络每秒处理125兆字节（125 MB）。在上述例子中，读的速率为225.7KBps，写的速率为176.2KBps，总计3.29Mbps。把它除以接口速率就可以得到约0.33%的使用率。所以nicstat或类似的工具并没有什么神奇之处，它们只是使用起来更方便。


* 网络无法维持100%的使用率。对于本地以太网，使用率持续超过40%就说明接口饱和了。如果是分组交换网络或者利用了不同传输介质的网络，那么最大持续使用率可能会不一样。请咨询网络架构师以确定合适的数值。这个数值和Java无关，Java只不过是利用了操作系统的网络参数和接口。


### 3.2 Java监控工具

* 在JVM所在的机器上，运行所有的这些工具都很容易。如果JVM运行在Docker容器中，非图形化工具（除了jconsole和jvisualvm）可以通过docker exec命令运行，也可以使用nsenter进入Docker容器。不管怎样，这两种情况都假设你在Docker镜像中安装了这些工具，这是值得推荐的做法。Docker镜像通常会被缩减到刚刚好可以运行应用程序的状态，因此可能只包含了JRE，但是在生产环境中，你迟早需要去查看应用程序，所以最好保证Docker镜像中有必要的工具（JDK中自带的）。

* JVM工具可以提供JVM进程的基本运行信息：启动后运行了多久、使用了哪些JVM标志和JVM系统属性等。

* JVM可以设置大量调优标志，其中许多标志是本书的重点。追踪这些标志及其默认值有点令人生畏，上面最后两个jcmd的例子在这方面很有用。command_line命令显示了命令行直接设定的标志，flags命令显示了命令行设置的标志和JVM直接设置的一些标志（因为它们的值可以自动测出来），加上-all命令可以列出所有的标志。

* 这些命令的标志数据会以上述例子中的两种方式显示。第一行输出中的冒号表示标志使用了非默认值。这可能是以下原因导致的：
·标志的值直接通过命令行设定；
·其他标志间接地改变了这个标志的值；
·JVM自动计算出了默认值。

* jinfo本身并没有显示标志是否可操纵（manageable），但是可操纵（正如PrintFlagsFinal输出中所展示的那样）的标志可以通过jinfo打开或者关闭。

* 信息太多？
PrintFlagsFinal命令会输出几百个可用的JVM调优标志（例如JDK 8u202有729个可用的标志）。这些标志中的绝大多数是为了支持工程师从正在运行（运行异常）的应用程序中收集更多的信息。当你知道一个叫作AllocatePrefetchLines的标志（默认值是3）后，你可能很想通过改变这个值让预读指令在特定处理器上运行得更好，但是这种随便的优化并不值得。如果没有令人信服的理由，那么任何一个标志都不应该更改。对AllocatePrefetchLines标志来说，需要了解应用程序的预读性能、运行应用程序的CPU的特点以及改变这个数字会对JVM代码本身有什么影响。

* 请注意，在JDK 8中，jinfo可以更改任一标志的值，但这并不意味着JVM会响应这个更改。比如，大多数影响GC算法行为的标志会在启动时决定垃圾回收器的行为方式，之后通过jinfo更改这些标志并不会改变JVM的行为，JVM会根据初始的算法继续执行。所以这个方法只对PrintFlagsFinal命令的输出结果中标记为manageable的标志起作用。在JDK 11中，jinfo会在你尝试修改不可变标志的值时报告错误。

* jconsole和jvisualvm都可以实时显示应用程序中正在运行的线程的数量。查看运行的线程栈很重要，可以用于判断线程是否阻塞。


### 3.3 性能分析工具

* 性能分析器是性能分析师工具箱中最重要的工具。Java有很多性能分析器，每个都各有优缺点。性能分析常常需要使用各种工具——特别是采样分析器。采样分析器往往会以不同的方式展示问题。因此，有的分析器在某些应用程序上能很好地确定性能问题，而在其他应用程序上就不能。

* 这是最常见的采样误差，但绝不是唯一的一个。想要减小误差，就要延长采样周期并减小采样间隔。减小采样间隔和“最小化性能分析的影响”这个目标相反，我们需要在这二者之间寻找一个平衡点。不同的性能分析工具处理这个平衡点的方式不同，这就是各种工具报告的数据有很大不同的原因之一。

* Java 8为工具提供了获取栈轨迹的不同方式。（这是比较旧的工具有安全点偏差，而比较新的工具往往没有安全点偏差的原因之一，不过这需要新工具用新的机制重写。）在程序层面，这是通过AsyncGetCallTrace接口实现的。使用了这个接口的性能分析器往往被称为异步分析器（async profiler）。这里的异步是指JVM提供栈信息的方式，而不是指性能分析工具的工作方式。之所以被称为异步，是因为JVM可以在任何时间点提供栈信息，而无须等待线程到达（同步）安全点。

* ·基于采样的分析器是最常见的性能分析器。
·因为采样分析器对性能的影响较低，所以其引入的测量失真也较小。
·以异步方式收集栈信息的采样分析器引入的测量失真更小。
·不同的采样分析器表现各不同，每一种都有适用的应用程序。

* 类似async-profiler和Oracle Developer Studio的工具，除了可以分析Java代码，还可以分析原生代码。


### 3.4 JFR

* 和其他监控工具一样，jmc可以对被监控的应用程序中任何可用的MBean（managed bean）进行JMX调用。

* 就像使用性能分析器一样，你应该自己判断大量线程阻塞在I/O是预期行为还是预示着性能问题。

* JFR最初是关闭的，开启它需要在应用程序的命令行中添加标志-XX:+FlightRecorder。这就开启了JFR功能，但是直到记录过程本身开启之后才会有记录。开启记录可以通过GUI或者命令行。


* ￼ 快速小结
·JFR可以尽最大可能查看JVM内部信息，因为它内建于JVM中。
·像所有工具一样，JFR给应用程序引入了一些系统开销。日常使用时，JFR能以较低的开销收集大量信息。
·JFR在性能分析中非常有用，在生产环境开启后也同样有用，因为可以检查影响性能的事件。


### 4.1 即时编译器：概览

* Java试图找到一个中间地带。Java应用程序需要编译，但不是针对特定CPU编译为特定的二进制文件，而是编译为一种中间的低级语言。这种语言（称为Java字节码）可以用java命令运行（和php命令运行解释型PHP脚本的方式一样）。这使得Java具有解释型语言的平台独立性。因为它执行的是理想化的二进制代码，所以java程序能够在代码执行时将其编译为平台相关的二进制代码。这种编译是在程序执行时发生的，所以它是“即时”的。

* 编译器可以进行的最重要的一种优化，涉及何时使用主内存中的值以及何时将值存储到寄存器中。


### 4.2 分层编译

* 曾经，JVM开发人员（和一些工具）有时将这两种编译器简称为C1（编译器1，client编译器）和C2（编译器2，server编译器）。现在，这些名称会比较贴切，因为客户端和服务器计算机之间的区别早已不复存在，所以我们之后将使用这些名称。

* 当这两种编译器不能兼容的时候，显而易见的问题是为什么需要做出选择：JVM不能从C1编译器开始，然后随着代码越来越热开始使用C2编译器吗？这种技术被称为分层编译，它是现在所有JVM都在使用的技术，可以用-XX:-TieredCompilation标志（默认值是true）关闭它。


### 4.3 常用的编译器标志

* 当JVM编译代码时，它会在代码缓存中保存一系列汇编语言指令。代码缓存的大小是固定的，一旦它被填满，JVM就不再编译代码了。
如果代码缓存太小，很容易就能看出潜在的问题。一些热点方法会被编译，另一些则不会被编译，应用程序最终会解释执行大量代码（非常慢）。


* 目前，没有一种很好的机制来计算一个特定应用程序需要多少代码缓存，所以当你需要增加代码缓存的大小时，恰好设置成所需的大小可能就得碰运气了。典型的选择是，简单地将其设置为默认值的2倍或者4倍。

* ·代码缓存是一种设定了最大值的系统资源，它影响JVM可以运行的编译后的代码总量。
·非常大的应用程序在默认配置下可能会用掉整个代码缓存，你可以监控代码缓存并在必要时增加其大小。

* 如果开启了PrintCompilation标志，每编译一个方法（或循环）后，JVM都会打印出关于刚刚编译了什么的信息。

* OSR编译
s
方法是同步的
!
方法有异常处理器
b
在阻塞模式下发生的编译
n
原生方法封装时发生的编译
这些属性中的第一个表示栈上替换（on-stack replacement，OSR）。即时编译是个异步的过程。当JVM认为某个方法应该被编译时，该方法就会被放在一个队列中。JVM会继续解释执行该方法，而不是等待编译完成。下一次调用该方法时，JVM才会执行它的编译版本（当然，假设编译已经完成）。

* 试想一个长期运行的循环。JVM会注意到该循环应该被编译，并将代码放入队列等待编译。但这还不够。JVM必须有能力在循环还在运行的时候就执行循环的编译版本——如果等循环和包含循环的方法运行完（这可能根本不会发生），效率会很低。因此，当循环的代码完成编译后，JVM会替换代码（在栈上），循环的下一次迭代会执行速度快得多的编译版本的代码。这就是OSR。

* 要想查看编译日志，需要程序以-XX:+PrintCompilation标志启动。如果程序启动时没有该标志，你可以通过jstat有限地了解编译器内部的工作情况。

* 正在编译时类被修改了。JVM稍后将再次编译，你会在之后的日志中看到方法被重新编译。
在所有情况下（除了缓存已满）编译都会重试。如果没有，一定是某个错误阻止了代码编译。这常常是因为编译器的bug。这种情况下，通常的补救措施是将其重构为编译器可以处理的更简单的代码。

* 这里循环的执行次数比构造方法本身还多，所以循环会经历OSR编译。请注意，这个方法的编译花了一些时间，它的编译ID是25，不过直到900范围内的其他方法编译了之后它才出现。[在阅读OSR相关的输出行（如本例）时，你可能将25和%放在一起，读成25%，还会想弄明白剩下的75%是什么。请记住这个数字是编译ID，而%表示OSR编译。]这就是典型的OSR编译，栈上替换比较难建立，不过在此期间其他编译可以继续进行

* 程序使用分层编译的时候，编译日志会输出每个方法在哪个层级被编译。在前面的样例输出中，代码的编译级别为3或4。目前我们讨论了两个编译器和一个解释器，其中C1编译器有3个级别，所有总共有5个编译级别：
0
解释代码
1
简单C1编译代码
2
受限C1编译代码
3
完全C1编译代码
4
C2编译代码
典型的编译日志显示，大多数方法第一次编译的级别是3：完全C1编译。（当然，所有方法都是从级别0开始的，只是在日志中不显示。）如果一个方法运行得很频繁，它就会到编译级别4（级别3的代码会被丢弃）。最常见的编译路径是：C1编译器得到代码是如何使用的信息之后，会利用这些信息进行优化，然后才开始编译。

* 在之前讨论PrintCompilation标志的输出时，我们提到了两种编译器会逆优化代码的情况。逆优化（deoptimization）意味着编译器不得不“撤销”之前的编译。结果就是应用程序的性能降低，至少到编译器重新编译相关代码为止。

* 第二个导致代码被丢弃的原因是分层编译的工作方式。当代码被C2编译器编译时，JVM必须替换已经被C1编译器编译的代码。它将旧代码标记为丢弃，并用逆优化机制替换为新编译的（更高效的）代码。因此，程序使用分层编译时，编译日志会显示大量方法被标记为丢弃。不要惊慌，这种“逆优化”实际上会让代码变得更快。

* 查看代码如何编译的最佳方式是开启PrintCompilation标志。
·开启PrintCompilation标志后的输出可以用来确保编译按照预期进行。
·分层编译可以在2个编译器的5个级别上进行。
·逆优化是指JVM替换之前编译的代码的过程。这通常发生在C2代码替换C1代码的情况下，但也可能因为应用程序的执行概况有变。


### 4.4 高级编译器标志

* 你经常可以看到更改CompileThreshold标志的建议。一些公开的Java基准测试使用了这个标志（经常被设置为8000），一些应用程序在发布时也默认使用了这个标志。

* 这一点有点反常：如果一个程序永远在执行，所有的代码最终都会被编译吗？事实并非如此。这是因为虽然编译器使用的计数器会随着方法和循环的执行增加计数，但是它们也会随着时间的推移而减少。每个计数器的值都会周期性地减少（特别是当JVM到达安全点时）。

* 实际上，这意味着计数器只是方法或循环最近热度的相对度量。这会产生一个副作用，那就是有些频繁执行的代码可能永远不会被C2编译器编译，即使这个程序永远运行。这些方法有时被形容为温热（与热相对）。在分层编译之前，这种情况下降低编译阈值是有益的。

* ·方法（或循环）的编译阈值可以通过调优标志设定。
·禁用分层编译时，调整这些阈值有时是有意义的，但是使用分层编译时，就不推荐使用这种优化了。

* 同样，如果有大量额外CPU周期可用，那么理论上，当编译器线程数量增加时，程序（至少在预热期）会受益。在现实生活中，很难获得这种好处。进一步来说，如果所有多余的CPU都是可用的，那么在整个应用程序执行期间，你最好尝试利用可用的CPU周期（而不仅仅是在开始时进行更快的编译）。

* 还有一个适用于编译线程的设置是-XX:+BackgroundCompilation标志，其默认值为true，这个设置意味着队列会像刚才解释的那样进行异步处理。这个标志可以设置为false，这种情况下，一旦方法可以被编译，想要执行它的代码就会等待，直到方法实际上完成编译（而不是继续在解释器中执行）。使用-Xbatch标志也可以禁用后台编译。

* 内联方法是编译器最重要的优化之一。良好地遵循面向对象设计的代码经常包含一些需要通过getter（也许还有setter）访问的属性

* 调用这种方法的系统开销是相当大的，尤其是相对于方法中的代码量而言。实际上，在Java的早期版本中，考虑到所有这些方法调用对性能的影响，一些性能优化技巧常常反对这种封装。幸好，如今的JVM已经可以对这种类型的方法进行常规的代码内联。

* 内联是默认启用的。可以通过-XX:-Inline标志禁用它，不过内联对性能提升非常重要，所以实际上你永远不会这么做（例如，禁用内联会使股票批处理测试的性能降低50%以上）。也正因为内联如此重要，而且还有其他许多调优标志，所以还会有关于优化JVM内联行为的建议。

* 有时你会看到建议说，应该增大MaxInlineSize标志的值，以便内联更多方法。这种关系常常被忽略的一个方面是，将MaxInlineSize的值设置为大于35字节，意味着一个方法在首次调用时可能就被内联了。但是，如果方法经常被调用，就说明它的性能重要得多，它总归是会被内联的（假设它小于325字节）。否则，优化MaxInlineSize标志的最终效果是，它可能会缩短测试所需的预热时间，但是不太可能对一个长期运行的应用程序产生很大影响。

* 如果开启逃逸分析（通过-XX:+DoEscapeAnalysis标志，默认值是true），那么C2编译器会进行大幅度的优化。

* 逃逸分析是编译器能够进行的最复杂的优化。这种优化经常导致微基准测试出错。

* JDK 9首次引入了对AVX-512指令的支持，不过默认情况下没有开启。对它的支持在开始时失败了几次，它先是被默认开启，然后又被默认禁用。例如，在JDK 11中，这些指令是默认开启的，但从JDK 11.0.6开始，这些指令再次被默认禁用。因此，即使在JDK 11中，对它的支持仍然是一项正在进行的工作。（顺便说一句，这并不是Java独有的，许多程序都在努力让AVX-512指令得到正确的支持。）

* 这个标志的默认值取决于运行JVM的处理器，JVM会检测CPU并选择它能支持的最高值。Java 8不支持级别3，所以你会看到大多数处理器上使用的值是2。在较新的Intel处理器上运行Java 11时，在11.0.5及之前的版本上默认值是3，在之后的版本上默认值是2。


### 4.5 分层编译的权衡

* javac编译器
在性能方面，编译实际上说的是JVM内置的即时编译。不过请回想一下，Java代码首先会被编译为字节码，这是通过javac进程发生的。所以我们在本节的最后，说几个关于它的要点。
关于javac编译器，最重要的一点是它完全不会影响性能（但有一个例外）。
·包含额外调试信息的-g标志不会影响性能。
·在Java应用程序中使用final关键字并不能更快地编译代码。
·用较新的javac版本重新编译通常不会使应用程序更快。
这三点是多年来沿用的建议，直到JDK 11出现。JDK 11引入了一种新的字符串连接方式，可以比以前的版本更快，但需要重新编译代码才能用。这是上面规则的一个例外。总的来说，你永远不需要通过重新编译字节码来使用新的特性。更多关于这方面的详细信息会在12.1节中给出。



### 4.6 GraalVM

* GraalVM对JVM性能有两个重要贡献。首先，插件技术允许GraalVM生成完全原生的二进制文件，我们将在下一节研究这个问题。
其次，GraalVM可以以常规JVM的模式运行，只是它包含了一个新的C2编译器实现。这个编译器是用Java写的（而传统的C2编译器是用C++写的）


### 4.7 预编译

* 提前编译（ahead-of-time compilation，简称AOT compilation）最初仅在JDK 9的Linux版本中可用，到JDK 11时所有平台都可以用了。从性能的角度来看，它仍然在发展过程中，

* 提前编译对于比较大的程序有好处，但是对于很小的、快速运行的程序没有帮助，甚至会阻碍它们的运行。这是因为它还是一个实验特性，也是因为它的架构需要让JVM加载共享库。

* GraalVM生成的二进制文件启动速度很快，特别是相较于在JVM中运行的程序。然而，在这种模式下，GraalVM优化代码时并没有C2编译器那么激进，所以对于运行得足够久的应用程序，传统的JVM最终会胜出。与提前编译不同，GraalVM原生二进制文件在执行期间不会使用C2编译器编译类。

* 同样，GraalVM生成的原生程序的内存占用在开始时比传统JVM少得多。然而，随着程序的运行和堆的增长，这种内存优势会逐渐消失。

* 编译成原生代码的程序，可以使用的Java特性仍然受到限制。这些限制包含以下几点：
·动态类加载（例如调用Class.forName()方法）
·Finalizer
·Java安全管理器（Java Security Manager）
·JMX和JVMTI（包括JVMTI性能分析）
·使用反射常常需要特殊编码或配置
·使用动态代理常常需要特殊配置
·使用JNI需要特殊编码或配置
所有这些都可以在GraalVM项目的演示程序中看到，这个演示程序会以递归的方式统计目录中的文件数。在只统计几个文件的情况下，GraalVM生成的原生程序相当小且相当快，但是随着工作量的增加和即时编译的介入，传统的JVM编译器会进行更好的代码优化，并且运行得更快，正


### 5.1 垃圾回收概览

* Java编程最吸引人的特性之一是开发人员不需要显式管理对象的生命周期：对象可以在被需要时创建，不再使用时由JVM自动回收。如果你像我一样，花了大量时间优化Java应用程序的内存使用，那么这个设计看起来像是一个缺点而不是特性（我在GC上花费的大量时间似乎也印证了这一点）。它必然是既有缺点也有优点的，我仍然记得在其他语言中跟踪空指针和悬空指针的艰难。我坚信，优化垃圾回收器比跟踪指针引起的bug要容易得多（且耗时更少）。

* 在垃圾回收器运行时，如果没有应用程序线程在运行，那么执行这些操作会比较简单。然而Java应用程序通常会运行大量线程，垃圾回收器也经常运行多个线程。这些线程从逻辑上可分成两组：一组执行应用程序逻辑，通常被称为mutator线程，因为它们作为应用程序逻辑的一部分会改变对象；另一组执行GC，当GC线程跟踪对象引用（用于回收对象）或者在内存中移动对象时，它们必须确保应用程序线程不使用这些对象。GC移动对象时尤其如此，操作过程中对象的内存位置变了，因此必须确保任何应用程序线程都不能再访问这些对象。所有线程都停止运行的停顿被称为STW停顿（stop-the-world pause）。这些停顿对应用程序性能的影响最大，尽量减少这些停顿是优化GC的重中之重。

* 常用的GC算法在新生代的回收过程中会有STW停顿。
随着对象被移动到老年代，老年代最终也会被填满，JVM需要在老年代中查找不再使用的对象并丢弃它们。这里体现了不同GC算法的最大不同之处。简单的算法会停止所有的应用程序线程，找到不使用的对象，回收它们的内存，然后压缩堆。这个过程被称为Full GC，它一般会造成应用程序线程较长时间的停顿。

* 考量哪种垃圾回收器适合你的情况时，需要考虑总体的性能目标。每种情况都需要权衡。比如在应用程序（如REST服务器）中，考量单个请求的响应时间时，需要考虑以下几点。

* ·单个请求会受停顿时间的影响，尤其是Full GC时较长时间的停顿。如果目标是尽量减少停顿对响应时间的影响，那么并发回收器可能更合适。


* ·如果平均响应时间比异常值（例如第90百分位响应时间）更重要，那么非并发回收器可能会产生更好的结果。
·并发回收器避免长时间停顿是以消耗额外的CPU周期为代价的。如果你的机器缺乏并发回收器所需的空闲CPU周期，那么非并发回收器是更好的选择。

* 同样，对于批处理应用程序，垃圾回收器的选择应遵循以下权衡原则。
·如果有足够的CPU可用，那么使用并发回收器来避免Full GC的停顿可以让任务执行得更快。
·如果CPU有限，那么并发回收器的额外CPU消耗会导致批处理任务花费更多的时间。

* Serial垃圾回收器是最简单的回收器。如果应用程序运行在客户端（运行32位JVM的Windows）机器上或者单处理器的机器上，那么它会是默认的回收器。曾经，Serial垃圾回收器似乎注定要被淘汰，但是容器化改变了这一点：只有一个核心（甚至是以两个CPU形式出现的超线程核心）的虚拟机和Docker容器让这个算法又有了使用的意义。

* G1 GC可以通过-XX:+UseG1GC标志开启。在大多数情况下，它是JDK 11的默认垃圾回收器。它在JDK 8中也能使用——特别是在JDK 8的后期版本中，因为这些版本已经从之后的版本中向后移植了很多重要的bug修复和性能增强。不过，在深入探索G1 GC时，你会发现，JDK 8中的G1 GC缺少了一个主要的性能特性，这个特性不适合该版本。

* CMS垃圾回收器自JDK 11被正式废弃，而且不鼓励在JDK 8中使用。从实际的角度来看，CMS的主要缺陷是它不能在后台处理过程中压缩堆。如果堆变得碎片化（这很可能在某些时候发生），那么CMS必须停止所有的应用程序线程并压缩堆，这就违背了使用并发垃圾回收器的初衷。因为这个原因和G1 GC的出现，CMS不再被推荐。

* 什么规则都有例外，尤其是在做性能监控或基准测试时。对于运行少量代码的小型基准测试，为了加快预热JVM，在测量期之前强制执行GC有一定的意义。（jmh也可以这样做，不过通常没有必要。）同样，对堆进行分析时，在堆转储之前强制触发Full GC通常也是个好主意。虽然大多数获取堆转储的技术会执行Full GC，但是你也可以通过其他方式强制执行Full GC，比如执行jcmd <进程ID> GC.run命令，或者用jconsole连接JVM并单击内存面板上的“执行GC”按钮。

* 另一个例外是远程方法调用（Remote Method Invocation，RMI）。作为分布式垃圾回收器的一部分，它每小时调用一次System.gc()方法。这个调用时间可以通过两个系统属性来更改：-Dsun.rmi.dgc.server.gcInterval=N和-Dsun.rmi.dgc.client.gcInterval=N。N以毫秒为单位，默认值是3 600 000（1小时）。
如果你运行的第三方代码错误地调用了System.gc()方法，那么可以通过在JVM参数中加上-XX:+DisableExplicitGC来阻止这些GC操作。默认情况下，这个标志的值为false。像Java EE服务器这样的应用程序常常包含这个参数，以防止RMI的GC调用干扰操作。

* 我们先从经验法则开始，G1 GC是更好的选择，但是规则都有例外。在垃圾回收的情况下，这些例外涉及应用程序相对于可用硬件所需要的CPU周期数以及G1 GC后台线程需要执行的处理量。如果你在使用JDK 8，那么G1 GC避免Full GC的能力也将是一个关键因素。如果G1 GC不是更好的选择，那在Throughput垃圾回收器和Serial垃圾回收器之间做选择就需要考虑机器的CPU数。


* 关于Serial垃圾回收器，还有另一点需要注意：堆很小（比如100 MB）的应用程序，无论可用的核心数量是多少，使用Serial垃圾回收器可能都会有更好的表现。

* 这实际上和单CPU时的情况没有什么不同。G1 GC后台线程和应用程序线程对CPU周期的竞争意味着，即使没有GC停顿发生，应用程序线程实际上也被暂停了。

* 只看测试期间的平均CPU使用率，会错过GC周期内发生的有趣之事。Throughput垃圾回收器在运行时（默认情况下）会消耗机器上100%的可用CPU

* 并发垃圾回收器中可能存在多个后台线程，但是效果是类似的。当这些后台线程运行时，它们会消耗CPU，并推高长期的平均CPU使用率。
监控系统中CPU使用率规则触发的情况很重要：你需要确保CPU告警不是由Full GC引起的100%CPU使用率造成的，也不是由后台并发处理线程引起的CPU高峰持续时间更长（但使用率更低）造成的。这两种情况对于Java应用程序来说是很正常的。
 


### 5.2 GC优化基础

* 和大多数性能问题一样，选择堆的大小是一个取舍问题。如果堆太小，那么应用程序会花费大量时间执行GC，也就没有足够的时间执行应用程序逻辑了。简单地设定一个非常大的堆也不是什么好办法。GC停顿的时间取决于堆的大小，随着堆的大小增加，停顿的持续时间也会增加。这样一来，停顿发生的频率确实会降低，但是停顿的持续时间会拖慢整体性能

* 在运行了很多应用程序的系统上，这个过程运行得很流畅，因为大多数应用程序不会在同一时间启用。对于Java应用程序来说，它的效果不是很好。如果这个系统上运行了一个堆大小为12 GB的Java应用程序，那么操作系统可能将堆处理成8 GB在RAM中，4 GB在磁盘中（这里做了一些简化，其他应用程序也是会用一部分RAM的）。JVM不会知道这一点，因为操作系统进行的交换对JVM是不公开的。因此，当JVM被告知它可以使用12 GB的内存之后，它会高兴地将其填满。这就会导致严重的性能问题，因为操作系统要将数据从磁盘交换到RAM（这是一个代价高昂的操作）。

* 因此，在调整堆的大小时，首要规则是设定堆的大小永远不要超过机器的物理内存——如果有多个JVM在运行，那么这适用于所有堆的总和。你也需要为JVM的原生内存以及其他应用程序留出一些内存空间。通常，常见的操作系统配置需要至少1 GB的空间。

* 堆有了初始值和最大值之后，JVM就可以根据工作量优化其行为。如果JVM发现堆在初始大小时，GC的次数太多，它就会不断地增加堆大小，直到JVM执行的GC数量“适当”，或者直到堆大小达到最大值。


* 请注意，即使你明确地设置了最大值，堆也会自动调整。启动时堆的大小为默认的初始值，JVM会增加堆的大小以满足GC算法的性能目标。设定一个比实际需要的更大的堆，并不一定会对内存不利，堆大小只会增长到足以满足GC的性能目标。
另外，如果你确切地知道应用程序需要多大的堆，那么不妨将初始值和最大值都设置为该值（例如-Xms4096m-Xmx4096m）。这会让GC稍微高效一点，因为它不再需要弄清楚是否应该调整堆大小。

* -XX:NewRatio=N
设置新生代与老年代的比例。
-XX:NewSize=N
设置新生代的初始值。
-XX:MaxNewSize=N
设置新生代的最大值。
-XmnN
将NewSize和MaxNewSize设置为同一个值的简单写法。

* 通过设定新生代的最小值和最大值来进行优化是十分困难的。如果堆的大小是固定的（通过设置-Xms等于-Xmx），通常最好也用-Xmn将新生代大小设为固定的。如果应用程序会动态调整堆大小，并且需要更大的（或更小的）新生代，那就重点设置NewRatio的值。


* 在全局范围内，自适应大小可以通过关闭-XX:-UseAdaptiveSizePolicy标志（默认值为true）来禁用。除了Survivor空间（第6章会详细研究），如果堆的最小值和最大值设置为相同的值，并且新生代的初始值和最大值也设为相同的值，那么自适应大小实际上就被关闭了。

* 要查看JVM是如何调整应用程序内空间大小的，需要开启-XX:+PrintAdaptiveSizePolicy标志。这样，执行GC时，GC日志里会包含回收过程中各个代是如何调整大小的详细信息。

* 当JVM加载类时，它必须记录这些类的某些元数据。这些数据占据了一个单独的堆空间，叫作元空间（metaspace）。在较旧的JVM中，这是由一个叫作永久代（permgen）的实现来处理的。
对于终端用户，元空间是不透明的。我们需要知道的是，它保存了一堆与类相关的数据，并且在某些情况下，需要调整这个区域的大小。

* 目前，并没有很好的方法来提前计算一个特定应用程序所需的元空间大小。元空间的大小与它所使用的类的数量成正比，所以更大的应用程序需要更大的区域。这是JDK技术提升让生活变得更轻松的另一个领域：优化永久代在过去相当普遍，而现在优化元空间相当罕见，主要原因是元空间大小的默认值非常宽裕。表5-10列出了默认的初始值和最大值。


* 元空间过大？
因为元空间的默认最大值是不受限制的，所以一个应用程序（特别是在32位JVM中）可能因为元空间被填满而耗尽内存。第8章讨论的原生内存跟踪（Native Memory Tracking，NMT）工具可以帮助诊断这种情况。如果元空间增长得太大，你可以将MaxMetaspaceSize的值设置得更低一些——但是应用程序的元空间被填满时，会抛出OutOfMemoryError异常。在这种情况下，找出类元数据过大的原因才是真正的补救方法。

* 堆转储（见第7章）可以用来诊断存在哪些类加载器，从而帮助判断是否存在类加载器泄漏，并且元空间即将被填满。除此之外，jmap可以和-clstats一起使用，以输出有关类加载器的信息。

* 因为这些GC操作会让所有的应用程序线程停止执行，所以JVM会尝试使用尽可能多的CPU资源，以尽量减少停顿时间。默认情况下，这表示JVM将在机器的每个CPU上都运行一个线程，最多同时运行8个，达到这个阈值后，JVM每1.6个CPU会再增加一个新线程。

* ·所有GC算法的基本线程数都是基于机器的CPU数量的。
·当多个JVM运行在同一台机器上时，基于文中公式计算出的线程数量会过高，实际情况中必须减少使用的线程数量


### 5.3 GC工具

* JDK 8提供了多种方式来开启GC日志。设定-verbose:gc和-XX:+PrintGC中的任意一个，都会创建简单的GC 日志（这两个标志是彼此的别名，日志默认情况下是关闭的）。-XX:+PrintGCDetails标志将创建包含更多信息的详细日志。推荐使用这个标志（默认情况下是false），因为只用简单的日志，往往很难诊断GC时到底发生了什么。


* 使用日志滚动可以限制GC日志中保存的数据量，这对于长期运行的服务器来说很有用，否则它的磁盘在几个月内就可能被日志填满。日志文件的滚动由这些标志控制：-XX:+UseGCLogFileRotation、-XX:NumberOfGCLogFiles=N和-XX:GCLogFileSize=N。默认情况下，UseGCLogFileRotation是关闭的。这个标志开启时，默认的文件数量是0（意味着无限多），默认的日志文件大小是0（意味着无限大）。因此，必须给以上标志设定值，才能让日志滚动按预期的方式工作。注意，如果给日志文件设置的大小不足8 KB，那么这个数值会向上取整。


* 将以上所有的标志放在一起，就构成了一组日志标志：
-Xloggc:gc.log -XX:+PrintGCTimeStamps -XX:+UseGCLogFileRotation￼ -XX:NumberOfGCLogFile=8 -XX:GCLogFileSize=8m
这组标志将用时间戳记录GC事件，以便与其他日志相关联，并将保留的日志限制在总计64 MB的8个文件中。这个日志记录非常小，甚至可以在生产系统上启用。

* 第一部分（gc*）设定了应该开启哪些模块的日志，即我们开启了所有GC模块的日志。有的标志可以只记录某个特定部分的日志（例如，gc+age将记录一个对象的晋升信息，这个主题将在第6章介绍）。这些特定的模块在默认日志级别的输出很有限。所以你可以使用类似于gc*,gc+age=debug的标志来记录所有来自gc模块的基本（info级别）信息，和debug级别的来自晋升代码的信息。通常情况下，将所有模块的日志记录在info级别就可以了。

* 有一件事需要注意：JDK 8和JDK 11对日志滚动的处理方式略有不同。假设我们设定日志文件名为gc.log，并且要保留3个文件。在JDK 8中，日志会这样写：
01. 开始时记录到gc.log.0.current；
02. 满了之后，将其重命名为gc.log.0，并开始记录到gc.log.1.current；
03. 满了之后，将其重命名为gc.log.1，并开始记录到gc.log.2.current；
04. 满了之后，将其重命名为gc.log.2，删除gc.log.0，并开始记录到新的gc.log.0.current；
05. 重复这个循环。
在JDK 11中，日志会这样写：
01. 开始时记录到gc.log；
02. 满了之后，将其重命名为gc.log.0，开始新的gc.log；
03. 满了之后，将其重命名为gc.log.1，开始新的gc.log；
04. 满了之后，将其重命名为gc.log.2，开始新的gc.log；
05. 满了之后，将其重命名为gc.log.0，删除旧的gc.log.0，开始新的gc.log。

* 如果你想知道为什么我们在之前的JDK 11命令中设定保留7个日志文件，这就是原因：这种情况下会有8个活跃文件。另外请注意，无论什么情况下，附加在文件名上的数字都不表示文件创建的顺序。数字是在一个循环内重复使用的，所以有一定的顺序，但最旧的日志文件可能是这组文件中的任何一个。

* 不幸的是，目前没有很多好的开源工具可以用来解析日志文件。像性能分析器一样，供应商已经开始提供支持，比如jClarity（Censum）和GCeasy。后者提供免费的基本日志解析服务。
可以使用jvisualvm或jconsole实时监控堆。jconsole的内存面板显示了堆的实时图表

* 对于脚本化的解决方案，jstat是首选的工具。它提供了9个选项，可以输出堆的不同信息。jstat -options可以提供完整的选项列表，其中一个有用的选项是-gcutil，它可以显示在GC上花费的时间，以及当前每个GC区域被填充的百分比。jstat的其他选项能够以KB为单位显示GC中各个区域的大小。

* 记住，jstat需要一个可选参数——重复命令的毫秒数——这样它可以随时间的推移，监控GC在一个应用程序中的效果。下面是每秒重复一次的样例输出


### 5.4 小结

* 垃圾回收器的性能是Java应用程序整体性能的一个关键因素。不过对于很多应用程序来说，唯一需要优化的是选择合适的GC算法，如果需要的话，增加应用程序的堆大小。自适应大小允许JVM自动优化其行为，使用给定的堆，提供更好的性能。
更复杂的应用程序将需要额外的调优，特别是针对特定的GC算法。如果本章中的简单GC设置不能提供应用程序所需的性能，请参考第6章介绍的优化内容。


### 6.1 理解Throughput回收器

* 垃圾回收器有3个必要的基本操作：找到不使用的对象、释放它们的内存，以及压缩堆。Throughput回收器可以在同一个GC周期内完成所有这些操作，这些操作合在一起被称为回收。这些回收器可以在单次操作过程中回收新生代或老年代。

* 开启了PrintGCDetails标志的JDK 8 GC日志中，Throughput回收器的一次Minor GC是这样的：
17.806: [GC (Allocation Failure) [PSYoungGen: 227983K->14463K(264128K)]￼              280122K->66610K(613696K), 0.0169320 secs]￼              [Times: user=0.05 sys=0.00, real=0.02 secs]
此次GC在程序启动17.806秒后发生。现在新生代中的对象占用了14 463 KB（约14 MB，在Survivor空间中）内存，GC之前，它们占用的内存是227 983 KB（约228 MB）。1此时新生代的总大小约是264 MB。
1实际上，227 983 KB只有222 MB。为了便于讨论，本章将1 MB视为1000 KB，假装我是磁盘制造商就能接受这一点。
同时，堆的总体占用量（包括新生代和老年代）从约280 MB减少到约67 MB，此时整个堆的大小约是613 MB。这个操作耗时不到0.02秒（位于输出末尾的真实时间0.02秒是由实际时间0.016 932 0秒四舍五入后得到的）。程序耗费的CPU时间比真实时间要多，因为新生代回收是由多个线程完成的（例子中是4个线程）。
在JDK 11日志中，同样信息的日志会是这样的：
[17.805s][info][gc,start       ] GC(4) Pause Young (Allocation Failure)￼ [17.806s][info][gc.heap        ] GC(4) PSYoungGen: 227938K->14463K(264128K)￼ [17.806s][info][gc,heap        ] GC(4) PSOldGen: 30799K->52147K(348568K)￼ [17.806s][info][gc,metaspace   ] GC(4) Metaspace: 3743K->3743K(1056768K)￼ [17.806s][info][gc             ] GC(4) Pause Young (Allocation Failure)￼                                           280M->66M(613M) 16.932ms￼ [17.806s][info][gc,cpu         ] GC(4) User=0.05s Sys=0.00s Real=0.02s
这里的信息内容与JDK 8 GC日志是相同的，只是格式不同。而且该日志条目有多行，JDK 8的日志条目实际上是一行（只是没有以这种格式复现）。该日志还会输出元空间的大小，但是元空间的大小不会在新生代回收过程中发生变化。样例第5行报告的堆的总大小不包含元空间。

* 这个操作的GC日志输出如下：
64.546: [Full GC (Ergonomics) [PSYoungGen: 15808K->0K(339456K)]￼           [ParOldGen: 457753K->392528K(554432K)] 473561K->392528K(893888K)￼           [Metaspace: 56728K->56728K(115392K)], 1.3367080 secs]￼           [Times: user=4.44 sys=0.01, real=1.34 secs]
现在，新生代被占用了0字节（新生代的大小约是339 MB）内存。注意，在图6-2中，这意味着Survivor空间也被清空了。老年代数据占用的空间从约458 MB减少到约393 MB，整个堆的使用量从约474 MB减少到约393 MB。元空间的大小没有变化，因为大多数Full GC不会回收元空间。（如果元空间用完了，JVM就会运行Full GC来回收它，这时候元空间的大小才会发生变化，我稍后会展示这一点。）Full GC要做大量工作，它耗费了约1.3秒的真实时间和约4.4秒的CPU时间（同样是4个并行线程）。
JDK 11中的Full GC日志输出如下。
[63.205s][info][gc,start       ] GC(13) Pause Full (Ergonomics)￼ [63.205s][info][gc,phases,start] GC(13) Marking Phase￼ [63.314s][info][gc,phases      ] GC(13) Marking Phase 109.273ms￼ [63.314s][info][gc,phases,start] GC(13) Summary Phase￼ [63.316s][info][gc,phases      ] GC(13) Summary Phase 1.470ms￼ [63.316s][info][gc,phases,start] GC(13) Adjust Roots￼ [63.331s][info][gc,phases      ] GC(13) Adjust Roots 14.642ms￼ [63.331s][info][gc,phases,start] GC(13) Compaction Phase￼ [63.482s][info][gc,phases      ] GC(13) Compaction Phase 1150.792ms￼ [64.482s][info][gc,phases,start] GC(13) Post Compact￼ [64.546s][info][gc,phases      ] GC(13) Post Compact 63.812ms￼ [64.546s][info][gc,heap        ] GC(13) PSYoungGen: 15808K->0K(339456K)￼ [64.546s][info][gc,heap        ] GC(13) ParOldGen: 457753K->392528K(554432K)￼ [64.546s][info][gc,metaspace   ] GC(13) Metaspace: 56728K->56728K(115392K)￼ [64.546s][info][gc             ] GC(13) Pause Full (Ergonomics)￼                                             462M->383M(872M) 1336.708ms￼ [64.546s][info][gc,cpu         ] GC(13) User=4.446s Sys=0.01s Real=1.34s

* ·Throughput回收器有两个操作：Minor GC和Full GC。每个操作都会标记、释放和压缩对应的目标分代。
·根据GC日志中的时间，可以快速确定Throughput回收器的GC对应用程序的整体影响。

* 这里有两个因素需要权衡。第一个是时间和空间的取舍，这是编程中的一个经典问题。更大的堆会消耗机器上更多的内存，消耗这些内存的好处是（至少在一定程度上）应用程序会有更高的吞吐量。
第二个权衡因素涉及执行GC所需的时间长度。增加堆的大小可以减少Full GC停顿的次数，但是GC时间较长，所以这可能延长平均响应时间。同样，为了缩短GC停顿时间，可以将更多的堆分配给新生代而不是老年代，但这又会增加老年代回收的频率。

* MaxGCPauseMillis标志用于设定应用程序可接受的最大停顿毫秒数。你可能很想把它设置为0或者其他较小的值，比如50毫秒。请注意，这个标志的值适用于Minor GC和Full GC。如果使用了一个非常小的值，那么应用程序最终会得到一个非常小的老年代，例如一个可以在50毫秒内被清理的老年代。这会导致JVM非常频繁地执行Full GC，应用程序的性能会很糟糕。所以要现实一点，将该值设置为可以实现的合理值。默认情况下，不设置这个标志。

* GCTimeRatio标志用于设定你希望应用程序在GC上花费的时间（相对于应用程序线程应该运行的时间）。它是一个比例，所以N的值需要仔细考虑。

* 因为默认不对停顿时间目标进行设置，所以堆大小自动调整的效果通常是，堆（和代）的大小会增加，直到达成设置的GCTimeRatio目标。实际上，该标志的默认设置很适用。每个人的经验各有不同，我的经验是，那些在GC中花费总时间3%到6%的应用程序，运行效果相当好。有时我甚至需要在内存严重受限的运行环境中优化应用程序，这些应用程序最终有10%到15%的时间花在GC上。GC对这些应用程序的性能有很大影响，但总体性能目标还是能达成的。

* 所以最佳设置取决于应用程序的目标。在没有其他目标的情况下，一开始我会将时间比例设置为19（GC时间占5%）。

* ·堆的动态优化是调整堆大小的第一步。对于大多数应用程序，它已经可以满足需要了，动态设置能最大限度地减少JVM的内存使用。
·静态地设置堆的大小也可以获得尽可能好的性能。动态优化时，JVM为合理的性能目标确定了堆的大小，以此开始静态优化是不错的选择。



### 6.2 理解G1垃圾回收器

* G1 GC有4个逻辑操作：
·新生代回收
·后台并发标记周期
·混合回收
·必要的Full GC

* 和常规的新生代回收一样，应用程序线程被暂停了（时长0.28秒），新生代被清空（所以Eden空间的大小是0）。71 MB的数据从新生代移到了老年代。JDK 8的日志读起来有些难度（2093–3242+1220=71），JDK 11的输出更清楚。

* 尽管Mixed GC周期通常说Mixed是GC的原因，但新生代回收有时也会被标记，通常在并发周期之后（G1 Evacuation Pause）。如果并发周期在老年代中发现可以被完全回收的区域，那么这些区域会在常规的新生代疏散停顿（evacuation pause）2期间被释放。从技术上讲，在回收器的实现中，这不是混合周期，但从逻辑上讲，这是：它包含对象从新生代被回收或者被晋升到老年代，同时垃圾对象（实际上是区域）从老年代被回收的过程。
2疏散指的是存活对象在区域之间复制的过程，疏散停顿需要暂停应用程序线程。——译者注

* G1 GC启动了一个标记周期，但是老年代在这个标记周期完成之前被填满了。在这种情况下，G1 GC会中止标记周期：
51.408: [GC concurrent-mark-start]￼ 65.473: [Full GC 4095M->1395M(4096M), 6.1963770 secs]￼  [Times: user=7.87 sys=0.00, real=6.20 secs]￼ 71.669: [GC concurrent-mark-abort]￼ [51.408][info][gc,marking     ] GC(30) Concurrent Mark From Roots￼ ...￼ [65.473][info][gc             ] GC(32) Pause Full (G1 Evacuation Pause)￼                                           4095M->1305M(4096M) 60,196.377￼ ...￼ [71.669s][info][gc,marking    ] GC(30) Concurrent Mark From Roots 191ms￼ [71.669s][info][gc,marking    ] GC(30) Concurrent Mark Abort
这种失败说明应该增加堆的大小，G1 GC的后台处理必须更快，或者必须优化标记周期以更快地运行（例如，使用更多的后台线程）。之后会详细分析如何做到这一点。

* ·G1 GC有多个周期（并发周期有多个阶段）。运行G1的JVM经过良好优化后应该只经历Young GC、Mixed GC和并发GC周期。
·G1并发周期的部分阶段会出现小的停顿。
·必要时应该优化G1，以避免Full GC周期。


* 优化G1 GC的主要目标是确保没有因并发模式失败或疏散失败而产生Full GC。防止以上情况出现Full GC的技术，也适用于优化频繁发生的Young GC必须等待根区域扫描完成的情况。

* 在JDK 8中，优化Full GC是至关重要的。G1 GC在JDK 8中执行Full GC时，使用的是单线程，这就会造成停顿时间比平常更长。在JDK 11中，Full GC由多个线程执行，从而使停顿时间更短（基本上，这和使用Throughput回收器时Full GC的停顿时间相同）。这种差异就是在使用G1 GC的时候，最好更新到JDK 11的一个原因（尽管在JDK 8中避免Full GC的应用程序表现也很好）。

* G1 GC使用两组线程。第一组通过-XX:ParallelGCThreads=N标志控制，你首次看到这个标志是在第5章。这个值会影响应用程序线程暂停阶段的线程数量，这个暂停阶段指新生代回收、混合回收，以及并发标记周期中应用程序线程必须暂停的阶段。第二组通过-XX:ConcGCThreads=N标志控制，它会影响用于并发标记的线程数量。
ConcGCThreads标志默认值的计算方式如下：
ConcGCThreads = (ParallelGCThreads + 2) / 4

* ·G1 GC的优化应该从设置合理的停顿时间目标开始。
·如果在此之后Full GC还有问题，而且堆的大小不能再增加了，那么可以针对具体的问题进行具体的优化：
·o为了让后台线程运行得更频繁，可以调整InitiatingHeapOccupancyPercent标志；
·o如果有额外的CPU可用，可以通过ConcGCThreads标志调整线程数量；
·o为了避免晋升失败，可以减小G1MixedGCCountTarget的值。


### 6.3 理解CMS回收器

* CMS回收器有3个基本操作：
·回收新生代（同时暂停所有的应用程序线程）
·运行并发周期来清理老年代的数据
·如果有必要，执行Full GC来压缩老年代


* 因此，可中止的预清理阶段会等到新生代被占用50%后才开始。理论上，两次新生代回收的中间时刻，是CMS避免连续停顿的最好机会。在以上输出中，可中止的预清理阶段约从90.8秒开始，等待大约1.5秒后发生了常规的新生代回收（在日志的92.392秒）。CMS根据过去的行为记录来预估下一次新生代回收的时间——在以上输出中，CMS计算结果是，它大约会在4.2秒后发生。所以在约2.1秒之后（94.4秒），CMS结束了可中止的预清理阶段（这个操作被称为阶段的中止，这是停止这个阶段的唯一方式）。最后，CMS执行重新标记阶段，这让应用程序线程暂停了0.18秒（在可中止的预清理阶段，应用程序线程不会被暂停）。

* 这是并发周期的最后一个阶段，至此CMS周期已经完成，老年代中未被引用的对象已经被释放（最终，堆的状态如图6-8所示）。不幸的是，日志中并没有提供任何关于被回收对象的信息，重置阶段的日志也没有关于堆占用量的信息。想要了解这些信息，就需要从下一次新生代回收的日志中挖掘信息：
98.049: [GC 98.049: [ParNew: 629120K->69888K(629120K), 0.1487040 secs]￼                 1031326K->504955K(2027264K), 0.1488730 secs]
现在比较老年代的占用情况，在第89.853秒（CMS周期开始之前），此时老年代大概占用了703 MB内存（整个堆被占用了约772 MB，其中约69 MB是Survivor空间，所以老年代占据了剩余的约703 MB）。在第98.049秒，垃圾回收结束，此时老年代大约占用504 MB内存。因此CMS周期清理了约199 MB的内存。
如果一切顺利，这些就是CMS回收器运行的一个完整周期，也是CMS回收器GC日志里的所有信息。不过，还有3条消息需要核查，它们的出现表明CMS回收器遇到了问题。第一个问题是并发模式失败：

* 当发生新生代回收，但是老年代中并没有足够的空间容纳预计要晋升的对象时，CMS会执行Full GC。所有的应用程序线程都会被暂停，老年代中的任何垃圾对象都会被清理，之后内存占用量会减少到约1366 MB——这个操作会让应用程序线程停顿超过5.6秒。该操作是单线程的，这是它耗时如此之长的原因之一（也是并发模式失败随着堆的增长而影响更大的原因之一）。
这种并发模式失败是CMS被废弃的主要原因。G1 GC也会有并发模式失败的情况，但是当它执行Full GC时，Full GC在JDK 11中是并行发生的（在JDK 8中不是）。因为CMS的Full GC必须单线程执行，所以它的执行时间会长很多倍。4

* 4CMS的Full GC本来也可以并行运行，但是当时G1 GC的开发工作优先级更高。

* 第二个问题是，老年代中有足够的空间容纳晋升对象，但空闲空间是碎片化的，所以对象晋升还是会失败：
6043.903: [GC 6043.903:￼         [ParNew (promotion failed): 614254K->629120K(629120K), 0.1619839 secs]￼         6044.217: [CMS: 1342523K->1336533K(2027264K), 30.7884210 secs]￼         2004251K->1336533K(1398144K),￼         [CMS Perm : 57231K->57231K(95548K)], 28.1361340 secs]￼         [Times: user=28.13 sys=0.38, real=28.13 secs]
此时，CMS开始了新生代回收，并判断老年代的空间足够容纳所有的晋升对象（否则，它会报告并发模式失败）。但这个判断被证明是错误的，CMS无法晋升对象，因为老年代是碎片化的（或者要晋升的内存量比CMS预期的要大，这种情况更少）。
结果，在新生代回收过程中（所有线程已经暂停），CMS回收并压缩了整个老年代。好消息是，堆压缩后，碎片化问题已经解决（至少暂时解决了）。但随之而来的是长达约28秒的停顿时间。这个时间比CMS并发模式失败的时间长得多，因为整个堆都要被压缩，而并发模式失败只需要释放堆中的对象。此时堆的情况和Throughput回收器的Full GC结束后的堆情况一样（图6-2）：新生代完全是空的，老年代已经被压缩。
最后一个问题是，CMS的日志可能显示了Full GC，但是没有任何常规的并发GC消息：
279.803: [Full GC 279.803:￼                 [CMS: 88569K->68870K(1398144K), 0.6714090 secs]￼                 558070K->68870K(2027264K),￼                 [CMS Perm : 81919K->77654K(81920K)],￼                 0.6716570 secs]
当元空间被填满并需要回收时，这种情况就会发生。CMS不会回收元空间，所以如果它被填满了，就需要Full GC来丢弃任何未被引用的类。6.4节介绍了如何解决这个问题。

* ·CMS有多种GC操作，不过预期应该进行Minor GC和并发周期这两种操作。
·CMS中的并发模式失败和晋升失败的代价很大；应该优化CMS，尽量避免这些情况。
·默认情况下，CMS不会回收元空间。

* 优化CMS的首要目的是确保不会发生并发模式失败或晋升失败。正如CMS的GC日志显示的那样，并发模式失败是因为CMS清理老年代的速度不够快。当执行新生代回收时，CMS计算出没有足够的空间容纳这些将晋升到老年代的对象，所以会先回收老年代。

* 我们可以通过多种方式来避免这种失败：
·让老年代空间更大一些，要么调整新生代和老年代的比例，要么增加更多的堆空间；
·更频繁地运行后台线程；
·使用更多的后台线程。

* 自适应大小和CMS回收器
CMS回收器使用两个标志MaxGCPauseMllis=N和GCTimeRatio=N来决定应该使用多大的堆和代。
CMS所采取的方法与其他回收器的一个显著不同之处，就是除非发生Full GC，否则永远不会调整新生代的大小。由于CMS的目标是永远不发生Full GC，这意味着精心调校过CMS的应用程序永远不应该调整新生代的大小。
由于CMS会自适应地调整堆和元空间的大小，因此在程序启动阶段，并发模式失败可能会频繁发生。启动CMS时采用较大的初始堆（和较大的元空间）是个好主意，这时增大堆是为了防止失败，这是一个特殊情况。

* 你可能想把值设置为0或者其他较小的数字，这样后台CMS周期会一直运行。但是通常不建议这么做，因为只要你知道了如何进行权衡，它就可能会运行得很好。
首先要权衡的是CPU时间。CMS后台线程持续运行会消耗相当多的CPU资源——每个后台CMS线程都会100%地消耗一个CPU。当多个CMS线程运行时，还可能会有短暂的爆发，总的CPU使用率会激增。如果这些线程本不必要运行，那就会浪费CPU资源。
另外，使用这些CPU周期也不一定是个问题。即使在最优的情况下，后台CMS线程有时也必须运行，因此，机器必须始终有足够的CPU周期来运行这些CMS线程。所以在确定使用什么样的机器时，你必须为这样的CPU使用率做好规划。
第二种权衡更为重要，它和停顿有关。正如GC日志所示，CMS周期的某些阶段会暂停所有应用程序线程。使用CMS的主要原因是为了限制GC停顿的影响，因此运行CMS的频率比需要的更高会适得其反。CMS周期的停顿一般比新生代停顿短得多，特定的应用程序可能对这些额外的停顿并不敏感——这就需要在额外的停顿和降低并发模式失败的概率之间做出权衡。不过，后台GC停顿持续运行很可能会导致整体停顿时间过长，最终降低应用程序的性能。
除非考虑到了这些权衡，否则不要将CMSInitiatingOccupancyFraction设置得低于堆中的活跃数据量，至少应加上10%到20%。

* 每个CMS后台线程都会100%地消耗机器上的一个CPU。如果一个应用程序的并发模式失败，并且有额外的CPU周期可用，可以设置-XX:ConcGCThreads=N标志来增加后台线程数量。CMS设置这个标志的方式和G1 GC不同，它使用以下公式计算：
ConcGCThreads = (3 + ParallelGCThreads) / 4
所以CMS能比G1 GC早一步增加ConcGCThreads的值。

* ·避免并发模式失败是实现CMS最佳性能的关键。
·避免这些失败的最简单的方法（在可能的情况下）是增加堆的大小。
·如果不行，下一步就是通过调整CMSInitiatingOccupancyFraction来提前启动并发后台线程。
·对后台线程数量进行优化也会有帮助。


### 6.4 高级优化

* 显然，这种情况不可能永远持续下去，否则任何对象都不会被移入老年代。对象在两种情况下会被移入老年代。第一种情况，Survivor空间非常小，当目标Survivor空间在新生代回收过程中被填满时，Eden空间中剩余的任何活跃对象都会被直接移入老年代。第二种情况，对于停留在Survivor空间中的对象，其经历的GC周期数量有限制，超过这个限制的对象会被直接移入老年代。这个限制被称为晋升阈值（tenuring threshold）。


* 永远晋升和永不晋升
晋升阈值总是在1和MaxTenuringThreshold之间。即使JVM启动时的初始晋升阈值即最大晋升阈值，JVM也会减小这个值。
有两个标志可以用极端的方式规避这种行为。如果你知道新生代回收之后存活下来的对象会存在很长时间，可以设定-XX:+AlwaysTenure标志（默认是false），这基本上相当于把MaxTenuringThreshold设为0。这是一种罕见的情况，表示对象总是会晋升到老年代，而不是存储在Survivor空间中。
第二个标志是-XX:+NeverTenure（默认也是false）。这个标志会影响两件事：一是它的行为将初始晋升阈值和最大晋升阈值认为是无穷大，二是它会防止JVM降低晋升阈值。换句话说，只要Survivor空间仍有空闲，任何对象都不会晋升到老年代。

* 本节描述JVM分配对象的细节。这些细节是很有趣的背景信息，对于经常创建大量大对象的应用程序来说很重要。在这里，大是一个相对的术语，正如你将在后文中看到的，它取决于JVM中特定缓冲区的大小。
这个缓冲区被称为线程本地分配缓冲区（thread-local allocation buffer，TLAB）。所有的GC算法都要考虑TLAB的大小，G1 GC对于非常大的对象（同样也是相对的术语，例如2 GB的堆中大于512 MB的对象）有额外的考量。非常大的对象对G1 GC的影响很大，但这种情况下，调整TLAB的大小（使用任何回收器时都可以用来克服大对象问题）相当少见，而调整G1 GC的区域大小（使用G1时用来克服大对象问题）则更常见。

* 事实证明，Eden空间分配速度如此快的一个原因是，每个线程都有一个专用的区域来分配对象——线程本地分配缓冲区，或者说TLAB。当对象直接分配在像Eden空间这样的共享空间时，需要使用同步机制来管理该空间内的空闲空间指针。通过给每个线程设置自己的专用分配区域，线程在分配对象时就不需要进行任何同步了。6

* TLAB满了之后，一定大小的对象就不能再分配到其中了。此时，JVM需要做出选择。一个选项是“清退”（retire）这个TLAB，并为相关线程分配一个新的TLAB，因为TLAB只是Eden空间的一部分，清退的TLAB会在下次新生代回收时被清理，随后可以重新使用。另一个选项是，JVM可以直接在堆上分配对象，并保留现有的TLAB（至少在线程给TLAB分配额外的对象之前）。假设TLAB有100 KB，已经分配了75 KB，如果新分配的对象需要30 KB，这时可以清退TLAB，但会浪费25 KB的Eden空间。或者可以直接将这个30 KB的对象分配在堆中，线程可以期待下一个被分配的对象能够放到仍然空闲的25 KB空间内。
JVM提供了可以控制这一点的参数（本节后面将讨论），但至关重要的是TLAB的大小。默认情况下，TLAB的大小取决于3个因素：应用程序中的线程数量、Eden空间的大小和线程的分配速率。


* 除JFR之外，监控TLAB分配最好的方法是在JDK 8的命令行中添加-XX:+PrintTLAB标志，或者在JDK 11的日志配置中加上tlab*=trace（它不仅仅会提供以下信息）。然后，每次新生代回收时，GC日志就会包含两组输出：一组描述每个线程的TLAB使用情况，另一组简要描述JVM整体的TLAB使用情况。


* 花了大量时间在TLAB之外分配对象的应用程序，将分配移至TLAB内会更有益处。如果只是少数特定类型的对象总是被分配在TLAB之外，那么最好的解决方案是修改代码。

* 分配大量大对象的应用程序时可能需要优化TLAB（通常更好的办法是在应用程序中使用较小的对象）。

* G1的区域大小可以用-XX:G1HeapRegionSize=N标志设置（默认值名义上是0，即需要使用刚才描述的动态值）。这里给出的值应该是2的幂（例如1 MB或2 MB），否则，该值会向下取整到最近的2的幂。

* 因为巨型对象被直接分配在老年代，所以它在新生代回收期间不会被释放。如果是短期对象，那么这会违背回收器的分代设计。巨型对象会在G1的并发GC周期中被回收。从好的方面来看，巨型对象可以被快速地释放，因为它是所占区域中唯一的对象。巨型对象在并发周期的清理阶段被释放（而不是在Mixed GC期间）。

* 在G1 GC中，巨型对象的分配曾经是一个相当严重的问题，因为找到分配对象所需的区域通常需要执行Full GC（也因为这样的Full GC过去并没有并行化）。在JDK 8u60（以及所有的JDK 11的版本）中，G1 GC的改进将这一问题的影响最小化，所以这个曾经很严重的问题不再是问题了。

* 6这是线程局部变量可以防止锁竞争的一种变体（见第9章）。

* AggressiveHeap标志（默认是false）是在Java的早期版本中引入的，是为了更容易设置各种命令行参数——这些参数适用于运行单个JVM的、内存很大的机器。虽然从这些早期版本开始，这个标志一直沿用至今，但它已经不再被推荐了（不过它还没有被正式废弃）。
这个标志的问题是，它隐藏了实际采用的优化方式，这让我们很难弄清楚JVM到底设置了什么。而且对于这个标志设置的一些值，现在都是自动设置的，因为运行JVM的机器能够提供更详尽的信息。因此，在某些情况下，开启这个标志会损害性能。我经常看到一些命令行包含这个标志，然后又覆盖了它设置的值。（过去，这是可行的，因为命令行后面的值会覆盖前面的值。不过这种行为并无保证。）

* 与所有的优化一样，你的情况可能会有所不同。如果你仔细地测试了AggressiveHeap标志，发现它提高了性能，当然可以使用它。只不过要注意它在背后做了什么，并应该认识到，一旦升级了JVM，这个标志的相对收益就需要重新评估。

* ·AggressiveHeap标志是历史遗留下来的。针对运行单个JVM的、非常大的机器，它会尝试给堆的参数设置合理的值。
·这个标志设置的值不会随着JVM的技术进步而调整，所以从长远来看，它的作用值得怀疑（虽然它现在仍被频繁使用）。

* ·在大多数机器上，堆大小的初始值和最大值的计算相当简单。
·在边界情况下，这些计算可能会相当复杂


### 6.5 实验性GC算法

* 有两个新设计的实验性回收器旨在解决这个问题。第一个是Z垃圾回收器（Z garbage collector，ZGC），第二个是Shenandoah垃圾回收器。ZGC在JDK 11中首次出现。Shenandoah GC在JDK 12中首次出现，但现在已经被向后移植到了JDK 8和JDK 11中。
AdoptOpenJDK构建的JVM（或者你自己从源码编译的JDK）包含这两个回收器，Oracle构建的只包含ZGC。
要使用这两个回收器，必须先设定-XX:+UnlockExperimentalVMOptions标志（默认情况下是false），然后设定-XX:+UseZGC或-XX:+UseShenandoahGC来替换其他GC算法。和其他GC算法一样，这两个回收器也有一些调优标志，但是随着算法的改进，这些标志在不断变化，所以现在我们用默认参数来运行。（两个回收器的目标都是使用最少的优化来运行。）


* 第一个影响是，堆不再需要分代（不再有新生代和老年代了，只有一个堆）。新生代背后的思想是，回收堆的一小部分而不是整个堆会更快一些，并且这些对象中有很多（理想情况下是大多数）是垃圾对象。所以大部分情况下，使用新生代造成的停顿更短。如果在回收期间应用程序线程不需要暂停，那么对新生代的需求就消失了，因此这些算法也就不再需要将堆分代。
第二个影响是，应用程序线程的操作延迟预期会减少（至少在很多情况下会）。例如一个REST调用，正常情况下会在200毫秒内执行完毕。如果这个调用被G1 GC的新生代回收中断，而此次回收耗时500毫秒，那么用户将看到此次REST调用耗费了700毫秒。虽然大多数调用不会遇到这种情况，但是有一些会，这些异常的调用会影响系统的整体性能。在不需要暂停应用程序线程的情况下，并发压缩回收器不会遇到这样的异常调用。

* 这两种回收器也会在单个线程的操作中引入延迟。不同算法的细节有所不同，但总体来说，应用程序线程对对象的访问是由一个屏障来守护的。如果对象恰好在移动过程中，应用程序线程就会在屏障处等待，直到移动完成。（对于这个问题，如果应用程序线程正在访问对象，GC线程必须在屏障处等待，直到它可以重新定位对象。）实际上，这是对对象引用使用锁的一种形式，不过这个术语会让这个过程看起来比实际情况严重得多。一般来说，这对应用程序的吞吐量影响不大。

* 有一个注意事项：垃圾回收的停顿一般是造成延迟异常的最大原因，正如这里讨论的这些。但是也存在其他原因，如服务器和客户端之间的临时网络拥堵、操作系统的调度延迟等。所以，虽然前面两个例子的大量异常值是由几毫秒的停顿造成的，这些停顿并发压缩回收器也有，但我们现在已经进入了一个领域，这个领域中的其他东西对总延迟也有很大影响。
 

* 这两种回收器对吞吐量的影响比较难辨别。和G1 GC一样，它们也依赖后台线程扫描和处理堆。因此，如果这些后台线程没有足够的CPU周期，回收器也会出现之前看到的并发失败，最终发生Full GC。而且并发压缩回收器通常会比G1 GC后台线程执行更多的后台处理。

* 相反，如果这些后台线程有足够的CPU，那么使用这两种回收器时的吞吐量将高于G1 GC或Throughput回收器的吞吐量。这又和第5章看到的情况一样了。第5章的例子显示，当G1 GC将GC处理转移到后台线程时，它的吞吐量比Throughput回收器的更高。并发压缩回收器对Throughput回收器有同样的优势，对G1 GC的优势略弱。

* JDK 11还包含了一个什么都不做的回收器：Epsilon回收器。当你使用这个回收器时，对象永远不会从堆中回收，当堆被填满时，你会得到一个内存溢出错误的提示。

* 在这种情况下，禁用垃圾回收有显著的优势，性能提升了30%。并且其他回收器需要大量的内存开销。和我们看到的其他两个实验性回收器一样，Epsilon回收器也是不分代的（因为对象不能被回收，所以没有必要建立一个单独的空间让对象快速回收）。所以对这个产生了2 GB对象的测试，Epsilon回收器需要的堆的总量刚好超过这个数。我们也可以用-Xmx2052m来运行这个程序。Throughput回收器需要额外的三分之一内存空间才能容纳它的新生代，G1 GC则需要更多的内存来设置它的所有区域。
为了使用这个回收器，需要设定-XX:+UnlockExperimentalVMOptions和-XX:+UseEpsilonGC。
使用这个回收器是有风险的，除非你确定程序需要的内存永不会比你提供的大。不过一旦遇到了适用Epsilon回收器的情况，它会带来很好的性能提升。


### 6.6 小结

* 你的应用程序能容忍Full GC的停顿吗？
如果不能，G1 GC是首选算法。即使你的应用程序可以容忍一些Full GC停顿，G1 GC通常也比并行回收器好，除非你的应用程序是CPU密集型的。
在默认设置下，你是否已经获得了所需的性能？
先试试默认设置。随着GC技术的成熟，自动优化一直在提升。如果你没有获得所需的性能，请确认是GC的问题。查看GC日志并看看你在GC中花费的时间，有长时间停顿的频率。对于忙碌的应用程序来说，如果GC花了3%或者更少的时间，就不需要进行太多的优化（如果减少异常值是你的目标，那么任何时候都可以进行尝试）。
停顿时间是否在某种程度上接近你的预期目标？
如果是，你可能只需要调整最大停顿时间。如果不是，你需要做一些其他的调整。如果停顿时间太长，但吞吐量还可以，你可以减小新生代的大小（如果是Full GC停顿，就需要减小老年代）。调整之后，你会得到更多的停顿，但时间更短。
尽管GC停顿时间很短，但吞吐量仍然上不去？
你需要增加堆（或者至少是新生代）的大小，但并不总是越大越好。越大的堆会导致越长的停顿时间。即使是并发回收器，默认情况下，更大的堆也意味着更大的新生代，所以你会发现新生代回收的停顿时间更长了。如果可以的话，请增加堆的大小，或者增加代的相对大小。
你是否正在使用并发回收器，且遇到了并发模式失败导致的Full GC ？
如果你有可用的CPU，可以尝试增加并发GC线程的数量，或者可以调整InitiatingHeapOccupancyPercent来更早地启动后台扫描。对于G1来说，如果有有待处理的Mixed GC，并发周期不会启动，所以可以尝试减小Mixed GC的数量目标。
你是否正在使用并发回收器，且遇到了晋升失败导致的Full GC ？
在G1 GC中，疏散失败（To空间溢出）表示堆是碎片化的。如果G1 GC更早地执行后台清理，那么更快地执行Mixed GC通常可以解决这个问题。这需要试着增加G1并发线程的数量，调整InitiatingHeapOccupancyPercent，或者减小Mixed GC的数量目标。


### 7.1 堆分析

* 大多数时候，这些工具只对堆中的活跃对象进行操作，在下一个Full GC周期内将被回收的对象不包括在这些工具的输出中。在某些情况下，这些工具会通过强制Full GC来实现这一点，所以使用这些工具后，应用程序的行为会受到影响。在其他情况下，这些工具会遍历堆并报告实时数据，在这个过程中不会释放对象。无论哪种情况，这些工具都需要时间和系统资源，所以通常它们不用于测量应用程序的执行情况。

* 最简单的分析方式是使用堆直方图（heap histogram）。直方图是快速查看应用程序中对象数量的方法，不需要生成完整堆转储（堆转储可能需要一段时间来分析，而且会占用大量磁盘空间）。如果一些特定的对象类型对应用程序内存造成了压力，通过堆直方图能够快速发现问题。
堆直方图可以通过jcmd获取（这里的8898是进程ID）

* 直方图擅于识别由于一两个特定类的实例过量而导致的问题，但是想要更深入地分析，就需要堆转储（heap dump）了。有很多查看堆转储的工具，其中大多数可以连接正在运行的程序以生成堆转储。但从命令行生成堆转储往往更容易，可以使用以下任一命令来完成：
% jcmd process_id GC.heap_dump /path/to/heap_dump.hprof
或
% jmap -dump:live,file=/path/to/heap_dump.hprof process_id

* 在jmap中包含live选项会在堆被转储之前强制执行Full GC。这对于jcmd来说是默认的，如果因为某些原因你想包含其他的对象（死对象），可以在jcmd命令行的末尾加上-all。如果你使用的命令会强制执行Full GC，这必然会让应用程序有长时间的停顿，不过即使你不强制执行Full GC，应用程序在写堆转储文件时也会停顿一段时间。

* 保留大量堆空间的对象通常被称为堆的支配者。如果堆分析工具显示有少数对象占据了堆的大部分空间，那么事情就很简单了。你需要做的就是少创建一些这类对象、缩短它们的保留时间、简化它们的对象图，或者把它们变小。这说起来容易做起来难，但至少分析起来很简单。

* 这个例子中使用的对象类型使得分析比通常情况下容易一些。如果建模时应用程序中的主要数据使用的是String对象而不是BigDecimal对象，并且存储在HashMap对象中而不是TreeMap对象中，问题就会变得更加困难。在堆转储中有成百上千的其他String对象和成千上万的其他HashMap对象。再就是，找对象的路径时需要一些耐心。一般的经验法则是，寻找路径应该从集合对象（如HashMap）而不是从条目（如HashMap$Entry）开始，并且要寻找最大的集合。

* JVM在以下几种情况下会抛出内存溢出（out-of-memory）错误。
·JVM没有可用的原生内存。
·元空间内存不足。
·Java堆本身内存不足：对于给定大小的堆，应用程序已经无法创建任何额外的对象。
·JVM花费了太多时间执行GC。

* 但是要注意，JVM有时会因为与内存无关的信息而报告该错误。用户可以运行的线程通常有数量限制，这种限制由操作系统或者容器施加。例如在Linux中，用户常常只被允许创建1024个进程（你可以通过运行ulimit-u来检查这个值1）。如果试图创建第1025个线程，JVM就会抛出OutOfMemoryError异常，声明没有足够的内存来创建原生线程，但实际上，这是操作系统对进程数量施加的限制导致的错误。
 

* 顺便说一下，类加载器的泄漏是你应该设置元空间最大大小的原因。如果不加限制，一个有类加载器泄漏的系统会消耗机器上所有的内存。

* 自动堆转储
内存溢出错误的发生是不可预知的，这让我们很难知道什么时候需要获取堆转储。一些JVM标志可以提供帮助。
-XX:+HeapDumpOnOutOfMemoryError
打开这个标志（默认是false）会让JVM在抛出内存溢出错误时创建堆转储。
-XX:HeapDumpPath=<path>
该标志指定堆转储的写入位置，默认位置是应用程序当前工作目录下的java_pid<pid>.hprof。这里的路径可以指定目录（这种情况下会使用默认文件名）,也可以指定要生成的实际文件名。
-XX:+HeapDumpAfterFullGC
运行Full GC之后生成堆转储。
-XX:+HeapDumpBeforeFullGC
运行Full GC之前生成堆转储。
当生成多个堆转储时（例如，发生了多次Full GC），堆转储的文件名中会附加一个序号。
如果应用程序因为堆空间而不可预知地抛出了内存溢出错误，而此时你需要堆转储来分析出现错误的原因，请打开这些标志。只是要注意，获取堆转储会延长停顿时间，因为代表堆的数据会被写入磁盘。

* 如果你希望JVM在堆内存用尽时退出，可以设置-XX:+ExitOnOutOfMemoryError标志，它默认是false。


* 前面这种情况描述的恢复操作会假设，当线程遇到内存溢出错误时，和线程正在进行的工作相关联的内存可以被回收，JVM可以恢复运行。这种情况并不总是会发生，我们来看JVM抛出内存溢出错误的最后一种情况：JVM判断它已经花费了太多时间执行GC。

* ·对于元空间和常规的堆，内存溢出错误发生的最常见原因是内存泄漏。堆分析工具可以帮助找到泄漏的根本原因。

* 1线程数量也计入此限制，因为在Linux中线程本质上是共享地址空间的进程。——译者注


### 7.2 减少内存使用

* 对象会占用一定的堆内存，所以减少内存使用最简单的方法是让对象更小。考虑到运行程序的机器的内存限制，不太可能将堆的大小增加10%，但是将堆中一半对象的大小减小20%可以达到相同的目标。正如在第12章讨论的那样，Java 11正是对String对象进行了这样的优化，这意味着Java 11的用户常常可以将堆的最大大小设置得比Java 8要求的小25%，而不会对GC或性能产生影响。

* 但这掩盖了一个重要的细节，即对象的大小总是被填充到8字节的倍数。如果A类中没有定义i，A类的实例仍然会占用16字节，其中有4字节只是用来将对象的大小填充到8的倍数，而不是用来保存i。如果没有定义i，B类的实例只会占用16字节，和A类一样大，即便B类有额外的对象引用。这也是B类的实例比A类的实例只多一个4字节的引用，却比A类大8字节的原因。

* 所以在对象中消除一些实例字段或减小一些字段的大小，可能会产生收益，也可能不会，但我们没有理由不这么做。
OpenJDK项目有一个可单独下载的工具，叫作jol。它可以计算对象的大小。

* 消除对象中的实例字段有助于让对象变得更小，但这存在一个灰色地带：如果对象字段存储的是基于一些数据计算出的结果，该怎么办？这就是计算机科学中典型的时间与空间的权衡，是使用内存（空间）存储这个值，还是花费时间（CPU周期）在需要的时候计算这个值？在Java中，这种权衡也适用于CPU时间，因为使用额外的内存会导致GC消耗更多的CPU周期。

* 这种情况绝对是因人而异的。决定使用内存缓存值还是重新计算值，需要在时间和空间上找到一个合理的平衡点，这取决于很多因素。如果减少GC是目标，大多数人可能更倾向于重新计算。

* 延迟初始化最好只在相关操作使用得不那么频繁时使用。如果操作很常用，那就不能节省内存了（因为内存总是会被分配），而且它还会导致常用操作有轻微的性能损失。

* 这里存在与多线程相关的重要问题：实例变量必须被声明为volatile，将实例变量赋值到局部变量会带来轻微的性能提升。更多详细信息将在第9章给出。偶尔，多线程代码的延迟初始化是有意义的，此时应该遵循这种设计模式。

* 如果在该方法的后续调用中（或者在类的其他地方）不使用该变量，那从一开始就没有理由将其作为实例变量。只需要在方法中创建一个局部变量。当方法完成时，局部变量会离开作用域，垃圾回收器会释放它。

* 这个过时引用的概念是关键：如果一个长期存活的类会缓存并丢弃对象引用，则必须注意避免过时引用。否则，显式地将一个对象引用设置为null基本不会给性能带来什么好处。

* 因此，没有理由不设计、不使用不可变对象，虽然它们因为无法改变、必须重新创建而看起来似乎无益于达到预期，但在处理这些对象时，往往可以进行一项优化，就是避免创建同一对象的重复副本。

* 这些不可变对象的单一表示被称为对象的标准化版本（canonical version）。


* 要将一个对象标准化，需要创建一个映射来存储该对象的标准化版本。为了防止内存泄漏，要确保映射中的对象被弱引用。

* ·不可变对象为标准化这种特殊的生命周期管理提供了可能性。
·通过标准化消除不可变对象的重复副本，可以极大地减少应用程序的堆使用量。


### 7.3 对象生命周期管理

* 对象重用常常通过两种方式实现：对象池和线程局部变量。全世界的GC工程师都抱怨这两种技术会影响GC的效率。特别是对象池，GC圈里很多人因为这个原因不喜欢它，而且由于其他原因，开发圈中很多人也不喜欢对象池。

* JDK提供了一些常见的对象池：线程池（将在第9章讨论）和软引用。软引用将在本节后面讨论，它本质上是一个很大的可重用对象池。同时，Java服务器依赖对象池来连接数据库和其他资源。线程局部变量也是如此。JDK中满是使用线程局部变量的类，以避免重新分配某种类型的对象。显然，Java专家也知道在某些场景下需要重用对象。

* 随机数生成器
Random类和（特别是）SecureRandom类的实例，在使用随机数种子初始化时成本很高。

* 限流（throttling）
限流对池性能的影响是有利的，池允许对稀缺资源的访问进行限流。正如第2章所讨论的那样，如果你试图给系统增加超过其处理能力的负载，性能就会下降。这是线程池很重要的一个原因，如果太多线程同时运行，CPU就会不堪重负，性能会下降（第9章提供了这样一个例子）。
这个原则也适用于远程系统访问，并且经常在JDBC连接中看到。如果向数据库发出的JDBC连接超出了其处理能力，数据库的性能就会下降。在这些情况下，最好通过限制池的大小来限制资源数（如JDBC连接数），即使这意味着应用程序中的线程必须等待一个空闲资源。


* 对竞争锁的线程进行微基准测试总是不太可靠。表7-3中最后一行表示，线程几乎一直在竞争Random对象上的锁。但在实际的应用程序中，竞争会少很多。不过你还是会看到一些共享对象的竞争，而每次创建新对象的成本都是使用ThreadLocalRandom对象的两倍以上。

* 和对象池或线程局部变量相比，不确定引用的优势在于，它最终会被垃圾回收器回收。如果一个对象池中包含了最后执行的10 000个股票查询，并且堆开始不够用了，那么应用程序也会受影响：存储这10 000个元素后，剩余的堆空间就是应用程序可以使用的所有堆空间。如果这些查询是通过不确定引用来存储的，那JVM就可以回收一些空间（取决于引用的类型），从而提供更大的GC吞吐量。

* 另外，如果满足以下两个条件，长期运行的应用程序也可以考虑增加SoftRefLRUPolicyMSPerMB的值。
·堆中有大量空闲空间可用。
·软引用的访问频率不高。

* 这就是同时访问。这就好比我们对JVM说：“嘿，只要别人对这个对象感兴趣，就让我知道它在哪里，但是如果他们不再需要它，就丢掉它，我自己会重新创建。”然而，软引用本质上是在说：“嘿，只要有足够的内存，而且看起来有人会偶尔访问它，就尽量保留它。”
在使用弱引用时，不理解这种区分常常会造成性能问题。不要错误地认为，弱引用就是清理得更快的软引用。软引用对象通常可以使用几分钟甚至几小时，但弱引用对象只有在它的所引对象仍然存在时才可以使用（在所引对象被回收后的下个GC周期，它就会被清理）。

* 基于不确定引用的集合可能很有用，但是应该谨慎使用。如果可行的话，可以让应用程序自己管理集合。

* 终结器太糟糕了，以至于finalize()方法在JDK 11中被废弃了（尽管在JDK 8中没有）。我们将在本节的剩余部分详细介绍为什么终结器这么糟糕，但首先要说明一下它产生的原因。终结器（finalizer）最初被引入Java是为了解决JVM管理对象生命周期时可能出现的问题。在像C++这样的语言中，当不再需要一个对象时，你必须显式地销毁它，对象的析构函数可以清理该对象的状态。在Java中，当对象离开作用域时会被自动回收，终结器充当了析构函数。

* 这就导致了终结器功能上的问题，即finalize()方法可能无意中为所引对象创建了一个新的强引用。而这又会导致GC性能损失。现在直到所引对象不再被强引用所引用时，它才会被回收。从功能上来说，这造成了一个大问题。当下一次所引对象可以被回收时，它的finalize()方法不会被调用，所以预期的对所引对象的清理也不会发生。这种错误足以成为尽量少用终结器的理由。

* 这个例子克服了传统终结器的两个局限性。一是它性能更好，因为和所引对象（这个例子中是哈希映射data）关联的内存在所引对象被回收后就会被释放（而不是在finalize()方法中释放）。二是在清理代码中所引对象没有办法复活，因为它已经被回收了。

* 当进行堆转储分析时，确保终结器队列上没有对象往往可以更方便一些。这些对象无论如何都会被回收，所以在堆转储中清理它们，可以更容易分析堆中正在发生什么。你可以通过执行这个命令来处理终结器队列

* 注意，这里的内部类是静态的。否则，它将包含指向Inflater类本身的隐式引用，这样Inflater对象就永远都不可能是虚可达的了，因为Cleaner总是有强引用指向InflaterZStreamRef对象，而该对象也总是有强引用指向Inflater对象。通常，执行清理的对象不能包含指向需要被清理的对象的引用。出于这个原因，不鼓励开发人员使用lambda，而是要使用类，因为lambda太容易引用外部类了。

* ·不确定引用（软引用、弱引用、虚引用和最终引用）改变了Java对象的普通生命周期。与对象池或线程局部变量相比，不确定引用允许对象以对GC更友好的方式被重用。
·当应用程序对一个对象感兴趣，且该对象在应用程序的其他地方被强引用引用了，才应该使用弱引用。
·软引用可以长期持有对象，它会提供一个简单的、GC友好的LRU缓存。
·不确定引用会消耗自身的内存，而且会长时间持有其他对象的内存，应尽量少用。
·终结器是一种特殊类型的引用，最初是为对象清理而设计的。不鼓励使用终结器，应该使用新的Cleaner类。

* 因此，最好使用小于32 GB的堆，或者至少比32 GB大几个GB的堆。一旦更多的内存被添加到堆中以弥补未压缩的引用所使用的空间，GC周期的数量就会减少。没有硬性规定说明，改善未压缩的oop对GC的影响之前，到底需要多少内存，但考虑到平均20%的堆内存会用于对象引用，规划至少38 GB的堆是一个好的开始。

* ·压缩的oop会在最有用的时候默认启用。
·使用了压缩的oop的31 GB的堆，和稍大一些但因为太大无法而使用压缩的oop的堆相比，常常有更好的性能。



### 7.4 小结

* 即使我们在大型硬件上运行有大堆的应用程序，也很容易忽略编程中正常的时间和空间（time/space）的权衡可能会转变为时间和时空（time/space-and-time）的权衡：使用过多的堆内存会导致更多的GC，从而让应用程序变慢。在Java中，管理堆始终是很重要的。



### 8.1 内存占用

* 堆往往会占用最多的JVM内存，但是JVM也会将内存用于其内部操作。这些非堆内存就是原生内存。应用程序中同样可以分配原生内存（通过JNI调用malloc()等方法，或者通过使用New I/O，即NIO）。JVM使用的原生内存和堆内存的总和就是一个应用程序总的内存占用（footprint）。
从操作系统的角度来看，这个总的内存占用是性能的关键。如果没有足够的物理内存来容纳应用程序总的内存占用，性能可能会受影响。这里的关键词是可能。机器启动期间只使用部分原生内存（例如，和加载类路径中的JAR文件相关的内存），如果这部分内存被换出（swap out），不一定会被发现。一个Java进程所使用的原生内存有一部分是和系统中其他Java进程共享的，还有一小部分是和系统中其他应用程序进程共享的。不过在大多数情况下，为了获得最佳性能，你需要确保所有Java进程总的内存占用不超过机器的物理内存（加上你想为其他应用程序留出的内存）。

* 上述是提交的内存（或分配的内存）和保留的内存（有时被称为进程的虚拟内存）之间的本质区别。JVM必须告诉操作系统，它的堆可能需要多达2 GB的内存，所以这2 GB的内存是保留的（reserved）。操作系统会承诺，当JVM增加堆的大小并试图分配额外的内存时，这2 GB的内存是可用的。

* 这种差异几乎适用于JVM分配的所有重要内存。随着编译的代码增多，代码缓存会从初始值增长到最大值。元空间的分配是独立于堆的，但它也会从初始值（提交的内存）增长到最大值（保留的内存）。

* 在Unix系统中，一个应用程序的内存占用，可以通过各种操作系统工具报告的进程驻留集大小（resident set size，RSS）来估算。这个值可以很好地估算出一个进程所使用的提交内存量，不过它有两个地方不太精确。第一，JVM和其他进程之间，在操作系统层面会共享少量页面（共享库的文本部分），它们会被计入每个进程的RSS中。第二，一个进程提交的内存随时都比载入页面所用的内存多。不过，对于监控内存总的使用情况，跟踪进程的RSS仍然是很好的第一步。在较新的Linux内核中，PSS对RSS进行了改进，它不再记录和其他程序共享的数据。

* JVM总的内存占用对其性能的影响很大，特别是在机器的物理内存受限时。内存占用是性能测试要评估的另一个方面，通常应该对其进行监控。

* 使用-XX:NativeMemoryTracking=off|summary|detail可以查看原生内存的信息。默认情况下，原生内存跟踪（Native Memory Tracking，NMT）是关闭的（off）。如果开启了概要模式（summary）或详情模式（detail），那么你可以随时通过jcmd获取原生内存的信息：
% jcmd process_id VM.native_memory summary

* 内存的使用情况可以分解成如下几个部分。第一部分，堆本身（不出所料）是保留的内存中最大的部分，为4 GB。但是堆的动态大小意味着它只增长到了268 MB（在这个例子中，堆的大小是-Xms256m -Xmx4g，所以堆的实际使用量只扩展了一小部分）：
-                 Java Heap (reserved=4194304KB, committed=268288KB)￼                             (mmap: reserved=4194304KB, committed=268288KB)
第二部分是用于容纳类元数据的原生内存。同样，请注意JVM保留的内存比它用来存放程序中24 316个类的内存要多。其中的提交内存会从MetaspaceSize标志的值开始，根据需要逐渐增加，直到达到MaxMetaspaceSize标志的值：
-                     Class (reserved=1182305KB, committed=150497KB)￼                             (classes #24316)￼                             (malloc=2657KB #35368)￼                             (mmap: reserved=1179648KB, committed=147840KB)
第三部分是77个线程栈，每个大约占用了1 MB内存：
-                    Thread (reserved=84455KB, committed=84455KB)￼                             (thread #77)￼                             (stack: reserved=79156KB, committed=79156KB)￼                             (malloc=243KB, #314)￼                             (arena=5056KB, #154)
第四部分是即时编译代码缓存。24 316个类不是很多，所以提交的代码缓存只占用了小部分的内存：
-                      Code (reserved=102581KB, committed=15221KB)￼                             (malloc=2741KB, #4520)￼                             (mmap: reserved=99840KB, committed=12480KB)
第五部分，GC算法用于处理过程的堆外内存空间。这部分内存的大小取决于正在使用的GC算法，简单的Serial回收器保留的内存比复杂的G1 GC回收器少得多（不过一般来说，这里的数量永远不会很大）：
-                        GC (reserved=199509KB, committed=53817KB)￼                             (malloc=11093KB #18170)￼                             (mmap: reserved=188416KB, committed=42724KB)
同样，编译器除了将结果代码放到代码缓存中，还会将这部分内存用于其自身的操作：
-                  Compiler (reserved=162KB, committed=162KB)￼                             (malloc=63KB, #229)￼                             (arena=99KB, #3)
第六部分，JVM的内部操作都在这里表示。它们大部分比较小，但一个重要的例外是直接字节缓冲区（direct byte buffer），它们也被分配在这里：
-                  Internal (reserved=10584KB, committed=10584KB)￼                             (malloc=10552KB #32851)￼                             (mmap: reserved=32KB, committed=32KB)
第七部分，符号表引用（类文件中的常量）也保存在这里：
-                    Symbol (reserved=12093KB, committed=12093KB)￼                             (malloc=10039KB, #110773)￼                             (arena=2054KB, #1)
第八部分，NMT本身的操作也需要一些内存（这是它没有被默认开启的一个原因）：
-    Native Memory Tracking (reserved=7195KB, committed=7195KB)￼                             (malloc=16KB #199)￼                             (tracking overhead=7179KB)
最后，还有一些比较小的JVM簿记（bookkeeping）部分。
-               Arena Chunk (reserved=188KB, committed=188KB)￼                             (malloc=188KB)￼ -                   Unknown (reserved=8192KB, committed=0KB)￼                             (mmap: reserved=8192KB, committed=0KB)
详细的内存跟踪信息
如果用-XX:NativeMemoryTracking=detail标志启动JVM，那么jcmd（最后一个参数是detail）将提供有关原生内存分配的详细信息。这包括整个内存空间的映射

* 随时间变化的NMT
NMT还可以跟踪内存分配是如何随着时间变化的。在JVM启动时开启NMT，然后你可以用以下命令建立一个内存使用基线：
% jcmd process_id VM.native_memory baseline
这会让JVM标记其当前的内存分配。之后，你可以将当前的内存使用情况与这个标记进行对比

* NMT自动关闭
在NMT的输出中，我们看到，NMT本身也需要原生内存。此外，开启NMT会创建后台线程，以协助进行内存跟踪。
如果JVM的内存或CPU资源变得非常紧张，那么NMT会自动关闭以节省资源。这通常是件好事，除非你需要诊断的是高压力下的问题。在这种情况下，为了确保NMT继续运行，可以关闭-XX:-AutoShutdownNMT标志（默认是true）。


* 因此，原生内存泄漏，即应用程序的RSS或工作集随着时间的推移不断增长，通常不会被NMT检测到。NMT监控的内存池一般都有上限（例如，堆的最大值）。NMT很有用，它可以告诉我们哪个内存池使用了大量的内存（因此需要优化以使用更少的内存），但一个应用程序不受限制地泄漏原生内存，通常是由原生库中的问题造成的。

* 一个很好的替代方案是，使用一个既能分析原生代码也能分析Java代码的性能分析器，例如第3章讨论的Oracle Studio Profiler，它是一个混合语言分析器。这个分析器有一个选项可以跟踪内存分配。有一点需要注意，它只能跟踪原生代码的内存分配，不能跟踪Java代码的，但在这种情况下，这正是我们所需要的。

* 所以当大量原生内存泄漏时，获取应用程序的堆转储并寻找这些Inflater对象和Deflater对象是很有帮助的。这些对象本身可能不会引发堆的问题（它们太小了），但是大量的这些对象就表明，它们会占用很大的原生内存。

* 从性能的角度来看，原生字节缓冲区非常重要，因为它们允许原生代码和Java代码在不复制的情况下共享数据。用于文件系统和套接字操作的缓冲区是最常见的例子。将数据写入原生NIO缓冲区之后，不需要在JVM和传输数据的C库之间复制数据，就可以将该数据发送到通道（例如，文件或套接字）。但如果使用的是堆字节缓冲区，那么JVM必须复制缓冲区的内容。

* 从优化的角度来看，使用任何一种编程模型都要注意一件事，那就是应用程序可以分配的直接字节缓冲区的内存总量受JVM的限制。为直接字节缓冲区分配的内存总量可以通过-XX:MaxDirectMemorySize=N标志设定。在当前的JVM中，这个标志的默认值是0。这个限制的含义经常变化，在Java 8的后期版本（以及Java 11的所有版本）中，这个限制的最大值等于堆的最大值。如果堆的最大值是4 GB，你可以在直接字节缓冲区和/或映射字节缓冲区中创建4 GB的堆外内存。如果需要，你还可以增加该值，使其超过堆的最大值。

* Linux系统内存泄漏
大型Linux系统由于其内存分配库的设计，有时会出现原生内存泄漏的情况。这些库将原生内存划分为分配段，这有利于多线程的分配（因为它限制了锁的竞争）。
但是，原生内存的管理方式并不像Java堆那样，特别是，原生内存从来不会进行压缩。因此，原生内存中的分配模式会导致第5章所描述的碎片化。
在Java中，原生内存的碎片化有可能会耗尽原生内存。这种情况最常发生在大型系统上（例如，超过8个核心的系统）。这是因为在Linux中，内存分区的数量是通过系统中核心的数量得出的。
有两件事可以帮助诊断这个问题。第一，应用程序会抛出OutOfMemoryError异常，表明原生内存已经耗尽。第二，如果查看进程的smaps文件，那么会发现它显示有许多小的（通常是64 KB）分配。在这种情况下，补救的办法是将环境变量MALLOC_ARENA_MAX设置为像2或4这样小的数。该变量的默认值是系统的核心数乘以8（这也是这个问题在大型系统上比较常见的原因）。在这种情况下，原生内存仍然会变得碎片化，但不会太严重。


### 8.2 针对操作系统的JVM优化

* 大页必须在Java和操作系统两个层面上开启。在Java层面，-XX:+UseLargePages标志可以开启大页的使用，默认情况下这个标志是false。并非所有的操作系统都支持大页，开启大页的方式显然也不尽相同。
如果在不支持大页的系统上开启了UseLargePages标志，那么JVM不会发出警告，它会使用常规的页面。如果在支持大页的系统上开启UseLargePages标志，但没有大页可用（比如它们已经被全部使用，或者操作系统配置错误），那么JVM会发出警告。


* 如果修改了limits.conf文件，用户必须再次登录才能使该值生效。此时，JVM就能分配必要的巨页了。可以运行以下命令验证它已经生效：

* Linux内核从2.6.32版本开始支持透明巨页。透明巨页理论上提供了与传统巨页相同的性能优势，但它和传统巨页有一些区别。
第一，传统巨页是锁定在内存中的，它们永远并不能被交换。对于Java来说，这是个优势，正如已经讨论过的，因为交换堆的一部分会影响GC性能。透明巨页可以被交换到磁盘上，这对性能是不利的。
第二，透明巨页的分配和传统巨页也有很大不同。传统巨页是在内核启动时预留的，它一直可用。透明巨页是按需分配的。当应用程序请求一个2 MB的页面时，内核会尽量在物理内存中为该页面找到2 MB的连续空间。如果物理内存是碎片化的，那么内核会花时间重新整理页面，这个过程类似于我们已经很熟悉的Java堆中的内存压缩。这意味着分配一个页面的时间可能会大大延长，因为它需要等待内核为页面腾出内存空间。
这会影响所有的程序，对于Java来说，这会导致非常长的GC停顿时间。在GC期间，JVM可能会扩展堆并请求新的页面。如果页面分配耗费了几百毫秒甚至一秒，那么GC时间会受到很大影响。
第三，透明巨页的配置在操作系统层面和Java层面都是不同的。细节如下。
在操作系统层面，透明巨页通过改变/sys/kernel/mm/transparent_hugepage/enabled的内容来配置

* 由于透明巨页在交换和分配方面的差异，通常不建议在Java中使用它。它的使用肯定会导致不可预测的停顿时间高峰。另外，特别是在默认开启了透明巨页的系统上，大部分使用它的情况下，你会看到性能上的提升，正如宣传的那样，这对应用程序是透明的。不过，如果想确保在使用巨页时能获得最流畅的性能，最好将系统设置为只在请求时使用透明巨页，并配置传统巨页供JVM使用。

* ￼ 快速小结
·使用大页通常会显著加快应用程序的速度。
·在大多数操作系统上，必须显式开启大页支持。


### 8.3 小结

* 虽然Java堆是最受关注的内存区域，但是对JVM性能至关重要的是其整体内存占用，特别是和操作系统相关部分的内存占用。本章讨论的工具可以让你跟踪随时间变化的内存占用（关键是，要关注JVM提交的内存，而非保留的内存）。
也可以通过优化JVM使用操作系统内存（特别是大页）的方式来提升性能。长期运行的JVM几乎总能受益于大页的使用，特别是当它有大堆时。


### 9.2 线程池和ThreadPoolExecutor

* 使用线程池的一个关键是，线程池的大小对获取最佳性能至关重要。线程池的性能会根据线程池大小这一基本选择而有所不同，在某些情况下，过大的线程池会对性能造成损害。

* 线程池有最小线程数和最大线程数。线程池会以最小数量的线程等待分配给它们的任务。因为创建线程是相当昂贵的操作，所以当一个任务被提交时，已经存在的线程会去获取它，这加快了整个操作的速度。另外，线程需要系统资源，它们的栈需要原生内存。空闲线程过多会消耗本可以被其他进程使用的资源。最大线程数可以起到必要的限流作用，防止线程同时执行过多的任务。

* 在有4个核心的机器上，一个线程池使用了给定数量的线程，在用它计算10 000只模拟股票价格的历史数据时，其性能如表9-1所示。当池中只有1个线程时，计算这个数据集需要55.2秒，当有4个线程时，只需要13.9秒。之后，随着线程的增加，需要的时间会稍稍增加。

* 超线程的效果
如果这4个CPU是超线程的，有2个核心和4个硬件线程，会发生什么？表9-2展示了在这台机器上进行上述测试的效果。和前面的例子一样，在JVM看来，这是一台4核机器，但是基准测试的扩展情况大不相同。

* 程序从1个CPU扩展到2个CPU时表现得很好，因为它们都完全使用了核心。但是添加2个超线程时只带来一点点好处。这是性能提升最不明显的情况。如果线程因I/O而暂停或者等待另一个线程持有的锁，那么超线程的好处就会更明显。不过，增加一个超线程通常只会带来20%的性能提升。

* 如果是这样的话，向线程池添加线程是有害的。尽管在第1章中我说过（有点开玩笑的成分）数据库永远是瓶颈，但瓶颈也可能是任何外部资源。

* 但重点是，同样的原则也适用于向CPU密集型或I/O密集型的数据库发送请求的REST服务器。你只查看了服务器的CPU，看到它的使用率远低于100%，而且还有额外的请求需要处理，就认为增加服务器线程会是个好主意。结果会让你大吃一惊，因为在这种情况下增加线程数实际上会降低总体吞吐量（而且可能会显著降低），这就跟上述只用了Java应用程序的例子一样。


* 这也是为什么线程池的自我优化非常困难。线程池通常可以得知待处理的工作量，甚至可以知道机器有多少CPU可用，但是它们通常无法得知其所在的整体运行环境的其他方面。因此，在工作仍待处理时添加线程——很多自我优化的线程池（以及ThreadPoolExecutor的某些配置）的一个关键特性——往往是错误的做法。

* 一旦确定了线程池的最大线程数，就该确定所需的最小线程数了。可以直截了当地说，它无关紧要，但为简单起见，几乎在所有的情况下，都可以将最小线程数设置成和最大线程数一样的值。

* 是否应该预先创建线程？
默认情况下，创建一个ThreadPoolExecutor时，它开始只会有一个线程。考虑一种配置，线程池需要8个核心线程，最大线程数是16。在这种情况下，核心设置可以被看作最小线程数，因为8个线程即使是空闲的，也会保持运行。但是这8个线程仍然不会在创建线程池时被创建，它们是按需创建的，创建后会保持运行。
在服务器示例中，这意味着在创建线程期间，前8个请求会略有延迟。正如你在本节所看到的那样，这种延迟的影响很小，你也可以提前创建这些线程（使用prestartAllCoreThreads()方法）。

* 一般来说，在线程池的线程数为最小值的情况下，新的线程创建后，它应该存留几分钟，以处理任何负载高峰。如果你有一个很好的到达率模型，可以根据这个模型来计算空闲时间。如果没有，空闲时间应以分钟为单位，至少在10分钟到30分钟。


* 线程池的待处理任务保存在队列或列表中。当池中的一个线程可以执行任务时，它会从队列取出一个任务。这可能会导致不平衡，因为队列中的任务数量可能会变得非常大。如果队列太长，队列中的任务就必须等待很长时间，直到前面的任务完成执行。例如一台超负荷的Web服务器，如果一个任务被添加到队列中，3秒内没有执行，那么用户很可能就去看另一个页面了。

* 无论如何，当队列长度达到限制时，向队列添加任务就会失败。ThreadPoolExecutor有一个rejectedExecution()方法可以处理这种情况（默认情况下，它会抛出RejectedExecutionException异常，但可以重写这个行为）。应用程序服务器应该向用户返回一个合理的响应（用一条消息说明发生了什么），REST服务器应该返回状态码429（太多请求）或503（服务不可用）。

* 线程池的一般行为是，它以最小数量的线程开始，如果所有线程都忙碌时来了一个新的任务，线程池会启动一个新的线程（直到达到最大线程数），并立即执行该任务。如果已经启动了最大数量的线程，并且它们都在忙碌中，新任务会被放在队列中。如果已经有很多任务在等待处理，在这种情况下，新任务会被拒绝。这是线程池的标准行为，但ThreadPoolExecutor的行为会有些不同。
ThreadPoolExecutor根据存放任务的队列类型来决定何时启动新的线程。队列类型有以下3种。

* 并发队列
当执行器使用并发队列（例如SynchronousQueue）时，在线程数量方面，线程池的表现符合预期。如果所有线程都在忙碌，而且池中的线程数小于最大线程数，那么新任务会启动一个新线程。但是，这个队列类型无法保留待处理的任务。如果最大数量的线程都在忙碌时来了一个任务，那么该任务会被拒绝。所以这个队列类型适用于管理少量任务，其他情况不适用。这个类的帮助文档建议设定一个非常大的最大线程数，即便任务完全是I/O密集型的，也可以这样做。但正如我们所看到的，在其他情况下这可能会适得其反。另外，如果你需要一个易于优化线程数的线程池，那么这个队列类型是更好的选择。
在这种情况下，核心池大小是最小池大小，也就是空闲时也会保持运行的线程数。最大池大小是池中的最大线程数。
Executors类的newCachedThreadPool()方法返回的是这种类型的线程池（它的最大线程数是没有限制的）。
无界队列
当执行器使用无界队列（例如LinkedBlockingQueue）时，任何任务都不会被拒绝（因为队列大小不受限制）。在这种情况下，执行器使用的线程数最多等于核心池大小，即最大池大小会被忽略。这本质上是模拟的传统线程池，其中核心池大小被解释为最大池大小。不过由于队列是无界的，因此如果任务的提交速度超过了运行速度，那么会有内存消耗过多的风险。
Executors类的newFixedThreadPool()方法和newSingleThreadScheduledExecutor()方法返回的是这种类型的线程池。第一种情况下，核心池大小（或最大池大小）是构造池时传递的参数值。第二种情况下，核心池大小是1。
有界队列
使用有界队列（例如ArrayBlockingQueue）的执行器采用了复杂的算法来决定何时启动新线程。例如，假设核心池大小是4，最大池大小是8，ArrayBlockingQueue的最大值是10。随着任务到达并被放入队列，线程池最多会运行4个线程（核心池大小）。即使队列完全填满，也就是它包含10个待处理任务，执行器也只能利用4个线程。
仅当队列已满，并且有新任务被添加到队列中时，才会有新线程被启动。执行器不会因为队列已满而拒绝任务，而是会启动新线程。新线程会运行队列中的第一个任务，为添加新的待处理任务腾出空间。
在这个例子中，让线程池有8个线程（最大池大小）的唯一方法是，当有7个任务正在处理，且队列中有10个任务时，将新的任务添加到队列中。
这个算法背后的想法是，即使队列中有适量的任务在等待运行，也只让线程池在大部分时间里运行4个核心线程。这使得线程池可以作为一个限流器（这是有利的）。如果任务积压得太多，线程池就会运行更多的线程来清理积压的任务（此时最大线程数可以作为第二个限流器）。
如果系统中没有外部瓶颈，并且有可用的CPU周期，那么一切都可以解决。新线程可以更快地处理任务队列，而且很可能让队列恢复到预期的长度。所以适合该算法的情况很容易构造。
另外，该算法并不会判断队列为什么会变长。如果这是由外部积压造成的，那么添加更多的线程是错误的做法。如果线程池运行在一台CPU密集型的机器上，那么添加更多的线程也是错误的。只有当系统中出现了额外的负载并导致积压时（例如，更多的客户端发出HTTP请求），添加线程才有意义。（然而如果是这样的话，为什么要等到队列长度到达某个界限时才增加线程呢？如果有额外的资源可以利用额外的线程，那么尽早添加线程会提高系统的整体性能。）
对于以上队列类型，每个都有许多支持和反对的论点，但在试图将性能最大化的时候，可以应用KISS原则：keep it simple，stupid。与往常一样，应用程序的需求可能会决定其他选择，但作为一般建议，不要使用Executors类来提供默认无界的线程池，这样你无法控制应用程序的内存使用情况。反而你应该设置ThreadPoolExecutor，让其有相同的核心线程数和最大线程数，并利用ArrayBlockingQueue来限制内存中待处理任务的数量。



### 9.3 ForkJoinPool

* ForkJoinPool类是为配合分治算法设计的。分治算法中的任务可以递归地分解成子集，这些子集可以并行处理，最后所有子集的结果会被合并为一个结果。这方面的经典例子是快速排序算法。

* 使用ThreadPoolExecutor不能高效地执行该算法，因为父任务必须等待其子任务完成，而线程池执行器中的线程不能向队列中添加另一个任务并等待任务完成，一旦其线程处于等待状态，它就不能用来执行任何子任务了。ForkJoinPool允许它的线程创建新的任务，然后挂起当前任务。当任务被挂起时，其线程可以执行其他待处理任务。

* 这里的fork()方法和join()方法是关键。如果没有这些方法（ThreadPoolExecutor执行的任务中就没有这些方法），很难实现这种递归。这些方法使用了一系列针对每个线程的内部队列来管理任务，并将线程从执行一个任务切换到执行另一个任务。这些细节对开发人员来说是透明的，如果你对算法感兴趣，那么会发现这些代码读起来很有趣。这里关注的重点是性能，ForkJoinPool和ThreadPoolExecutor之间存在哪些权衡？
首先，fork/join范式所实现的挂起使得所有任务只需要几个线程来执行。使用该示例代码统计一个有200万个元素的数组中的double值，会创建400多万个任务，但这些任务只需要几个线程（甚至是一个线程，如果这对运行测试的机器有意义的话）就能轻松执行。而使用ThreadPoolExecutor运行类似的算法将需要400多万个线程，因为每个线程都必须等待其子任务完成，这些子任务只有在池中有额外线程可用时才能完成。所以，fork/join的挂起让我们可以使用原本不能使用的算法，这是性能上的胜利。
其次，像这样的简单算法并不是特别适用于在现实世界中使用的ForkJoinPool。这个线程池非常适合以下情况：
·算法的合并部分会执行一些有趣的工作（而不是像本例中那样简单地将两个数字相加）；
·算法的叶子计算所执行的工作足以抵消创建任务的开销。
在不满足以上两种情况时，很容易会将数组分割成块，然后使用ThreadPoolExecutor让多个线程扫描数组

* 使用这个线程池的一个规则是，要确保拆分任务是有意义的。不过ForkJoinPool的第二个特性使它更加强大：它实现了工作窃取。这基本是个实现细节，它意味着池中的每个线程都有一个自己所派生任务的队列。线程会优先处理自己队列中的任务，如果自己的队列是空的，那么它们会从其他线程的队列中窃取任务。结果是，即使400万个任务中有一个执行时间很长，ForkJoinPool中的其他线程也可以完成剩余的所有任务。而ThreadPoolExecutor就不是这样的了，如果它的一个任务需要很长时间，其他线程就无法执行其他的任务。

* 这种情况被称为不均衡，因为有些任务比其他任务耗时更长（因此前面例子中的任务是均衡的）。总之，这就形成了一个总结性建议：当任务很容易被分割成均衡的集合时，使用分区的ThreadPoolExecutor会有更好的性能，而当任务不均衡时，使用ForkJoinPool会有更好的性能。

* Java可以自动并行化某些类型的代码。这种并行化依赖于使用ForkJoinPool类。为此，JVM创建了一个公共的ForkJoinPool，它是ForkJoinPool类的一个静态元素，默认大小是目标机器上处理器的数量。

* 这段代码会并行计算模拟价格的历史数据。forEach()方法将为数组列表中的每个元素都创建一个任务，每个任务由公共的ForkJoinTask池处理。这本质上等同于本章开头的测试，那个测试使用了线程池来并行计算模拟价格的历史数据。不过和显式使用线程池相比，这段代码写起来更容易。

* 在使用并行流结构和其他自动并行化特性的时候，如果你需要优化公共池的大小，可以考虑将所需的值减1。


### 9.4 线程同步

* 在完美的世界里，或者在一本书的例子中，要避免线程同步是相对容易的。在现实世界中，这就不一定那么容易了。

* 同步和Java并发工具
在本节中，同步指的是代码块对一组变量的访问看上去是串行的：每次只有一个线程可以访问内存。这包括由synchronized关键字保护的代码块，也包括用java.util.concurrent.lock.Lock类的实例保护的代码，以及java.util.concurrent和java.util.concurrent.atomic包中的代码。
严格地说，原子类不使用同步，至少在CPU编程方面是这样。原子类利用了比较并交换（Compare and Swap，CAS）CPU指令，而同步需要独占访问资源。使用CAS指令的线程在同时访问同一资源时不会阻塞，而需要同步锁的线程在另一个线程持有该资源时会阻塞。
这两种方法具有不同的性能（本节后面会讨论）。不过，即使CAS指令是无锁和非阻塞的，它们仍然会表现出阻塞结构的大部分行为。它们的最终结果会让开发人员认为，线程只能串行地访问受保护的内存。

* 同步的第二个开销是Java特有的，并取决于Java内存模型（Java Memory Model）。Java不同于C++和C这样的语言，它对关于同步的内存语义有严格的保证，并且该保证适用于基于CAS的保护、传统的同步，以及volatile关键字。

* 我在第1章中提到过，应该学会避免Java中性能不高的代码结构，即使这看起来像是在“过早优化”代码（事实并非如此）。一个有趣的案例是下面这个循环，这也是一个真实的例子：
Vector v;￼ for (int i = 0; i < v.size(); i++) {￼     process(v.get(i));￼ }
在生产环境中，这个循环耗费的时间惊人，合乎逻辑的假设是process()方法是罪魁祸首，但事实并非如此，问题也不在于size()和get()方法调用（它们已经被编译器内联）。Vector类的get()和size()是同步的，所以，所有这些调用所需的寄存器刷新是个巨大的性能问题。1

* ·线程同步有两个性能开销：它限制了应用程序的可扩展性，并且需要获取锁。
·同步的内存语义、基于CAS的结构，以及volatile关键字会对性能产生负面影响，特别是在有很多寄存器的大型机器上。

* 如果可以完全避免同步，加锁的损失就不会影响应用程序的性能。这可以用两个通用的方法来实现。
第一个方法是，在每个线程中使用不同的对象，这样访问对象时就不存在竞争了。为了实现线程安全，很多Java对象是同步的，但它们未必需要共享。Random类就是这样的。第12章展示了JDK中的一个例子，它使用线程局部技术开发了一个新的类，以避免Random类中的同步。
另外，很多Java对象的创建成本很高，或者会使用大量内存。以NumberFormat类为例，这个类的实例不是线程安全的，而且创建一个实例所需要满足的国际化需求让构造新对象的成本很高。一个程序可以通过单一的、全局共享的NumberFormat实例来避免这种开销，但对该共享对象的访问需要同步。
使用ThreadLocal对象反而是更好的模式。

* 使用上述代码片段构建一个只有两个线程运行的微基准测试，那么共享资源上将存在大量竞争。这也不是现实中的情况，在实际的应用程序中，两个线程不可能总是同时访问共享资源。增加更多的线程只是增加了更多不现实的竞争。

* ·如果对资源的访问是无竞争的，基于CAS的保护会比传统的同步稍微快一些。如果访问始终是无竞争的，完全无保护还会再快一些，并且可以避免边界情况，比如刚才看到的Vector类的寄存器刷新。
·如果对资源的访问存在轻度或者适度的竞争，基于CAS的保护会比传统的同步更快（通常会快得多）。
·随着所访问资源的竞争越来越激烈，传统的同步将在某个时候成为更高效的选择。在实践中，这种情况只发生在运行了很多线程的大型机器上。

* java.util.concurrent.atomic包中的类使用了基于CAS的原语，而不是传统的同步。因此，这些类（例如AtomicLong类）的性能往往比编写同步方法来增加long变量要快，至少在对CAS原语的竞争变得过于激烈之前是这样。

* Java有一些类可以解决有太多线程竞争访问基本类型原子值的情况，如原子的加法器和累加器（例如LongAdder类）。这些类比传统的原子类更具可扩展性。当多个线程更新一个LongAdder时，该类可以分别为每个线程保存更新。这些线程不需要等待其他线程完成操作，因为这些值（本质上）被存储在一个数组中，每个线程都可以快速返回。之后，当一个线程试图检索当前值时，这些值会被累加起来。

* ·避免同步对象的竞争是减轻其性能影响的有效方法。
·线程局部变量永远都不会发生竞争，它们非常适合保存实际上不需要在线程间共享的同步对象。
·对于确实需要共享的对象，基于CAS的工具是一种避免传统同步的方法。

* 关于同步对性能的影响，有一个很少讨论的方面是伪共享（false sharing），它也被称为缓存行共享（cache line sharing）。它是多线程程序的产物，曾经相当隐蔽。但是随着多核机器成为常态，其他更明显的同步性能问题得到解决，伪共享问题就越来越重要。
伪共享的发生和CPU处理缓存的方式有关。

* 另外，检测伪共享需要直觉和实验。如果一次普通的性能分析显示某个循环花费了惊人的时间，请检查是否有多个线程在循环中访问非共享变量。（在这个领域，性能优化更像是一门艺术而非科学，连Intel VTune Amplifier手册都说“避免伪共享的主要方法是代码检查”。）

* @Contended注解
JDK私有类中的一个特性可以减少设定字段上的缓存竞争（JEP 142）。这是通过使用@sun.misc.Contended标记由JVM自动填充的变量来实现的。
这个注解是私有的。在Java 8中，它属于sun.misc包，没有什么能阻止你在自己的代码中使用这个包。在Java 11中，它属于jdk.internal.vm.annotation包，由于Java 11使用了模块系统，如果不用-add-exports标志将该包添加到java.base模块导出的类集中，就无法使用这个包编译类。
默认情况下，JVM会忽略这个注解，除非它在JDK的类中。要让应用程序代码使用该注解，需要加上-XX:-RestrictContended标志，它默认是true（意味着该注解仅限于JDK的类使用）。
另外，如果要禁用JDK的自动填充，请设置-XX:-EnableContended标志，它默认是true。这会减小Thread和ConcurrentHashMap类的大小，这两个类都使用这个注解来填充它们的实现，以防止伪共享。
 


### 9.5 JVM线程优化

* 这个原生栈的大小是1 MB（32位的Windows JVM除外，它的原生栈大小是320 KB）。在64位的JVM中，通常不会修改这个值，除非机器的物理内存相当紧张，并且较小的栈大小可以防止应用程序用完原生内存。在内存受限的Docker容器内，尤其如此。

* 特别是，使用线程池的应用程序（包括某些应用程序和REST服务器）在偏向锁生效时，往往表现得更差。在这种编程模型中，各个线程有同等的机会访问竞争的锁。对于这类应用程序，可以通过-XX:-UseBiasedLocking禁用偏向锁，从而获得小幅性能提升。偏向锁是默认开启的。


* 操作系统会为机器上运行的每个线程计算一个当前优先级。当前优先级既考虑了Java分配的优先级，也考虑了许多其他因素，其中最重要的因素是线程上次运行到现在有多长时间。这确保了所有线程都有机会运行。无论其优先级如何，都不会有线程因为等待访问CPU而“饥饿”。


* 无论在哪种情况下，都不能依赖线程的优先级来决定它的运行频率。如果某些任务比其他任务更重要，就必须使用应用程序逻辑来确定它们的优先级。


### 9.6 监控线程和锁

* 绝大多数JVM监控工具提供了关于线程数量（以及线程正在做什么）的信息。像jconsole这样的交互式工具可以显示JVM内的线程状态。在jconsole的线程面板上，你可以实时观察程序执行过程中线程数量的增减情况。图9-2展示了一个例子。
某一时刻，该应用程序（NetBeans）最多使用了45个线程。在图的开始可以看到一个爆发点，此时最多使用了38个线程，最终使用的线程数稳定在30到31。jconsole还可以输出单个线程栈，如图9-2所示，Java2D Disposer线程正在等待某个引用队列的锁。

* 在这个示例中，sun.awt.AppContext.get()方法中与HashMap关联的锁被竞争了163次（超过66秒），导致被测请求的响应时间平均增加了31毫秒。栈轨迹表明，竞争源于JSP写入java.util.Date对象的方式。为了提升这段代码的可扩展性，可以使用线程局部的日期格式化器，而不是简单地调用日期的toString()方法。
这个过程——从直方图中选择阻塞事件并检查调用代码——对任何类型的阻塞事件都有效。这是因为该工具与JVM紧密集成。


* 线程栈可以显示线程被阻塞的严重程度（因为被阻塞的线程已经在安全点了）。如果连续的线程转储显示许多线程阻塞在一个锁上，那么可以断定这个锁有显著的竞争。如果连续的线程转储显示许多线程阻塞在等待I/O上，那么可以得出结论：无论它们正在读取什么I/O，都需要进行优化（如果它们正在执行数据库调用，需要优化它们正在执行的SQL或者数据库本身）。


### 9.8 小结

* 了解线程的运行方式可以获得很大的性能优势。不过，线程性能没有太多可以优化的，因为可以调整的JVM标志相对较少，而且这些标志的效果也十分有限。
相反，良好的线程性能需要遵循两个最佳实践准则：管理线程数和限制同步影响。在适当的性能分析和锁分析工具的帮助下，可以检查和修改应用程序，使线程和锁的问题不会对性能产生负面影响。


### 第10章 Java服务器

* 这些较新的框架提供了基于响应式编程的编程模型。作为该模型的核心，响应式编程使用基于事件的范式处理异步数据流。尽管响应式编程看待事件的方式不同，但就我们的目的来说，响应式编程和异步编程提供了相同的性能优势：能够扩展程序（特别是扩展I/O）以处理很多连接和数据源。



### 10.1 Java NIO概览

* 现在，客户端不再与特定的服务器线程耦合，所以可以优化服务器线程池，以处理我们期望服务器处理的并发请求数。在之前的例子中，大小为2的线程池足以处理所有100个客户端的请求。如果请求到达得不均匀，但平均思考时间还是30秒，那么可能需要五六个线程来处理并发请求的数量。非阻塞I/O让我们可以使用比客户端数量少很多的线程来处理并发请求，这极大地提高了效率。


### 10.2 服务器容器

* ·使用Java NIO API的非阻塞I/O，可以通过减少处理多个客户端所需的线程数量来扩展服务器。
·这个技术意味着服务器将需要一个或多个线程池来处理客户端的请求。该线程池应该根据服务器可能处理的最大并发请求数进行优化。
·此外，还需要一些额外的线程来充当选择器（是作为工作线程池的一部分，还是作为一个单独的线程池，这取决于服务器框架）。
·通常，服务器框架有一种机制，可以将长请求推迟到另一个线程池中，这为主线程池提供了更强大的请求处理能力。


### 10.3 异步出站调用

* 所有的HTTP客户端都提供了一种机制来进行池化，不过HttpURLConnection内部的池化机制经常被误解。默认情况下，这个类会池化5个连接（每台服务器）。不过与传统的池不同，这个类中的池并不会对连接进行限流。如果请求第6个连接，那一个新的连接就会被创建，完成任务后会被销毁。在传统连接池中没有这种短暂的连接。所以在HttpURLConnection类的默认配置下，经常有大量短暂的连接，而且很容易将这些看成是没有池化的连接（此时Javadoc也没有什么用处。尽管在其他地方有池化的记录，但Javadoc从来没有提到过池化功能）。

* ·一定要确保HTTP客户端连接池的设置是正确的。
·异步HTTP客户端可以通过在多个线程之间分配工作来提高性能，从而增加并发量。
·与使用传统I/O构建的客户端相比，使用NIO构建的异步HTTP客户端需要的线程更少，但REST服务器仍然需要相当多的线程来处理异步请求。

* 异步JDBC包装器的提议（和实现）通常会将JDBC的工作推迟到一个单独的线程池中。这与Jersey异步HTTP客户端类似。从程序的角度来看，这些API是异步的。但在实现中，后台线程被阻塞在I/O通道上，我们并没有通过这个方法获得可扩展性。
JDK之外的许多项目可以填补这个空白。使用最广泛的是Spring项目的Spring Data R2DBC。这需要使用另外的API，而且其驱动只适用于某些数据库。不过，对于关系数据库的非阻塞访问来说，这是最好的选择。
对于NoSQL数据库，情况有些类似。另外，原本就不存在访问NoSQL数据库的Java标准，你在编程时无论如何都要依赖数据库的专有API。所以针对响应式NoSQL数据库的Spring项目可以用于真正的异步访问。


### 10.4 JSON处理

* 为了获得最大的灵活性，对象模型提供了Java语言级别的数据表示形式。你可以用熟悉的对象及其属性来操作数据。对开发人员来说，解码添加的复杂性（大部分）是透明的，这可能会使应用程序的这一部分速度稍慢，但开发人员提升的代码生产率可以抵消这一问题。


* 所有的JSON解析器都是拉解析器，它的操作方式是从流中按需检索数据。对于本节的测试，基本的拉解析器以下面这个循环作为其主要逻辑：
parser = fac

* ·处理JSON有两种方式：创建POJO对象和直接解析。
·这个选择取决于应用程序的需要。直接解析提供了过滤的能力和通用的性能提升机会。当对象很大时，创建JSON对象往往会导致GC问题。
·Jackson解析器通常是最快的解析器，应该优先选择它，而不是选择默认的实现。


### 11.2 JDBC

* 编写的JDBC驱动可以在Java应用程序（数据库客户端）中执行更多的工作，也可以在数据库服务器上执行更多的工作。最好的例子是Oracle数据库的瘦驱动和胖驱动。编写瘦驱动是为了让Java应用程序的内存占用很小，它依赖数据库服务器来完成更多的处理工作。胖驱动恰恰相反，它将工作从数据库移至Java应用程序，代价是Java客户端需要进行更多处理、消耗更多内存。大多数数据库中存在这两种驱动。

* 不要把驱动类型（2型和4型）和驱动方式（胖驱动和瘦驱动）混为一谈。虽然2型驱动往往是胖驱动，4型驱动往往是瘦驱动，但这不是必然的。最后，2型驱动好还是4型驱动好，这取决于运行环境和具体的驱动。真的没有一种先验的方法可以知道哪一种性能更好。

* ·初始化连接对象的成本很高，在Java中它们经常被池化——无论是在JDBC驱动中还是在JPA和其他框架中。
·和其他对象池一样，优化连接池很重要，否则它会对垃圾回收器产生不利影响。在这种情况下，优化连接池也很必要，这样它就不会对数据库的性能产生不利影响。

* ·Java应用程序通常会重复执行相同的SQL语句。在这种情况下，重用预处理语句会有显著的性能提升。
·预处理语句必须在每个连接的基础上进行池化。大多数JDBC驱动和数据框架可以自动地做到这一点。
·预处理语句会消耗大量的堆空间。必须仔细优化语句池的大小，避免因池化太多非常大的对象而产生GC问题。

* 事务的提交成本很高，所以我们的一个目标是在一个事务中执行尽可能多的工作。不幸的是，这个目标和另一个目标完全对立，因为事务可能持有锁，所以它们的执行时间应该尽可能短。这里肯定有一个平衡点，而如何确定这个平衡点则取决于应用程序，以及它对锁的要求。下文会更详细地介绍关于事务隔离和锁的内容。我们先来看看可以选择哪些方法来优化事务处理本身。

* 一次性提交所有数据显示出最好的性能。但在这个例子中，应用程序语义可能会要求单独提交每年的数据。有时，其他的要求会阻碍我们获取最佳性能。

* ps1语句在员工数据表上建立了一个显式锁（explicit lock），这表示在该事务处理期间，其他事务不能访问该行数据。实现这一点的SQL语法不是标准的，你必须查阅你的数据库供应商提供的帮助文档，了解如何实现所需的锁定级别。常见的语法结构包含FOR UPDATE子句。这种锁被称为悲观锁（pessimistic locking）。它会主动阻止其他事务访问相关数据。

* 在数据库中，乐观并发是通过版本列实现的。当从某一行选择数据时，选择的数据必须包含预期的数据和版本列。例如要选择关于我的信息，可以使用下面的SQL语句

* 因此，不自动重试是一件好事（而且往往是唯一可用的解决方案），但这确实也意味着应用程序要负责处理异常。应用程序可以重试事务（也许一次或两次），可以提示用户输入不同的数据，也可以简单地通知用户操作失败。不存在一个放之四海而皆准的答案。

* 这里的问题是，这400 896行数据存放在哪里。如果在调用executeQuery()时返回整个数据集，那么应用程序会在堆中保存大量活跃数据，这可能会引发GC或其他问题。相反，如果只在调用next()方法时返回一行数据，那么在处理结果集的过程中，应用程序和数据库之间会产生大量的往返流量。


### 11.3 JPA

* 写入更少的字段
优化数据库写操作的一种常见方法是，只写入已经改变的字段。在HR系统中，用来让我的工资翻倍的代码可能需要从我的员工记录中检索20个字段，但需要写回数据库的字段只有一个（这非常重要）。
JPA实现应该能够显式地完成这种优化。这也是JPA的字节码必须增强的原因之一。通过这个过程，JPA供应商可以跟踪代码中的值何时被改变。当JPA代码得到适当的增强后，将我的双倍工资写回数据库的SQL语句会只更新有变化的那一行信息。

* 预热测试
Java性能测试，尤其是基准测试，通常都有预热期。正如第4章所讨论的那样，这使得编译器能够以最佳的方式编译代码。
还有一个例子可以表明预热期是有益的。在JPA应用程序的预热期，使用最频繁的实体会被加载到L2缓存中。随着这些实体被预先加载，在测试的测量期，性能会有很大不同。上一个例子尤其如此，它在加载实体时没有使用查询，不过这一般也是本节所有表中第一次执行和第二次执行之间的区别。


### 11.5 小结

* 适当地优化JDBC和JPA对数据库的访问是影响中间层应用程序性能最重要的方法之一。请记住以下这些最佳实践。
·尽可能通过适当地配置JDBC或JPA来进行批量读写。
·优化应用程序执行的SQL语句。对于JDBC应用程序，这是基本的标准SQL命令的问题。对于JPA应用程序，一定要考虑L2缓存。
·尽量减少锁的使用。当数据不太可能产生竞争的时候，使用乐观锁。当数据存在竞争的时候，使用悲观锁。
·确保使用预处理语句池。
·确保使用适当大小的连接池。
·设置合理的事务范围。它应该尽可能大，这样才不会对应用程序的可扩展性产生负面影响，因为事务在处理过程中会持有锁。


### 12.1 字符串

* ·字符串去重
最简单的方式是让JVM找到重复字符串后去重（deduplicate）。整理所有的引用，让它们指向单一的副本，并释放剩余的副本。这只有在使用G1 GC并指定-XX:+UseStringDeduplication标志（默认是false）时才能做到。这个特性存在于20版本之后的Java 8和所有版本的Java 11中。
这个特性默认情况下没有开启，原因有三个。第一，它需要在G1 GC的新生代回收和混合回收阶段进行额外的处理，这会延长回收的时间。第二，它需要一个额外的线程与应用程序线程同时运行，这可能会占用应用程序线程的CPU周期。第三，如果重复字符串很少，那么应用程序的内存使用量会更高（而不是更低），这些额外占用的内存来自于跟踪所有字符串以寻找重复而产生的簿记工作。

* 将字符串表大小设置得过高所带来的损失是非常小的。每个桶仅占用8字节，所以比最优值多几千个条目也只需要几千字节的原生（非堆）内存。

* 1Chacun à son goût是歌剧爱好者说YMMV（your mileage may vary，你的情况可能会有所不同）的意思。

* ·字符串的单行连接会产生良好的性能。
·对于多次连接操作，一定要使用StringBuilder。
·在JDK 11中重新编译后，某些类型的字符串单行连接的速度会大幅加快。


### 12.2 缓冲I/O

* ·关于缓冲I/O的问题很常见，这是由简单输入输出流类的默认实现引发的。
·对于文件和套接字，还有像压缩和字符串编码这样的内部操作，必须适当地对I/O进行缓冲。


### 12.3 类加载

* 对于任何试图在动态系统中优化程序启动速度或者新代码部署速度的人来说，类加载的性能都是痛苦之源。
这有很多原因。首要原因是，类数据（Java字节码）通常不能被快速访问。该数据必须从磁盘或网络加载，必须能在类路径上的某个JAR文件中找到，还必须能在某个类加载器中找到。有一些方法可以帮助解决这个问题。一些框架将它们从网络上读取的类缓存到一个隐藏目录中，这样下次启动同一应用程序时，就可以更快地读取这些类。将一个应用程序打包成较小的JAR文件也可以提升它的类加载性能。

* 加快类加载速度的最佳方法是，为应用程序创建一个类数据共享存档。幸运的是，这不需要修改代码。


### 12.4 随机数

* Random类和ThreadLocalRandom类的区别在于，Random类的主要操作（nextGaussian()方法）是同步的。该方法会被任何检索随机数的方法所使用，因此无论如何使用随机数生成器，锁上都会产生竞争。如果两个线程同时使用同一个随机数生成器，那么其中一个线程不得不等待另一个线程完成操作。这就是有线程局部版本ThreadLocalRandom的原因，当每个线程都有自己的随机数生成器时，Random类的同步就不再是问题。（正如第7章所讨论的那样，线程局部版本还有显著的性能优势，它会重用创建成本很高的对象。）

* 这两个类和SecureRandom类的区别在于使用的算法。Random类（以及继承自它的ThreadLocalRandom类）实现了一个典型的伪随机算法。这个算法相当复杂，但它是确定性的。如果知道初始种子，就可以确定该引擎将生成的确切数字序列。这意味着黑客能够通过特定生成器查看这个数字序列，并最终算出下一个数字会是什么。虽然好的伪随机数生成器可以生成一系列看起来非常随机的数字（甚至符合随机性的概率预期），但它们并不能真正做到随机。
而SecureRandom类使用一个系统接口来为其随机数据获取种子。该数据的生成方式和操作系统有关，但一般来说，这个源提供的数据基于真正的随机事件（如鼠标的移动）。这被称为基于熵的随机性（entropy-based randomness）。对于依赖随机数的操作来说，它更安全。

* 然而，更好的解决方案是设置操作系统，使其提供更多的熵，这可以通过运行rngd守护进程来实现。不过需要确保rngd守护进程配置使用了可靠的硬件熵源（例如/dev/hwrng，如果可用的话），而不是使用了像/dev/urandom这样的熵源。这个解决方案的好处是可以解决机器上所有应用程序的熵问题，而不仅仅是Java应用程序。


### 12.5 Java原生接口

* 当数组被固定时，垃圾回收器无法运行。所以JNI代码中开销最大的错误之一就是在长期运行的代码中固定字符串或数组。这会阻碍垃圾回收器运行，实际上JNI代码完成前，也阻塞了所有的应用程序线程。固定数组这部分很关键，尽可能缩短固定时间十分重要。
通常，你会在GC日志中看到GC Locker Initiated GC。这表示垃圾回收器需要运行，但无法运行，因为某个线程在JNI调用中固定了数据。只要该数据取消固定，垃圾回收器就可以运行了。如果你经常看到这个GC提示，请让JNI代码变得更快，你的其他应用程序线程因等待GC运行已经延迟了。
有时，让对象固定较短时间这个目标，与减少跨越JNI边界的调用这个目标相冲突。在这种情况下，前一个目标更为重要，即使这意味着要进行多次跨越JNI边界的调用，所以要让固定数组和字符串的时间尽可能短。


### 12.6 异常

* JDK的某些API在处理异常时可能导致性能问题。很多集合类在检索不存在的条目时会抛出异常。例如Stack类，如果调用pop()方法时栈是空的，它就会抛出EmptyStackException。在这种情况下，最好先利用防御性编程检查栈的长度。（另外，和许多集合类不同，Stack类支持null对象，所以pop()方法返回null并不能说明栈是空的。）
关于异常的不当使用，JDK中最臭名昭著的例子发生在类加载的过程中。当使用ClassLoader类的loadClass()方法加载某个找不到的类时，它会抛出ClassNotFoundException，但这其实并不是一种异常情况。不能期望单个类加载器知道如何加载应用程序中的每个类，这就是为什么类加载器有层次之分。


### 12.7 日志

* 日志是让性能工程师爱恨交加的功能之一。每当被问到为什么应用程序运行得很慢时，我首先会要求提供任何可用的日志，希望应用程序产生的日志有线索说明它正在做什么。每当被要求审查工作代码的性能时，我都会立即建议关闭所有的日志语句。

* 在代码中引入日志时要牢记的第三个原则是，即使没有开启日志，也很容易在无意间写出具有副作用的日志代码。这是另一个例子，证明“过早”优化代码是件好事。正如第1章的例子所示，只要记录的信息中包含方法调用、字符串连接或任何其他的分配操作（例如，为MessageFormat的参数分配Object数组），都要记得使用isLoggable()方法。


### 12.8 Java集合API

* 不幸的是，在Java的早期版本中，同步，甚至是没有竞争的同步，有很大的性能问题，所以当第一个重大修订版本发布时，集合框架采取了相反的做法：所有新的集合类默认都是非同步的。虽然那时同步性能已经有了很大的提升，但它仍然有开销，而非同步集合可以让每个人都写出更快的程序（偶尔也会出现因并发修改非同步集合导致的bug）。

* 为了让性能损失最小化，请确保在构建集合时尽可能准确地估计其最终大小。


* 当大多数开发人员被问到如何快速地对任意数组排序时，他们会将快速排序作为答案。好的性能工程师会想知道数组的大小，如果数组足够小，最快的排序方式是使用插入排序。4数组大小至关重要。

* 4快速排序的实现通常会对小数组使用插入排序。就Java而言，Array.sort()方法的实现呈现出来的是，对于任何少于47个元素的数组，使用插入排序比使用快速排序更快。


### 12.9 Lambda和匿名类

* 关于Lambda的性能，最基本的问题是，和它们的替代匿名类相比，它们的性能如何。结果表明，两者差别不大。

* Lambda和匿名类加载
此差异会显现出来的一种边界情况，发生在启动和类加载的过程中。在看到Lambda的代码时，你可能会认为，它是创建匿名类的语法糖。但这并不是它的工作原理，Lambda的代码会创建一个静态方法，该方法通过一个特殊的帮助类来调用。匿名类是真正的Java类，它有单独的类文件，并会通过类加载器加载。
正如你在本章前几节看到的那样，类加载的性能很重要，特别是在类路径很长的时候。所以，启动有很多匿名类（相对于有很多Lambda）的应用程序，可能会出现比这里更大的差异。


### 12.10 流和过滤器的性能

* 流最重要的性能优势是，它们被实现为延迟处理的数据结构。

* 因此，过滤器比迭代器快得多的一个原因是，它们可以进行算法上的优化。延迟处理的过滤器实现可以在完成需要做的事情后结束处理，这样处理的数据更少。


### 12.11 对象序列化

* 一般来说，降低对象序列化成本的方法是序列化更少的数据。这可以通过将字段标记为transient来做到，标记后它们默认不会被序列化。类可以提供特定的writeObject()方法和readObject()方法来处理这些数据。如果不需要某些数据，将其标记为transient就足够了。


### 附录 调优标志总结

* 表A-1：优化即时编译器的标志


### 关于作者

* 斯科特•奥克斯（Scott Oaks）是Oracle公司架构师，从事Oracle云平台软件的性能优化工作。在加入Oracle之前，他是Sun公司的Java布道师，并在2001年加入Java性能工程小组，专注于Java的性能优化工作。除了本书，他还著有多部涉及Java线程、Java安全等方面的著作。

