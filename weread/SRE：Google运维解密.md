## SRE：Google运维解密

 **孙宇聪**


### 序言

* Margaret 曾经说过：“无论对一个软件系统运行原理掌握得多么彻底，也不能阻止人犯意外错误。”在这次危机过后，Margaret 之前提交的修改申请很快就被批准了。


### 第1章 介绍

* 极端来说，研发部门想要：“随时随地发布新功能，没有任何阻拦”，而运维部门则想要：“一旦一个东西在生产环境中正常工作了，就不要再进行任何改动。”由于两个部门使用的语境不同，对风险的定义也不一致。在现实生活中，公司内部这两股力量只能用最传统的政治斗争方式来保障各自的利益。运维团队常常宣称，任何变更上线前必须经过由运维团队制定的流程，这有助于避免事故的发生。例如：运维团队会列出一个非常长的检查清单，历数所有以前曾经出现过的生产事故，要求研发团队在上线任何功能之前必须将所有这些事故模拟一遍，确保不会重现。这个清单通常没有任何标准，每项事故的可重现程度、问题价值并不一定是一致的。而开发团队吃过苦头之后也很快找到了自己的应对办法：开发团队宣称他们不再进行大规模的程序更新，而是逐渐转为功能开关调整、增量更新，以及补丁化。采用这些名词的唯一目的，就是为了绕过运维部门设立的各种流程，从而能更快地上线新功能。

* SRE 处理运维工作的一项准则是：在每8～12小时的on-call 轮值期间最多只处理两个紧急事件。这个准则保证了on-call工程师有足够的时间跟进紧急事件，这样SRE可以正确地处理故障、恢复服务，并且要撰写一份事后报告。如果一次轮值过程中处理的问题过多，那么每个问题就不可能被详细调查清楚，运维工程师甚至没有时间从中学习。如果小规模部署下还无法做到合理报警，规模扩大之后这个情况就会更严重。相对而言，如果一个项目的紧急警报非常少，能够持续稳定运行，那么保持这么多on-call工程师可能就是浪费时间。

* “错误预算”起源于这样一个理念：任何产品都不是，也不应该做到100% 可靠（显然这并不适用于心脏起搏器和防抱死刹车系统等）。一般来说，任何软件系统都不应该一味地追求100% 可靠。因为对最终用户来说，99.999% 和 100% 的可用性是没有实质区别的（详见附录A）。从最终用户到服务器之间有很多中间系统（用户的笔记本电脑、家庭WiFi、网络提供商和输电线路等），这些系统综合起来可靠性要远低于 99.999%。所以，在99.999% 和 100%之间的区别基本上成为其他系统的噪声。就算我们花费巨大精力将系统变为100% 可靠也并不能给用户带来任何实质意义上的好处。

* 监控系统是SRE团队监控服务质量和可用性的一个主要手段。所以，监控系统的设计策略值得着重讨论。最普遍的和传统的报警策略是针对某个特定的情况或者监控值，一旦出现情况或者监控值超过阈值就触发E-mail警报。但是这样的报警策略并不是非常有效：一个需要人工阅读邮件和分析警报来决定目前是否需要采取某种行动的系统从本质上就是错误的。监控系统不应该依赖人来分析警报信息，而是应该由系统自动分析，仅当需要用户执行某种操作时，才需要通知用户。

* 任何需要人工操作的事情都只会延长恢复时间。一个可以自动恢复的系统即使有更多的故障发生，也要比事事都需要人工干预的系统可用性更高。当不可避免地需要人工介入时，我们也发现与“船到桥头自然直”的态度相比，通过事先预案并且将最佳方法记录在“运维手册（playbook）”上通常可以使MTTR 降低3倍以上。初期几个万能的工程师的确可以解决生产问题，但是长久看来一个手持“运维宝典”经过多次演习的on-call工程师才是正确之路。虽然不论多么完备的“运维手册”也无法替代人的创新思维，但是在巨大的时间压力和产品压力下，运维手册中记录的清晰调试步骤和分析方法对处理问题的人是不可或缺的。因此，Google SRE将大部分工作重心放在“运维手册”的维护上，同时通过“Wheel ofMisfortune”等项目[2]不断培训团队成员。

* 软件系统一般来说在负载上升的时候，会导致延迟升高。延迟升高其实和容量损失是一样的。当负载到达临界线的时候，一个逐渐变慢的系统最终会停止一切服务。换句话说，系统此时的延迟已经是无穷大了。SRE的目标是根据一个预设的延迟目标部署和维护足够的容量。SRE和产品研发团队应该共同监控和优化整个系统的性能，这就相当于给服务增加容量和提升效率了。[3]


### 第2章 Google 生产环境：SRE视角

* 如果一个任务实例资源使用超出了它的分配范围，Borg会杀掉这个实例，并且重启它。我们发现，一个缓慢的不断重启的实例要好过一个永远不重启一直泄露资源的实例。


### 第3章 拥抱风险

* 下面这个例子说明了一个提供私人信息的系统中自然发生的完全和部分服务中断的区别。假设有一个联系人管理应用程序，一种情况是导致用户头像显示失败的间歇性故障，另一种情况是将A用户的私人联系列表显示给B用户的故障。第一种情况显然是一个糟糕的用户体验，SRE会努力去快速地解决这个问题。然而，在第二种情况下，暴露私人数据的风险可能会破坏基本的用户信任。因此，在第二种情况下，在进行调试和事后的数据清理时，完全停止该服务更加恰当。

* 在Bigtable的例子中，我们可以构建两个集群：低延迟集群和高吞吐量集群。低延迟集群是为那些需要延迟较低和可靠性较高的服务而设计的。为了确保队列长度最小和满足更严格的客户端隔离要求，Bigtable系统可以配备更多的冗余资源。另一方面，高吞吐量集群的配置冗余度较低，利用率较高。在实践中，高吞吐量集群只有低延迟集群成本的10%～50%。由于Bigtable是广泛部署的系统，这种成本上的降低非常显著。

* 通常，现有团队已经找到了某种非正式的风险与成本的平衡点。不幸的是，人们很少能证明这种平衡是最佳的，而不是单单由参与谈判的工程师的谈判能力决定的。这些决定也不应该受办公室政治、不理智的恐惧或一厢情愿的希望所驱使（事实上，Google SRE的非官方座右铭就是“希望不是一种策略”）。相反，我们的目标是定义一个双方都同意的客观指标，该指标可以用来指导谈判的方向。一项决策越是基于数据做出的，常常就越好。


### 第4章 服务质量目标

* 选择一个合适的SLO是非常复杂的过程。第一个困难点是很有可能无法确定一个具体的值。例如对外部传入的HTTP请求来说，每秒查询数量（QPS）指标是由用户决定的，我们并不能针对这个指标设置一个SLO。

* 我们在这里采用了一个很有趣的解决办法：SRE保证全球Chubby服务能够达到预定义的SLO，但是同时也会确保服务质量不会大幅超出该SLO。每个季度，如果真实故障没有将可用性指标降低到SLO之下，SRE会有意安排一次可控的故障，将服务停机。利用这种方法，我们可以很快找出那些对Chubby全球服务的不合理依赖，强迫服务的负责人尽早面对这类分布式系统的天生缺陷。

* 我们不应该将监控系统中的所有指标都定义为SLI；只有理解用户对系统的真实需求才能真正决定哪些指标是否有用。指标过多会影响对那些真正重要的指标的关注，而选择指标过少则会导致某些重要的系统行为被忽略。一般来说，四五个具有代表性的指标对系统健康程度的评估和关注就足够了。

* 所有的系统都应该关注：正确性。是否返回了正确的回复，是否读取了正确的数据，或者进行了正确的数据分析操作。正确性是系统健康程度的一个重要指标，但是它更关注系统内部的数据，而不是系统本身，所以这通常不是SRE直接负责的。

* 某些指标的汇总看起来是很简单的，例如每秒服务请求的数量，但是即使这种简单度量也需要在某个度量时间范围内进行汇总。该度量值是应该每秒获取一次，还是每分钟内的平均值？后者可能会掩盖仅仅持续几秒的一次请求峰值。假设某个系统在偶数秒处理200个请求，在其他时间请求为0。该服务与持续每秒处理100个请求的服务平均负载是一样的，但是在即时负载上却是两倍。同样的，平均请求延迟可能看起来很简单，但是却掩盖了一个重要的细节；很可能大部分请求都是很快的，但是长尾请求速度却很慢。

* 我们应该从思考（或者调研）用户最关心的方面入手，而非从现在能度量什么入手。用户真正关心的部分经常是度量起来很困难，甚至是不可能的，所以我们需要以某种形式近似。然而，如果我们只是从可以简单度量的数值入手，最终的SLO的作用就会很有限。因此，与其选择指标，再想出对应的目标，不如从想要的目标反向推导出具体的指标。

* 了解系统的各项指标和限制非常重要，但是仅仅按照当前系统的标准制定目标，而不从全局出发，可能会导致团队被迫长期运维一个过时的系统，没有时间去推动架构重构等任务。
保持简单

* 虽然要求系统可以在没有任何延迟增长的情况下无限扩张，或者“永远”可用是很诱人的，但是这样的要求是不切实际的。就算有一个系统能够做到这一点，它也需要花很长时间来设计和构建，同时运维也很复杂—最关键的是，这可能比用户可以接受的（甚至是很开心地接受的）标准要高太多。

* 理解系统行为与预期的符合程度可以帮助决策是否需要投入力量优化系统，使其速度更快、更可用，或者更可靠。如果服务一切正常，可能力量应该花在其他的优先级上，例如消除技术债务、增加新功能，或者引入其他产品等。


### 第5章 减少琐事

* 如果系统正常运转中需要人工干预，应该将此视为一种Bug。
“正常”的定义会随系统的进步而不断改变。


### 第6章 分布式系统的监控

* 监控台页面（dashboard）提供某个服务核心指标一览服务的应用程序（一般是基于Web的）。该应用程序可能会提供过滤功能（filter）、选择功能（selector）等，但是最主要的功能是用来显示系统最重要的指标。该程序同时可以显示相应团队的一些信息，包括目前工单的数量、高优先级的Bug列表、目前的on-call工程师和最近进行的生产发布等。

* 紧急警报的处理会占用员工的宝贵时间。如果该员工正在工作时间段，该警报的处理会打断他原本的工作流程。如果该员工正在家，紧急警报的处理则会影响他的个人生活，甚至是把他从睡眠中叫醒。当紧急警报出现得太频繁时，员工会进入“狼来了”效应，怀疑警报的有效性甚至忽略该警报，有的时候在警告过多的时候甚至会忽略掉真实发生的故障。由于无效信息太多，分析和修复可能会变慢，故障时间也会相应延长。高效的警报系统应该提供足够的信息，并且误报率非常低。

* 虽然平时看看流量图表可能很有意思，但是SRE团队通常会小心地避免任何需要某个人“盯着屏幕寻找问题”的情况。

* 这里应该注意，在一个多层系统中，某一个服务的现象是另外一个服务的原因。例如，数据库性能问题。数据库读操作很缓慢是数据库SRE检测到的一个现象。然而，对前端SRE来说，他们看到的是网站缓慢，数据库读操作的缓慢则是原因。因此，白盒监控有时是面向现象的，有时是面向原因的，这取决于白盒系统所提供的信息。

* 监控系统的4个黄金指标分别是延迟、流量、错误和饱和度（saturation）。如果我们只能监控用户可见系统的4个指标，那么就应该监控这4个。

* 如果我们度量所有这4个黄金指标，同时在某个指标出现故障时发出警报（或者对于饱和度来说，快要发生故障时），能做到这些，服务的监控就基本差不多了。

* 区分平均值的“慢”和长尾值的“慢”的一个最简单办法是将请求按延迟分组计数（可以用来制作直方图）：延迟为0～10ms之间的请求数量有多少，30～100ms之间，100～300ms之间等。将直方图的边界定义为指数型增长（这个例子中倍数约为3）是直观展现请求分布的最好方式。

* 复杂是没有止境的。就像任何其他软件系统一样，监控系统可能会变得过于复杂，以至于经常出现问题，变更非常困难，维护起来难度很大。

* 每个紧急警报的回复都应该需要某种智力分析过程。如果某个紧急警报只是需要一个固定的机械动作，那么它就不应该成为紧急警报。

* 从这种角度出发，我们可以得出以下结论：如果某个紧急警报满足上述四点，那么不论是从白盒监控系统还是黑盒监控系统发出都一样。最好多花一些时间监控现象，而不是原因。针对“原因”来说，应该只监控那些非常确定的和非常明确的原因。


### 第7章 Google 的自动化系统的演进

* 对于SRE而言，自动化是一种力量倍增器，但不是万能药。当然，对力量的倍增并不能改变力量用在哪的准确性：草率地进行自动化可能在解决问题的同时产生出其他问题。因此，虽然我们认为在大多数情况下以软件为基础的自动化是优于手动操作的，但是比这两个选择更好的方案是一个不需要这些的系统设计— 一个自治的系统。或者换一种方式来看，自动化的价值不仅来源于它所做的事情，还包括对其的明智应用。我们将讨论自动化系统的价值，以及SRE对自动化系统的态度的演进历史。

* 在行业内普遍认同的是，在产品生命周期中一个问题越晚被发现，修复代价越高；参见第17章。一般来说，解决实际生产中出现的问题是最昂贵的，无论是时间还是金钱方面，这意味着，构建一个在问题发生之后马上应对的自动化系统，对于降低系统的总成本非常有利，前提是该系统足够大。

* 最后，节省时间是一个经常被引用的使用自动化的理由。虽然大家经常选择这个依据来支持自动化，但是在很多情况下这种优势不能立即计算出来。工程师对于一个特定的自动化或代码是否值得编写而摇摆不定，不停地比较写该代码所需要花费的精力与不需要手动完成任务所节省的精力。[15]这里很容易忽略的一个事实是，一旦你用自动化封装了某个任务，任何人都可以执行它们。因此，时间的节省适用于该自动化适用的所有人。将某个操作与具体操作的人解耦合是很有效的。

* 如果我们持续产生不可自动化的流程和解决方案，我们就继续需要人来进行系统维护。如果我们要雇佣人来做这项工作，就像是在用人类的鲜血、汗水和眼泪养活机器。这就像是一个没有特效但是充满了愤怒的系统管理员的Matrix世界。

* 广泛使用的工具有Puppet、Chef、cfengine，甚至 Perl都提供了自动化完成特定任务的方法，主要区别在于对帮助进行自动化的组件的抽象层次不同。Perl这种完整的语言提供了POSIX级别的接口，理论上在系统API层面上提供了一个基本上是无限的扩展范围。[17]而Chef和Puppet则提供了一些“开箱即用”的抽象层，通过对这些抽象层的操作可以直接操作服务或者其他高级对象。这里的妥协十分经典：高层次的抽象更容易管理和进行逻辑推理，但当你遇到一个“有漏洞的抽象”时，就会系统地、重复地甚至不一致地出现故障。例如，我们经常假设，将一个新的二进制文件发布到集群中是原子性的；该集群最终会全部变成新版本，或者全部维持旧版本。然而，现实中的行为很复杂：集群网络中途可能会发生故障；物理机可能发生故障；与集群管理层的通信可能会失败，使系统进入不一致的状态；视具体情况不同，新的二进制文件可能安装了，但没有推送，或者推送了但是没有启动，或者启动了但是无法验证。没有几个抽象模型能够成功地模拟这些结果，大部分模型都会中止并要求人工干预。而那些真正糟糕的自动化系统甚至都不会这样做。

* 虽然通过自动化，我们可以在一个每周强制两次重启的世界里仍然保证MySQL的高可用，但是这也带来了它自己的一套成本，所有的应用程序都必须增加很多错误处理逻辑。由于MySQL开发世界中的常态是假设MySQL实例是整个技术栈中最稳定的成分，这种改变意味着我们要定制类似JDBC这样的软件，以便对我们的故障环境更加宽容。然而，与决策者一起迁移到MoB上的益处对这些成本来说是很值得的。迁移到MoB之后，我们的团队在无聊的运维任务上花费的时间下降了95%。整个故障转移过程是自动化的，所以单个数据库任务中断不再给任何人发出紧急报警。

* 通过消除运维相关服务的团队维护和运行自动化代码的责任，我们创造了一个不合理的组织激励机制：
● 某个最主要任务是加速现存的集群上线的团队是没有动力去减少服务运维团队在生产流程后期运维服务产生的技术负债的。
● 一个不亲自运行自动化的团队是没有动力去建设一个能够很容易自动化的系统的。
● 一个产品经理的时间表如果不受低质量的自动化影响，他将永远优先新功能的开发，而不是简化和自动化。

* 此功能最终使得连续性的、自动的操作系统升级只需要很少的持续[19]工作—这种工作不会根据生产部署的总规模而增加。当机器状态略有偏差时，可以自动进行修补；对SRE来说，机器的损坏和生命周期管理基本上已经不需要任何操作了。成千上万的机器加入系统，或者出现问题，被修复，这一切都不需要SRE的任何操作。回应 Ben Treynor Sloss 曾说过的话：通过将问题看作是一个软件问题的方法，最初的自动化努力给我们争取了足够的时间，把集群管理转化为自治系统，而不是自动化系统。我们通过相关的数据分布、API、集中式架构，以及经典的分布式软件系统研发进入了基础设施管理领域。

* 很久以前，负责某个特定机架退役的自动化系统出现了问题，但只有磁盘擦除步骤已经成功完成。随后，清退过程被重启，以调试为什么失败。在这次操作中，当该程序试图给磁盘清除系统发送机架中的机器列表时，代码得出需要进行清空的机器列表是（正确的）空的。不幸的是，空列表有着特殊含义，它被解释为“所有”。这意味着自动化系统几乎将colo中的所有机器都送去进行磁盘清除了。

* 几分钟内，高效的磁盘清除程序擦除了我们的CDN上所有机器的磁盘，这些机器再也不能终止用户连接了（或做任何有用的事情）。我们仍然可以从我们自己的数据中心来服务全体用户，事实上几分钟后对外部的唯一影响就是会有轻微的延迟增加。据我们所知，由于良好的容量规划，很少有用户注意到这个问题（至少我们这一点做得不错！）。接下来，我们花费了大概两天时间重装受影响的colo机架上的机器；然后我们又将接下来的几周时间用来进行代码审计，在我们的自动化系统中添加更多的合理性检查，包括速率限制，以及使整个退役流程具有幂等性。


### 第8章 发布工程

* 我们的目标是让部署流程与服务的风险承受能力相结合。在开发环境或者预生产环境中，我们可能会每小时构建一次，同时在所有测试通过之后自动发布更新。对于大型面向用户的服务来说，我们可能会先更新一个集群，再以指数速度更新其他集群直到全部完成。对敏感的基础设施服务来说，我们可能会将发布扩展到几天内完成，根据这些实例所在的地理位置交替进行。


### 第9章 简单化

* 软件系统本质上是动态的和不稳定的。[25]只有真空中的软件系统才是永远稳定的。如果我们不再修改代码，就不会引入新的Bug。如果底层硬件或类库永远不变，这些组件也就不会引入Bug。如果冻结当前用户群，我们将永远不必扩展系统。事实上，一个对SRE管理系统的方法不错的总结是：“我们的工作最终是在系统的灵活性和稳定性上维持平衡。”

* 与生活中的其他东西不同，对于软件而言，“乏味”实际上是非常正面的态度！我们不想要自发性的和有趣的程序；我们希望这些程序按设计执行，可以预见性地完成商业目标。Google工程师Robert Muth曾说过，“与侦探小说不同，缺少刺激、悬念和困惑是源代码的理想特性。”生产环境中的意外是SRE最大的敌人。

* 极端地说，当你指望一个Web服务7×24可用时，在某种程度上，每一行新代码都是负担。SRE推崇保证所有的代码都有必须存在的目的的实践。例如，审查代码以确保它确实符合商业目标，定期删除无用代码，并且在各级测试中增加代码膨胀检测。

* 模块化的概念同样适用于数据格式。Google的Protobuf[27]的一个主要优势和设计目标就是创建一个同时向后和向前兼容的传输格式。


### 第10章 基于时间序列数据进行有效报警

* Google的监控系统经过10年的发展，从传统的探针模型（使用脚本测试，检查回复并且报警）与图形化趋势展示的模型已经演变为一个新模型。这个新模型将收集时间序列信息作为监控系统的首要任务，同时发展了一种丰富的时间序列信息操作语言，通过使用该语言将数据转化为图表和报警取代了以前的探针脚本。

* 近年来，监控系统也经历了一次类似神武纪生物系统大爆炸的爆发式增长：Riemann、Heka、Bosun、Prometheus 都是开源软件中与Borgmon 基于时间序列数据报警理念类似的系统。Prometheus[3]与Borgmon十分类似，尤其是当你对比其中的规则计算语法时。变量收集和规则计算的理念在所有这些项目中都很类似。希望借助这些工具，读者可以自行试验、部署本章内描述的一些想法。

* 每当Borgmon计算完成一条报警规则时，结果永远是真（true）或假（false）。如果结果为真，那么产生一条报警。经验证明，报警规则经常反复变动（flap，快速切换状态）。因此，每条报警规则都指定了一个最小持续时间值。只有当警报持续时间超过这个值的时候，才会发送警报。一般来说，这个周期至少被设置为两个计算周期，以确保偶尔一次的信息收集失败不会立刻触发报警。

* 但是，白盒监控并不能完全代表一个被监控系统的所有状态。完全依赖白盒监控，就意味着我们并不知道最终用户看到的是什么样。例如：白盒监控只能看到已经接收到的请求，并不能看到由于DNS故障导致没有发送成功的请求，或者是由于软件服务器崩溃而没有返回的错误。同时，报警策略也只包含了工程师能想到的错误情况。

* Google SRE团队通常利用探针程序（prober）解决了该问题。探针程序使用应用级别的自动请求探测目标是否成功返回。这些探针可以直接向报警管理服务发送Alert RPC，也可以由Borgmon收集其结果信息。探针程序还可以针对返回结果进行应用级别的校验。例如检查HTTP回复中的HTML结果。探针程序甚至可以将结果中的值取出作为timeseries暴露给Borgmon使用。SRE团队经常使用探针程序收集响应速度的直方图和回复大小，以便观察最终用户可见的性能指标。探针程序是一种检查测试模型和一些高级time-series创建功能的混合体。

* 保证监控系统的维护成本与服务的部署规模呈非线性相关增长是非常关键的。这是在SRE工作中不断重现的一个模式，SRE确保他们做的每一件事都能够扩展到更大的规模。


### 第11章 on-call轮值

* 现代理论研究指出，在面临挑战时，一个人会主动或非主动（潜意识）地选择下列两种处理方法之一（参见文献[Kah11]）：
● 依赖直觉，自动化、快速行动。
● 理性、专注、有意识地进行认知类活动。
当处理复杂系统问题时，第二种行事方式是更好的，可能会产生更好的处理结果，以及计划更周全的执行过程。

* 在这些压力释放的荷尔蒙的影响下，on-call工程师往往会选择反应性的、未经详细考虑过的操作，这些操作很容易导致“过度联想”现象的产生。“过度联想”是在on-call中非常容易产生的现象，举例来说，当on-call工程师收到本周内第4个同样报警信息时，很容易联想起前3次报警都是由于某个外部系统造成的虚假报警，于是很自然地将第4次报警也归类为虚假报警，从而没有认真处理，导致真实事故的发生。

* 在应急事件处理过程中，最理想的方法论是这样的：在有足够数据支撑的时候按步骤解决问题，同时不停地审视和验证目前所有的假设。

* 让on-call SRE知道他们可以寻求外部帮助，对减轻on-call压力也很有帮助。最重要的资源有：
● 清晰的问题升级路线。
● 清晰定义的应急事件处理步骤。
● 无指责，对事不对人的文化氛围（参见文献[Loo10]和[All12]）。

* 最后，在应急事件处理结束时，仔细评估哪些地方有问题，哪些地方做得好是非常关键的。而且应该采取措施避免再次发生同样的问题。SRE团队必须在大型应急事件发生之后书写事后报告，详细记录所有事件发生的时间线。这些事后报告都是对事不对人的，为日后系统性地分析问题产生的原因提供了宝贵数据。犯错误是不可避免的，软件系统应该提供足够的自动化工具和检查来减少人为犯错误的可能性（参见文献[Loo10]）

* 虽然给一个非常安静的系统on-call值班是很幸福的事情，但是当一个系统太稳定，或者SRE on-call的周期太长会发生什么呢？SRE团队运维压力不够也是一个不良现象。长时间不操作生产环境会导致自信心问题，包括自信心太强以及自信心不够。这些现象只有在下一次发生问题时，才会显现出来。
为了避免这种问题，应该控制SRE团队的大小，保证每个工程师每个季度至少参与oncall一次，最好两次。这样可以保证团队成员有足够的生产环境操作经验。“命运之轮”（见第28章）也是一种有助提高技能和共享知识的团队活动。同时，Google每年举办一次持续数天的全公司灾难恢复演习（DiRT），针对理论性和实际性的灾难进行演练（参见文献[Kir12]）。


### 第12章 有效的故障排查手段

* 值得警惕的是，理解一个系统应该如何工作并不能使人成为专家。只能靠调查系统为何不能正常工作才行。

* 系统正常，只是该系统无数异常情况下的一种特例。

* 从理论上讲，我们将故障排查过程定义为反复采用假设-排除手段的过程：[24]针对某系统的一些观察结果和对该系统运行机制的理论认知，我们不断提出一个造成系统问题的假设，进而针对这些假设进行测试和排除。

* ● 关注了错误的系统现象，或者错误地理解了系统现象的含义。这样会在错误的方向上浪费时间。
● 不能正确修改系统的配置信息、输入信息或者系统运行环境，造成不能安全和有效地测试假设。
● 将问题过早地归结为极为不可能的因素（例如认为是宇宙射线造成数据变化，虽然有可能发生，但是并不应该在解决问题初期做这个假设），或者念念不忘之前曾经发生过的系统问题，认为一旦发生过一次，就有可能再次发生。
● 试图解决与当前系统问题相关的一些问题，却没有认识到这些其实只是巧合，或者这些问题其实是由于当前系统的问题造成的。（比如发现数据库压力大的情况下，环境温度也有所上升，于是试图解决环境温度问题。）


* 最后，我们要记住，相关性（correlation）不等于因果关系（causation）。[28]一些同时出现的现象，例如集群中的网络丢包现象和硬盘损坏现象可能是由同一个原因造成的，比如说供电故障。但是网络丢包现象并不是造成硬盘损坏现象的原因，反之亦然。更糟的是，随着系统部署规模的不断增加，复杂性也在不断增加，监控指标越来越多。不可避免的，纯属巧合，一些现象会和另外一些现象几乎同时发生。[29]

* 在大型问题中，你的第一反应可能是立即开始故障排查过程，试图尽快找到问题根源。这是错误的！不要这样做。
正确的做法应该是：尽最大可能让系统恢复服务。这可能需要一些应急措施，比如，将用户流量从问题集群导向其他还在正常工作的集群，或者将流量彻底抛弃以避免连锁过载问题，或者关闭系统的某些功能以降低负载。缓解系统问题应该是你的第一要务。在寻找问题根源的时候，不能使用系统的用户并没有得到任何帮助。当然，快速定位问题时仍应该及时保存问题现场，比如服务日志等，以便后续进行问题根源分析时使用。

* 更进一步，你可能需要在日志系统中支持过滤条件——记录所有满足X的操作。X的范围很广，例如过滤 Set RPC 中内容长度小于1024字节的，或者是操作时间超过10 m s的，或者是调用了 rpc_handler.py中doSomethingInteresting（）函数的操作的。日志记录系统最好能够按需、快速、有选择启用。

* 现象：一个Spanner集群出现延迟过高的情况，RPC请求超时。
为什么：Spanner 服务器的任务实例把CPU占满了，无法处理用户发来的请求。
哪里：Spanner服务器的CPU消耗在了哪里？通过采样（profiling）得知该任务正在将日志记录排序，写入磁盘。
哪里：这段排序代码中的哪部分在消耗资源？是在一段正则表达式的计算过程中。
解决方案：重写该正则表达式，避免使用回溯（backtracking）。在代码中寻找类似的问题。考虑使用RE2，该类库保证不使用回溯，同时保障运行时间与输入呈线性关系。[33]

* 将你的想法明确地记录下来，包括你执行了哪些测试，以及结果是什么。[35]尤其是当你处理更加复杂的问题时，良好的文档可以让你记住曾经发生过什么，可避免重复执行。[36]如果你修改了线上系统，例如给某个进程增加了可用资源。系统化和文档化这些改变有助于将系统还原到测试前的状态，而不是一直运行在这种未知状态下。

* 一项试验中出现的负面结果是确凿的。这些结果确切地告诉了我们有关生产环境中的信息、设计理念对错或者现有系统的性能极限。这些负面结果能够帮助其他人决定他们的试验和设计是否值得一试。举例来说，某个研发团队可能决定不使用某个Web服务器，因为由于锁竞争的缘故只能处理800个并发连接，而不是需要的8000个。当下一个研发团队想要评测Web服务器时，他们可以直接利用之前的负面结果判断而不是从头开始。他们可以根据（a）需要的并发连接数少于800个（b）锁竞争问题已经被解决了来决定是否使用。

* 工具和方法可能超越目前的试验，为未来的工作提供帮助。例如，性能测试工具和负载生成器从成功或失败的试验过程中都可以产生。很多Web服务器管理人员都从Apache Bench，一个负载测试软件中获利。虽然第一个测试结果很可能是不尽如人意的。

* 当你最终确定了某个因素是问题根源时，应该将系统中出错的部分，你是如何定位问题的，和你是如何修复问题的，如何防止再次发生等写下来。这就是事后总结。希望这时候系统是活着的！（事后总结也称为验尸报告，但是这里是在问题解决后才写的，服务已经恢复。）

* 有很多方法可以简化和加速故障排查过程。可能最基本的是：● 增加可观察性。在实现之初就给每个组件增加白盒监控指标和结构化日志。● 利用成熟的、观察性好的组件接口设计系统。


### 第13章 紧急事件响应

* 首先，别惊慌失措！这不是世界末日，你也并不是一个人在战斗！作为一个专业人士，你已经接受过如何正确处理这样的情况的训练。通常来说，我们处理的问题一般不涉及真实的物理危险，只是计算机系统出现了问题。最差的情况下，也只是半个互联网都停止运转了（Google网络规模已经是全球排名前三）。所以请深吸一口气，慢慢来。

* 在灾难发生的一小时内，所有访问权限都得到了恢复，同时所有的服务都恢复了正常。这次测试带来的严重后果促使开发者对代码类库中的问题进行了一次彻底的修复，同时制定了一个周期性测试机制来保证这类严重问题不再重现。（此项事故应该是由于数据库连接类库中没有正确处理某个数据库失去响应的情况，导致了进程阻塞或者其他所有数据库也不能被正常访问）。

* 由于我们没有在测试环境中测试回滚机制，没有发现这些机制其实是无效的，导致了事故总时长被延长了。我们现在要求在大型测试中一定先测试回滚机制。

* 时间和经验一再证明，系统不但一定会出问题，而且会以没有人能够想到的方式出问题。Google学到的最关键的一课是，所有的问题都有对应的解决方案，虽然对一个面对着疯狂报警的工程师来说，它可能不是那么显而易见。如果你想不到解决办法，那么就在更大的范围内寻求帮助。找到更多团队成员，寻求更多的帮助，做你需要做的一切事情，但是要快。最高的优先级永远是将手头问题迅速解决。很多时候，触发这个事故的人对事故了解得最清楚，一定要充分利用这一点。
非常重要的是，一旦紧急事件过去之后，别忘了留出一些时间书写事后报告。


### 第14章 紧急事故管理

* 事故总控负责人最重要的职责就是要维护一个实时事故文档。该文档可以以wiki的形式存在，但是最好能够被多人同时编辑。大部分Google团队使用Google Docs，但是Google Docs 团队使用Google Sites做这件事：利用你正要修复的服务来修复该服务恐怕不是什么好主意。

* 先宣布事故发生，随后找到一个简单解决方案，然后宣布事故结束，要比在问题已经持续几个小时之后才想起流程管理更好。应当针对事故设立一个明确的宣布条件。Google团队依靠下面几个宽松的标准——如果下面任何一条满足条件，这次事故应该被及时宣布。
● 是否需要引入第二个团队来帮助处理问题？
● 这次事故是否正在影响最终用户？
● 在集中分析一小时后，这个问题是否依然没有得到解决？


### 第15章 事后总结：从失败中学习

* 在SRE的文化中，最重要的就是事后总结“对事不对人”。一篇事后总结必须重点关注如何定位造成这次事件的根本问题，而不是指责某个人或某团队的错误或者不恰当的举动。一篇对事不对人的事后总结假设所有参与事件处理的人都是善意的，他们在自己当时拥有的信息下做了正确的举动。如果因为某些“错误的”举动就公开指责或者羞辱某个人或团队，那么人们就会自然地逃避事后总结。

* 未经评审的事后总结还不如不写。为了保障每个写完的草稿都得到评审，我们鼓励定期举行评审会议。在这些会议上，我们应该注意着重解决目前文档中的疑问和评论，收集相关的想法，将文档完成。

* 在引入事后总结机制的时候，最大的阻力来源于对投入产出比的质疑。下面的策略可以帮助面对这些挑战：
● 逐渐引入事后总结流程。首先进行几次完整的和成功的事后总结，证明它们的价值，同时帮助确定具体书写事后总结的条件。
● 确保对有效的书面总结提供奖励和庆祝。不光通过前面提到的公开渠道，也应该在团队或个人的绩效考核中体现。
● 鼓励公司高级管理层认可和参与其中。Larry Page（Google创始人之一）经常称赞事后总结的价值之高


### 第16章 跟踪故障

* 甚至有的时候，我们需要人为制造一些宕机时间，以免给内部用户造成某种假象（通常指某些服务设计的架构已经决定发生故障会耗时很久才能解决，但是经常由于运气因素而造成非常稳定的假象。某些团队选择定期制造人为宕机时间，以避免用户过于依赖该服务）。


### 第17章 测试可靠性

* 通过独立的单元测试的软件组件被组装成大的系统组件。工程师通过在这个组件中运行一个集成测试（integration test）来检验该组件的功能的正确性。依赖注入（dependency injection），利用类似Dagger[53]这样的工具，我们可以创建出复杂依赖的mock（测试中替代真实逻辑的伪组件），用以方便地测试某个系统组件。一个常见的例子是通过依赖注入来用轻便的mock替换一个有状态的数据库，同时保持一模一样的行为特征。

* 另外一种系统测试可保证之前的Bug不会重现。回归测试可以被理解为曾经发生过的，导致系统故障或产生错误信息的Bug列表。通过将这些Bug记录为系统测试或者集成测试，重构代码的工程师可以保证他们不会偶然间将他们曾经辛苦调查和修复的Bug又带回来。

* 为了更好地控制不可预知性，同时避免最终用户受到影响，变更可能没有按照它们被加入源代码版本控制系统的顺序发布。发布过程经常按阶段进行，使用某种机制逐渐将用户切换到新系统上。同时，监控每个新阶段以保证新环境没有遇到任何未知的问题。所以，整个生产环境通常并不能用任何一个源代码版本控制系统的版本来代表。


* 为了安全地操作某个系统，SRE需要理解系统和组件的性能边界。在很多案例中，单独的组件在超过某个临界值时并不能优雅地降级，而是灾难性地失败。工程师使用压力测试来找到Web服务的性能边界。压力测试能够回答以下问题：
● 数据库容量满到什么程度，才会导致写请求失败。
● 向某应用每秒发送多少个请求，将导致应用过载并导致请求处理失败。

* 金丝雀测试并不真的是一个测试，而是一种结构化的最终用户验收测试。配置测试和压力测试可以测试在某种特定情况下的服务的特殊表现，而金丝雀测试更多的是比较随意的。这种测试将代码置于比较难以预测的生产环境的实时用户流量之下，看代码是否产生问题。因此，这种测试是不完美的，有的时候会漏掉某些Bug。

* 是否能够将源代码按重要程度分出优先级？用研发管理和项目管理的行话来说，如果所有任务都是高优先级的，那么它们就也全是低优先级的。是否可以将要测试的系统组件按重要度排序（用什么标准来衡量重要度都可以，关键要排序）？


* 要注意，不是所有的软件项目都可以平等对待。某些人身安全相关或者业务收入相关的系统相对一些非生产使用的脚本来说当然需要对应更高的测试质量和测试覆盖度。

* 更有意思的是，我们的自动化工具可能正在修改另一个自动化工具依赖的环境，或者这两个工具正在同时修改对方依赖的环境！例如，某个集群升级工具可能在推进更新的时候需要消耗集群资源。同时，某个容器平衡器将会意识到这个问题，试图将该工具迁移到其他机器上。有的时候当该集群升级工具试图升级容器平衡工具的时候，循环依赖就产生了。这种循环依赖可能并不是问题，只要API提供良好的重启机制，同时只要有人记得针对这种情况写测试就行了

* 如果我们让用户在一年中测试更多的版本，平均失败周期（MTBF）反而会下降，因为用户有更多的机会接触到有问题的版本。然而，我们可以从此得出哪些区域需要更多地进行测试。如果这些测试被建立起来，每项测试都可以确保问题未来不再重现。仔细的可靠性管理可以将对不确定性的限制（由代码测试覆盖度决定）与用户可见问题的控制结合起来，调节发布的节奏。这种组合可以最大化从运维中和实际用户那里得到的反馈信息。这些反馈信息可以更好地驱动测试覆盖度，从而驱动产品发布速度。

* 在另一方面，自动发布机制应该在理想情况下自动阻止更新，直到这种问题组合不再可能出现。另一方面，对应的研发团队也可以考虑暂时禁止某些问题实例接受请求，直到版本问题解决。

* 测试是工程师提高可靠性投入回报比最高的一种手段。测试并不是一种只执行一次或两次的活动，而是持久不断的。写出优质的测试需要付出的成本是很大的，建立和维护测试基础框架以推行强测试文化也是一样。在未充分理解一个问题之前，我们没法修复它，而在工程领域，了解一个问题的方法只有实际度量。本章提到的方法论和工具可以为更好地度量失败与不确定性提供一个坚实的基础，可以帮助工程师在编写和发布软件时，推演可靠性。



### 第18章 SRE部门中的软件工程实践

* 与这些工具的直接用户—其他SRE—的密切联系使得获取直接的和高质量的用户反馈变得很容易。向一个对问题和解决方案都很熟悉的内部团体发布工具可以让开发团队更快地进行迭代。内部用户一般对UI的不足和alpha版本的问题有很强的包容性。

* 从产品角度上来说，Auxon为收集基于意图的服务资源要求和依赖信息提供了工具。这些用户的意图通过一系列对服务的要求来表达，例如：“我的服务必须在每个大陆有N+2的冗余度”或者“前端服务器至多只能距离后端服务器 50ms”。Auxon将这些信息通过用户配置信息或者编程API收集起来，同时将这些人工指定的产品意图转化成机器可以使用的限制条件。这些需求条件可以指定优先级，这样在资源不够的情况下可以更好地分配资源。这些资源需求—也就是产品意图—最后会形成一个巨大的线性规划表达式。Auxon通过对该表达式求解，并且利用一系列组合的最优压缩算法最终形成一个资源分配计划。


* 在开发过程中，时刻从用户角度出发来设计对提高可用性很重要。使用工具的工程师可能没有时间和精力去通过源代码学习如何使用一个工具。虽然内部用户相对来说比较能够容忍界面上的不足之处，适当地提供文档还是有必要的。SRE平时的工作已经很忙了，如果你的解决方案使用起来太困难，或者太令人迷惑，SRE通常会选择自己开发了。

* 虽然SRE开发的软件项目主要服务于技术项目经理（TPM）和有技术能力的工程师，但任何一个足够创新的软件项目对用户来说都有一定程度的学习曲线。不要害怕在项目初期给新用户提供一定程度的帮助，以确保他们可以更好地使用该工具。有的时候自动化同时也会面对一定程度上的情绪挑战，比如有的时候用户会恐惧自己的工作被一个shell脚本所替代。通过和早期用户一对一地共同协作，我们可以更好地处理这些情绪问题，并且更好地展示这项工具的作用在于消除重复性手工劳动，而配置文件、修改的流程和最终结果仍由这些团队负责。后来的用户可以借鉴早期用户的成功经历。

* 一个项目的目标应该与整个组织的发展目标一致，这样技术领导层可以为该项目的影响力背书，从而在自己的团队内或者整个组织内宣传你的项目。组织内部的推广以及评审可以帮助避免相互冲突的项目或者重复的项目发生。同时，一个能够为整个组织服务的项目也会容易招募到工程师和支撑性资源。
什么样的项目是不合适的呢？和很多你在其他软件工程项目上能直观识别的红色警告一样，例如同时改变太多组件的项目，或者软件设计要求全有或全无的上线方式，无法采用迭代式开发的项目。因为Google SRE项目目前是以所服务的项目为核心进行组织的，SRE主导的开发项目非常容易过于专注某一个服务，而仅仅对整个Google的一小部分提供帮助。因为团队的关注点在于提高所服务产品的用户体验，SRE项目经常不能做到足够通用以被其他的SRE团队采用。另外一方面，过于通用的框架也会带来问题。如果某个工具太过专注于灵活或者通用，很可能无法针对任何一个具体案例产生足够的价值。如果一个项目目标太大、太抽象，很可能需要投入非常大的研发力量，但是却没有足够可靠的实际案例来证明自己的实用性

* 在开发软件的过程中，我们可能不可避免地想要走一些捷径。一定要抵制住这种诱惑，用要求产品研发团队的标准来要求自己，例如：
● 问自己这样的问题：如果这个产品是其他团队开发的，我们是否会考虑使用？
● 如果你的解决方案被广泛采用，它可能会成为SRE正常工作的关键工具。因此可靠性是很重要的，该项目是否有足够的代码评审？是否有端到端的集成测试？可以找另外一个SRE团队，让他们按常规服务生产交接的标准来评价你的产品。
软件工程项目建立可信度很慢，但是失去它却非常快。


### 第19章 前端服务器的负载均衡

* 我们先来讨论以下两个常见的用户流量场景：一个简单的搜索请求和一个视频上传请求。用户想要很快地获取搜索结果，所以对搜索请求来说最重要的变量是延迟（latency）。而对于视频上传请求来说，用户已经预期该请求将要花费一定的时间，但是同时希望该请求能够一次成功，所以这里最重要的变量是吞吐量（throughput）。两种请求用户的需求不同，是我们在全局层面决定“最优”分配方案的重要条件。

* 但是在局部层面，在一个数据中心内部，我们经常假设同一个物理建筑物内的所有物理服务器都在同一个网络中，对用户来说都是等距的。因此在这个层面上的“最优”分配往往关注于优化资源的利用率，避免某个服务器负载过高。

* 一个可能的解决方案是使用EDNS0 扩展协议（参见文献[Con15]），该协议在递归解析器发送的请求中包括了最终用户的子网段。这样权威服务器可以返回一个对最终用户来讲最优的回复。虽然这个协议还不是正式规范，但是这些明显的优势使得最大的DNS解析器（例如OpenDNS和Google[68]）已经开始采用了。

* 幸运的是，的确有一种替代方案。既不需要在内存中保存所有连接的状态，也不会在单个机器出故障时重置所有连接，这就是一致性哈希（consistent hashing）算法。1997年提出的一致性哈希算法（参见文献[Kar97]）描述了一种映射算法，在新的后端服务器被添加或者删除时保持相对稳定。这种算法在后端资源变化时，最小程度地减少了对现存连接的影响。最终结果是，我们平时可以使用简单的连接跟踪机制，但是在系统压力上升时切换为一致性哈希算法，例如在处理分布式拒绝服务攻击时（DDoS）。


### 第20章 数据中心内部的负载均衡系统

* 在理想情况下，某个服务的负载会完全均匀地分发给所有的后端任务。在任何一个时间点上，最忙和最不忙的任务永远消耗同样数量的CPU。


* 跛脚鸭状态
后端任务正在监听端口，并且可以服务请求，但是已经明确要求客户端停止发送请求。
当某个请求进入跛脚鸭状态时，它会将这个状态广播给所有已经连接的客户端。但是那些没有建立连接的客户端呢？在Google的RPC框架实现中，不活跃的客户端（没有建立TCP连接的客户端）也会定期发送UDP健康检查包。这就使跛脚鸭状态可以相对较快地传递给所有的客户端—通常在一到两个RTT周期内—无论它们处于什么状态下。

* 很多服务的请求处理所需要的资源是不同的。在实际工作中，我们发现在很多服务中，成本最高的请求相比成本最低的请求会多消耗1000倍（甚至更多）的CPU资源。在请求处理成本无法事先预知的情况下，轮询策略就更难均衡负载了。

* 处理物理服务器的差异—也就是不需要强同质性—是Google很长时间的一个难题。理论上来说，对待繁杂的资源容量的解决方案是很简单的：只要将CPU预留值根据CPU/物理服务器类型来倍增就行了。但是，在实际实践中，要实现这个方案，需要我们的任务编排系统跟踪某个任务平均机器性能的取样数据，同时在具体调度资源的时候将其考虑在内。例如，2CPU单位的物理服务器X（慢机器）等同于0.8 CPU 的物理服务器Y（快机器）。根据这个信息，任务编排系统可以根据某个进程所处机器的类型调节CPU预设值。为了将这个任务变得简单，我们创造了一个虚拟CPU单位（Google计算单元GCU）。我们用GCU来给CPU性能建模，同时维护一个GCU与不同CPU类型的性能映射表。

* 加权轮询策略通过在决策过程中加入后端提供的信息，是一个针对简单轮询策略和最闲轮询策略的加强。
加权轮询策略理论上很简单：每个客户端为子集中的每个后端任务保持一个“能力”值。请求仍以轮询方式分发，但是客户端会按能力值权重比例调节。在收到每个回复之后（包括健康检查的回复），后端会在回复中提供当前观察到的请求速率、每秒错误值，以及目前资源利用率（一般来说，是CPU使用率）。客户端根据目前成功请求的数量，以及对应的资源利用率成本进行周期性调节，以选择更好的后端任务处理。失败的请求也会记录在内，对未来的决策造成影响。


### 第21章 应对过载

* Google在多年的经验积累中得出：按照QPS来规划服务容量，或者是按照某种静态属性（认为其能指代处理所消耗的资源：例如某个请求所需要读取的键值数量）一般是错误的选择。就算这个指标在某一个时间段内看起来工作还算良好，早晚也会发生变化。有些变动是逐渐发生的，有些则是非常突然的（例如某个软件的新版本突然使得某些请求消耗的资源大幅减少）。这种不断变动的目标，使得设计和实现良好的负载均衡策略使用起来非常困难。

* 从负载均衡策略的角度看，重试请求和新请求是无法区分的。也就是说，我们并不会使用任何特殊的逻辑来保证某个重试请求真的会发往另外一个后端任务。在后端数量较多的情况下，这个“概率”本身就很大。增加新的逻辑确保请求发往不同的后端会将我们的API复杂度无谓地提高。

* 所以，我们认为保护某个具体任务，防止过载是非常重要的。简单地说：一个后端任务被配置为服务一定程度的流量，不管多少额外流量被指向这个任务，它都应该保证服务质量。并且由此得出，后端任务不应该在过载情况下崩溃。这些要求应该在一定流量范围内得到满足—有可能是两倍的配置流量，甚至10倍。我们可以接受超出某阈值时系统会崩溃的情况，但是应该将该阈值提升到某种很难发生的程度。


* 一个常见的错误是认为过载后端应该拒绝和停止接受所有请求。然而，这个假设实际上是与可靠的负载均衡目标相违背的。我们实际上希望客户端可以尽可能地继续接受请求，然后在有可用资源时才处理。某个设计良好的后端程序，基于可靠的负载均衡策略的支持，应该仅仅接受它能处理的请求，而优雅地拒绝其他请求。


### 第22章 处理连锁故障

* 如果请求没有成功，以指数型延迟重试。

* 为什么人们总是忘记增加一点点抖动因素呢？

* 一个糟糕透顶的场景：由于CPU资源减少，请求处理速度变慢，内存使用率上升，导致GC触发次数增多，导致CPU资源的进一步减少。我们将此称之为“GC死亡螺旋”。


* 同样的，这些软件服务器可能对负载均衡层来说是处于不健康状态，从而导致负载均衡可用容量的降低：软件服务器可能进入跛脚鸭状态或者无法处理健康检查。这种情况和软件服务器崩溃很类似，越来越多的软件服务器呈现不健康状态，每个仍健康的软件服务器都在很短的一段时间内接受了大量请求而进入不健康状态，导致能够处理请求的软件服务器越来越少。
会自动避免产生错误的软件服务器的负载均衡策略会将这个问题加剧—某几个后端任务产生了错误，会导致负载均衡器不再向它们发送请求，进而使得其余软件服务器的负载上升，从而再次触发滚雪球效应。

* 在队列中的排队请求消耗内存，同时使延迟升高。例如，如果队列大小是线程数量的10倍，而单个线程处理单个请求的耗时是100ms。如果队列处于满载状态，每个请求都需要1.1s才能处理完成，大部分时间都消耗在排队过程中。


* 一种简单的流量抛弃实现方式是根据CPU使用量、内存使用量及队列长度等进行节流。限制队列长度的详细讨论请参见上一小节“队列管理”。例如，一个有效的办法是当同时处理的请求超过一定量时，开始直接针对新请求返回 HTTP 503（服务不可用）。
另外的做法包括将标准的先入先出（FIFO）队列模式改成后入先出（LIFO），以及使用可控延迟算法（CodDel）（参见文献[Nic12]），或者类似的方式更进一步地避免处理那些不值得处理的请求（参见文献[Mau15]）。如果某个用户的Web搜索请求由于RPC排队因素已经等待了10s，很有可能该用户已经放弃了，已经刷新了浏览器又发送了一个新请求：这时回复第一个RPC请求已经没有任何意义，因为它已经没用了！这个策略和层层向下传递RPC截止时间的策略结合起来，工作得十分好，可参见本章后面的“请求延迟和截止时间”一节。

* ● 复杂的流量抛弃和优雅降级系统本身就可能造成问题—过于复杂的逻辑可能会导致软件服务器以外的服务进入降级模式运行，甚至进入反馈循环。设计时应该实现一种简单的关闭降级模式，或者是快速调节参数的方式。将这个配置文件存储在一个强一致性的存储系统（如 Chubby）中，每个软件服务器都可以订阅改变，可以提高部署速度，但是同时也会增加整个系统的同步性风险（如果配置文件有问题，全部服务器同时都会受到影响）。

* 这里为了描述这个场景，简化了一些细节，[75]但是这里很好地展现了重试是如何摧毁一个系统的。注意临时性的过载升高，或者使用量的缓慢增加都有可能造成这种情况。

* ● 一定要使用随机化的、指数型递增的重试周期。可参见文献[Bro15]中提到的文章Exponential Backoff and Jitter（http://www.awsarchitectureblog.com/2015/03/backoff.html）。如果重试不是随机分布在重试窗口里的，那么系统出现的一个小故障（某个网络问题）就可能导致重试请求同时出现，这些请求可能会逐渐放大（参见文献[Flo94]）。

* ● 从多个视角重新审视该服务，决定是否需要在某个级别上进行重试。这里尤其要避免同时在多个级别上重试导致的放大效应：高层的一个请求可能会造成各层重试次数的乘积数量的请求。如果服务器由于过载不能提供服务，后端、前端、JavaScript层各发送3次重试（总计4次请求），那么一个用户的动作可能会造成对数据库的64 次请求（43）。在数据库由于过载返回错误时，这种重试只会加重问题。

* 很多连锁故障场景下的一个常见问题是软件服务器正在消耗大量资源处理那些早已经超过客户端截止时间的请求。这样的结果是，服务器消耗大量资源没有做任何有价值的工作，回复已经被取消的RPC是没有意义的。

* ● 当使用按键值空间分布的某种共享资源时，应该考虑按键值分布限制请求数量，或者使用某种滥用跟踪系统。假设你的后端要处理来自不同客户端的性能和特征各异的请求，我们可以考虑限制一个客户端只能占用25% 的线程总数，以便在某个异常客户端大量产生负载的情况下提供一些公平性。

* 理解服务在高负载情况下的行为模式可能是避免连锁反应最重要的一步。知道系统过载时如何表现可以帮助确定为了修复问题所需要完成的最重要的工程性任务。最不济这种知识也能够在紧急情况下帮助on-call工程师处理故障。

* 如果我们压力测试一个有状态的服务，或者使用缓存的服务，压力测试代码应该记录多次交互的状态，同时检查高负载情况下的回复正确性，这通常是某些非常难以查找的并行Bug出现的时候。

* 取决于服务本身，我们可能无法直接控制所有的客户端代码。但是，理解大客户是如何与服务交互的还是很有益处的。


* 接下来，SRE书写了一篇事后总结，详细说明了触发问题的事件，哪些做得好，哪些可以做得更好，和一系列待办事项来避免这个情景重现。例如，在服务过载的情况下，GSLB负载均衡器可以将一些流量导入邻近的数据中心。同时，SRE团队启用了自动伸缩机制，于是任务的数量可以自动跟着流量增长，这样他们就不用再操心这类问题了。

* 当一个系统过载时，某些东西总是要被牺牲掉。一旦一个服务越过了临界点，服务一些用户可见错误，或者低质量结果要比尝试继续服务所有请求要好。理解这些临界点所在，以及超过临界点系统的行为模式，是所有想避免连锁故障的运维人员所必需的。
如果不加小心，某些原本为了降低服务背景错误率或者优化稳定状态的改变反而会让服务更容易出现事故。在请求失败的时候重试、负载自动转移、自动杀掉不健康的服务器、增加缓存以提高性能或者降低延迟：这些手段原本都是为了优化正常情况下的服务性能，但是也可能会提高大规模的服务故障的几率。一定要小心评估这些改变，否则灾难就会接踵而至。


### 第23章 管理关键状态：利用分布式共识来提高可靠性

* 该逻辑非常符合直觉：如果两个节点无法通信（因为网络出现了分区问题），那么整个系统要么在一个或多个节点上无法处理数据访问请求，要么可以照常处理请求，但是无法保障每个节点的数据具有一致性。

* 大多数支持BASE语义的系统都依赖于多主复制（multimaster replication）机制，在这种模式下写操作可以并行在多个进程上执行，通过某种机制（常常是简单的“最后操作为准”）来解决冲突。这种方式通常被称为最终一致。然而，最终一致可能会带来意想不到的问题（参见文献[Lu15]），尤其是当时钟漂移（在分布式系统中，这是不可避免的），或者网络分区（参见文献[Kin15]）发生的时候。[77]


* 系统设计师不能通过牺牲正确性来满足可靠性或者性能的要求，尤其是在处理关键数据时。例如，假设某个系统处理财务交易：可靠性和性能在最终结果不正确的情况下一文不值。系统必须能够可靠地在多个进程中同步关键状态。分布式共识算法就提供了这种功能。

* 事实上，很多分布式系统问题最后都归结为分布式共识问题的不同变种，包括领头人选举，小组成员信息，各种分布式锁和租约机制，可靠的分布式队列和消息传递，以及任何一种需要在多个进程中共同维护一致的关键状态的机制。所有这些问题都应该仅仅采用经过正式的正确性证明的分布式共识算法来解决，还要确保这个算法的实现经过了大量测试。任何一种临时解决这种问题的方法（例如心跳，以及谣言协议）在实践中都会遇到可靠性问题。

* 严格来讲，在有限时间内解决异步式分布式共识问题是不可能的。正如Dijkstra 的获奖文章—FLP impossibility result（参见文献[Fis85]）写的那样，在不稳定的网络条件下，没有任何一种异步式分布式共识算法可以保证一定能够达成共识。

* 在实际操作中，我们通过保证给系统提供足够的健康的副本，以及良好的网络连接状态来保障分布式共识算法在大多数情况下是可以在有限时间内达成共识的。同时整个系统还需要加入随机指数型延迟。这样，我们可以保障重试不会造成连锁反应，以及本章后面会提到的角斗士（dueling proposers）问题。这个协议在一个有利环境下能够保障安全性，同时提供足够的冗余度。
最初的分布式共识问题的解决方案是Lamport的Paxos 协议（参见文献[Lam98]），但是也有其他的协议能够解决这个问题，包括 Raft（参见文献[Ong14]）、Zab（参见文献[Jun11]），以及 Mencius（参见文献[Mao08]）。Paxos本身也有很多变种尝试提升性能（参见文献[Zoo14]）。这些变种常常只是在某一个小细节上不同，例如给某个进程一个特殊的领头人角色以简化协议实现等。

* 提案的严格顺序性解决了系统中的消息排序问题。需要“大多数”参与者同意才能提交提案的要求保证了一个相同的提案无法提交两个不同的值，因为两个“大多数”肯定会至少重合一个节点。当接收者接收某个提案的时候，必须在持久存储上记录一个日志（journal），因为接收者必须在重启之后仍然保持这个状态。


* 很多成功使用分布式共识算法的系统常常是作为该算法实现的服务的一个客户端来使用的，例如ZooKeeper、Consul，以及 etcd。Zookeeper（参见文献[Hun10]）是第一个获得一定行业影响力的开源共识系统，因为它使用起来很方便，特别是对那些没有设计为使用分布式共识的系统来说。Google的Chubby 服务也很类似。Chubby的作者指出（参见文献[Bur06]），通过将共识系统作为一个服务原语提供，而非以类库方式让工程师链接进他们的应用程序，可以使得这些系统不需要为了高可靠共识服务而进行部署上的改变（例如运行正确数量的副本，处理小组成员问题，以及处理性能问题等）。

* 基于其他的非共识算法实现的系统经常简单地依赖于时间戳来决定返回哪些数据。时间戳在分布式系统中问题非常大，因为在多个物理机器上保证时间同步是不可能的。Spanner（参见文献[Cor12]）通过针对最差情况下的不确定性建模，同时在必要时减慢处理速度来解决这个问题。

* 锁（lock）是另外一个很有用的协调性原语，可以用RSM实现。在一个分布式系统中，一些工作进程原子性地操作某些输入文件，同时将产生结果。分布式锁可以保障多个工作进程不会操作同一个输入文件。在实践中，使用可续租约（renewable Lease）而不是无限时间锁是很有必要的，避免某个进程崩溃而导致锁无限期被持有。分布式锁的具体实现超出了本章讨论的范围，但是要记住，分布式锁是一个应该被小心使用的底层系统原语。大多数应用程序应该使用一种更高层的系统来提供分布式事务服务。

* 复合式Paxos（Multi-Paxos）协议采用了一个强势领头人（strong leader）进程的概念：除非目前没有选举出任何的领头人进程，或者发生了某种错误，在正常情况下，提议者只需要对满足法定人数的进程发送一次消息就可以保障系统达成共识。这种强势领头人在很多共识协议中都适用，在系统需要传递大量消息的时候非常合适。


* 组内的另外一个进程可以在任何时间提交消息，并成为一个新的提议者，但是提议者角色的更换会带来性能上的损失。首先系统需要额外的时间重新执行协议的第一阶段部分，更重要的是，更换提议者可能会造成一种“提议者决斗”的状况。在这种状况下，两个提议者不停地打断对方，使得没有一个新的提议可以被成功接收。如图23-8所示，这种场景是一种活跃死锁的场景，可能会无限进行下去。


* 许多系统都是读操作居多的，针对大量读操作的优化是这些系统性能优化的关键。复制数据存储的优势在于数据同时在多个地点可用，也就是说，如果不是所有的读请求都需要强一致性，数据就可以从任意一个副本来读取。这种设计对某些应用程序来说非常好，比如Google的Photon系统（参见文献[Ana13]）。该系统使用一个原子性的“比较”然后“设置”操作进行状态修改（受原子性寄存器的启发）。该操作必须要绝对一致，但是相比而言读操作就可以从任意一个副本进行，因为过期数据仅仅会造成额外工作，而不是错误的结果（参见文献[Gup15]）。这种取舍是非常值得的。

* 法定租约技术针对数据的一部分给系统中的法定人数进程发放了一个租约，这个租约是带有具体时间范围的（通常很短）。在这个法定租约有限期间，任何对该部分数据的操作都必须要被法定租约中的所有进程响应。如果租约中的任何一个副本不可用，那么该部分数据在租约过期前将无法被修改。
法定租约对大量读操作的系统是非常有用的，尤其是当数据的某一部分是被集中在某一个地理区域的进程所读取的时候。

* 很多共识系统使用TCP/IP作为通信协议。TCP/IP是面向连接的，同时针对消息的先进先出（FIFO）顺序提供了强保障。但是在发送任何数据之前，都需要先建立新的TCP/IP连接，也就是一次网络往返需要进行三次握手协议。TCP/IP慢启动机制也限制了连接的初始带宽。常见的TCP/IP窗口大小在4～15KB之间。


* 快速Paxos协议（Fast Paxos，参见文献[Lam06]）是Paxos协议的一个变种，意在优化Paxos算法在广域网络中的性能。使用该协议的每个客户端可以直接向组内的接收者们发送Propose消息，而不需要像经典Paxos和复合Paxos那样通过一个领头人进程发送。这里的思路是用一个从客户端到所有接收者的并发消息来替代经典Paxos协议中的两个消息发送操作：
● 从客户端到提议者的一个消息。
● 从提议者到组内其他成员的一个并发消息。

* 几乎在所有关注性能的分布式共识系统设计中，都采用了单个稳定领头人进程机制，或者是某种领头人轮换机制—预先分配给每个副本一个独立的分布式算法编号（通常是对交易编号取模）。使用轮换机制的算法包括Mencius（参见文献[Mao08]）和Egalitarian Paxos（参见文献[Mor12a0]）。
在广域网络中，在客户端分布在不同的地理区域而共识组分布相对集中的情况下，这种领头人选举机制可以使客户端所见的延迟降低，因为领头人与共识组成员的RTT将会比客户端到各成员的RTT要低

* 一般来说，共识系统都要采用“大多数”的法定过程。也就是说，一组2f+1副本组成的共识组可以同时承受f个副本失败而继续运行（如果需要容忍拜占庭式失败，也就是要能够承受副本返回错误结果的情况，则需要3f+1个副本来承受f个副本失败（参看文献[Cas99]））。针对非拜占庭式失败的情况，最小的副本数量是3—如果仅仅部署两个副本，则不能承受任何一个副本失败）。3个副本可以承受1个副本失败。大部分系统的不可用时间都是由计划内维护造成的（参见文献[Ken12]）:3个副本使该系统可以在1个副本维护时继续工作（这里假设其余两个副本可以承受系统负载）

* 如果共识系统中大部分的副本已经无法访问，以至于无法完成一次法定进程，那么该系统理论上来说已经进入了一个无法恢复的状态。因为至少有一个副本的持久化日志无法访问。在这个状态下，有可能某个操作仅仅只在无法访问的副本上执行了。这时，系统管理员可以强行改变小组成员列表，将新的副本添加到小组中，使用其他可用成员的信息来填充。但是数据丢失的可能性永远存在—这种情况应该全力避免

* 系统性能和仲裁过程中不需要的副本数量有直接关系：系统中的一小部分副本可以落后，从而使得其他参与仲裁过程的副本跑得更快（只要领头人进程性能良好即可）。如果副本之间的性能差距很大，那么系统副本的任何一个失败都会影响性能，因为这时慢的副本也必须要参与仲裁过程。系统能够承受的失败和落后副本的数量越多，那么整体的系统性能就可能越好。

* 一般来说，随着副本之间的距离增加，副本之间的网络往返时间也会增加，但是系统能够承受的失败程度也会增加。对多数共识系统来说，网络往返时间的增加会导致操作延迟的增加。

* 有的时候，不断增加系统可承受的故障域大小并不一定合理。例如，如果共识系统的所有客户端都在某个故障域内（例如，纽约州范围内），虽然在广域范围内部署一个分布式系统可以使得在这个故障域出现故障时继续工作（例如，龙卷风Sandy发生时），但是这是不是值得呢？很有可能不值得，因为该系统的所有客户端都无法工作了，该系统没有实际价值。这种额外的成本，不管是延迟上的、吞吐量上的，还是计算资源上的付出都没有任何回报。

* 我们考虑副本位置时，应该将灾难恢复考虑在内：在某个存储关键数据的系统中，共识系统的副本也就是该系统数据的在线拷贝。然而，当处理关键数据时，即使已经有了可靠的共识系统部署在不同的故障域范围内，我们也必须经常将数据备份在其他地方。因为有两个故障域是我们永远无法逃避的：软件本身和系统管理员的人为错误。软件系统中的Bug可能会在罕见情况下浮现造成数据丢失，而错误的系统配置也可能会造成类似情况。而人工操作可能会执行错误的命令，从而造成数据损坏。

* 这里需要说明的是，向一个采取“大多数”法定仲裁过程系统中增加新的副本可能会降低系统的可用性，如图23-10所示。对ZooKeeper和Chubby来说，一个常见的部署使用5个副本，一个法定仲裁过程需要3个副本参与。整个系统可以在两个副本，也就是40% 不可用的情况下仍然正常工作。当使用6个副本时，仲裁过程需要4个副本：也就是超过33% 的副本不可用就会导致系统无法工作。


* 领头人角色变化的次数
领头人角色的快速变化会影响那些使用稳定领头人角色的共识系统的性能，所以领头人角色变化的次数应该被监控。共识算法通常使用新的租约或者新的视图编号来标记领头人角色的变动，所以这可能是一个比较好的监控指标。领头人角色的快速改变意味着领头人正在快速变换（flapping），可能是由于网络连接问题导致的。而视图编号的降低，可能预示着软件中的一个严重Bug。


### 第24章 分布式周期性任务系统

* anacron 是一个特例。anacron 在系统恢复运行时，会试图运行那些在宕机时间中本来应该运行的程序。但是这种重新运行机制只会对那些每天运行一次或者更短周期的任务起作用。这种功能对运行在工作站和笔记本上的维护性任务来说非常有用。该功能是通过将所有已注册的Cron任务上次运行的时间记录在一个文件中来实现的。

* Cron任务的这种多样性使得讨论系统的可靠性变得很难：针对Cron这样的服务，并没有一个可以适用所有情况的固定答案。一般来说，在系统允许的情况下，我们更倾向于最差情况下跳过某个任务的执行，而不会冒有可能将任务执行两次的风险。这是因为通常情况下，试图修复某个任务没有执行的问题，要比修复任务执行两次造成的问题容易。Cron任务的所有者，可以（并且应该）监控他们自己的Cron任务：例如，某个任务的所有者可以通过Cron服务暴露的状态信息监控自己的任务，也可以建立起独立的、针对Cron任务执行效果的监控。在任务没有执行的情况中，任务所有者可以针对该任务采取合适的行动。与之相比，之前提到的发邮件任务，如果试图从运行两次的情况中恢复经常是很难的，甚至是不可能的。因此，在Cron服务中，我们优先于“失效关闭（fail-close）”，以此来避免系统化地制造困难局面。

* 为了能够判断RPC是否成功发送，下列情况中的一个必须被满足：
● 所有需要在选举过后继续的，对外部系统的操作必须是幂等的（这样我们可以在选举过后重新进行该操作）。
● 必须能够通过查询外部系统状态来无疑义地决定某项操作是否已经成功。

* 一定要小心任何大型分布式系统都熟知的问题：惊群效应（thundering herd）。根据用户的配置，Cron服务可能会造成数据中心用量的大幅增加。当人们想要配置一个“每日任务”时，他们通常会配置该任务在午夜时运行。这个配置可能在一台机器上还可行，但是当你的任务会产生数千个MapReduce工作进程的时候就不可行了。尤其是当其他30个团队也按照同样的配置在同一个数据中心来运行每日任务呢？为了解决这个问题，我们在crontab 格式上增加了一些扩展。


### 第25章 数据处理流水线

* 由于大数据与生俱来的海量级别和处理的高复杂度，这种程序通常会被串联起来执行，一个程序的输出作为另外一个程序的输入。这种编排方式有很多种原因，但是最常见的原因是，这样设计有助于理解系统的逻辑，但是这在系统效率方面并不一定是最优方案。我们将这样构建的程序称为多相流水线（multiphase pipeline），因为整个链条中的每一个程序都是一个独立的数据处理单元。

* 处理大数据的一个关键思想是利用“令人羞愧的并发性”（embarrassingly parallel）算法（参见文献[Mol86]）将巨大的工作集切割为一个个可以装载在单独的物理机器上的小块。有的时候工作分块所需要的处理资源是不等的，而且理解为什么这一块工作需要更多的资源是很困难的。举例来说，在一个按客户分块的工作集中，某些客户的工作分块可能会比其他的都大，由于单个客户已经是系统的最小分块单元，整个数据流水线的运行时间就至少等于处理最大的客户的工作分块所需的时间。

* 这个问题的解决方案是为正常运行提供足够的服务容量。然而，在一个共享的、分布式的环境中资源的获取要依赖供需关系的变化。正如我们所预计的那样，开发团队一般不愿意费力获取资源，再将其贡献给一个公共池中共享。为了解决这个问题，批处理调度资源和生产系统调度资源必须有所区分，以便更好地区分资源的获取成本。

* Workflow协议在每个任务的元数据中嵌入了一个服务器令牌，用来唯一标记某个特定的“主任务”。这样可以避免某个异常的或者是配置错误的“主任务”破坏整个流水线的状态。客户端和服务端在每个操作时都会检查令牌，这样就避免了在配置错误的时候难以查找错误。

* 为了做到自动灾难迁移，图25-6中的一个助手二进制文件（标签为stage 1）会在每个本地Workflow系统中运行。本地Workflow系统除此之外就没有其他的改动了，就像图中“do work”那个盒子中那样。这个助手二进制文件相当于MVC中的控制器角色，负责创建和删除任务引用，同时负责在全球Workflow系统中更新一个特殊的心跳任务。如果心跳任务一段时间没有更新，另外一个远端的Workflow助手任务会将这个目前的任务接管过来，以保障系统正常运行，不受任何环境变化影响。


### 第26章 数据完整性：读写一致

* 如果某个服务的在线时间SLO是99.99%，那么一整年只能宕机1个小时。这个SLO 的标准其实很高，很有可能会超出绝大多数互联网用户，甚至是企业用户的预期。
但是，假设我们针对一个2GB的用户数据提供99.99% 的数据完整性SLO（也就是有200KB会被损坏），不管它们是文档、可执行文件，还是数据库。这么小的损失带来的问题大多数情况下将会是毁灭性的—可执行文件会乱码，数据库会彻底无法加载。

* 大多数云计算应用都是优化以下5项的某种组合：在线时间、延迟、规模、创新速度和隐私。

* 没有人真的想要备份数据，他们只想恢复数据。

* 相对于关注没人愿意做的备份任务来说，我们应该通过关注更重要的（但并不是更简单的）恢复任务来鼓励用户进行备份。备份其实就像纳税一样，是一个服务需要持久付出的代价，来保障其数据的可用性。我们不应该强调应该纳多少税，而是应该强调这些税会用来提供什么服务：数据可用性的保障。因此，我们不会强迫团队进行“备份”，而是要求：
● 各团队需要为不同的失败场景定义一系列数据可用性SLO。
● 各团队需要定期进行演练，以确保他们有能力满足这些SLO。

* 能够提供这种时间点恢复能力，同时能够覆盖该应用所使用的ACID与BASE类型的数据存储，同时还能够满足严格的在线时间、延迟、扩展性、更新速率，以及成本的解决方案就像神话中的怪兽一样，几乎不可能存在。

* 当别人问你“是否有备份”的时候，一个经典的错误回答是“我们有比备份更好的机制—复制机制！”复制机制很重要，包括提高数据的本地性（locality），保护某个部署点单点事故等。但是有很多数据丢失场景是复制机制无法保护的。一个自动同步多个副本的数据存储多半会在你发现问题之前，将损坏的数据记录以及错误的删除动作更新到多个副本上。


* 就算不考虑成本问题，频繁地进行全量备份也是非常昂贵的。最主要的是，这会给线上服务用户的数据存储造成很大的计算压力，以至于影响服务的扩展性和性能。为了缓解这种压力，我们需要在非峰值时间段进行全量备份，同时在繁忙时间段进行增量备份。

* 在不同的层级中转移大量数据是很昂贵的，但是后续层级的数据存储容量并不会与生产系统线上数据容量竞争。所以，后续层级的备份数据通常产生的频率很低，但是可以保存的时间更长。

* 带外数据校验比较难以正确实现。当校验规则太严格的时候，一个简单的合理的修改就会触发校验逻辑而失败。这样一来，工程师就会抛弃数据校验逻辑。如果规则不够严格，那么就可能漏过一些用户可见的数据问题。为了在两者之间取得恰当的平衡，我们应该仅仅校验那些对用户来说具有毁灭性的数据问题。

* 如果数据恢复测试是一个手工的、分阶段进行的操作，那么就不可避免地成为一个讨人嫌的麻烦事。这样会导致测试过程要么不会被认真执行，要么执行得不够频繁以至于作用有限。因此，在任何情况下，都应该追求完全自动化这些测试步骤，并且保证它们能持续运行

* 但是，161,000条中的一小部分是由用户上传的。Google Music团队发出了一条指令，远程遥控这些受影响的用户的Google Music客户端软件自动重新上传3月14日之后的文件。整个过程持续了一周时间，最终全部数据恢复工作结束。

* 大规模部署的、复杂的服务中会产生很多无法完全被理解的Bug。永远不要认为自己对系统已经足够了解，也不要轻易将某些失败场景定性为不可能。我们可以通过“信任仍要验证”、“纵深防御”等手段来保护自己。（注意，这里可不是说让一个初学者来管理上文说的那个数据删除流水线！）

* 不经常使用的系统组件一定会在你最需要的时候出现故障。数据恢复计划必须通过经常性的演习来保障可用性。由于人类天生不适合持续性、重复性地进行测试活动，自动化手段是必备的。然而，如果没有专人负责数据问题，而是让有其他工作的工程师兼职来弄，那么中间一定会出现问题。

* 当能做到在N时间范围内恢复数据之后，我们可以通过更迅速的、更细致的数据丢失检测来降低N，最后的目标一定是将N趋近于0。接下来，我们可以将重点从恢复转为预防，目标一定是保障全部数据随时可用—当你做到了这一点之后，每天去海边沙滩上睡也都不是问题了。


### 第27章 可靠地进行产品的大规模发布

* 1.不可改变的截止日期（Google可不能因为网站没有上线而不过圣诞节）,2.大规模宣传活动，3.数百万的使用人群，4.流量增速非常快（所有人都会在平安夜浏览这个网站）。可不要小看几百万个小孩同时刷新网站—这个项目有潜力能将Google的全部服务搞垮。

* LCE团队的成员会在整个服务生命周期的不同阶段进行审核（audit）工作。大部分审核工作是在新产品或者新服务发布之时进行的。如果某个团队在没有SRE支持的情况下发布产品，LCE会介入提供咨询服务确保发布平稳进行。如果某项产品已经有了很强的SRE支持，LCE团队经常也会在关键功能发布时介入帮助。发布一个新产品所面临的挑战经常和平时稳定运行一个可靠性很强的服务是完全不同的（SRE团队的强项在于后者）。LCE团队因为参与过几百次发布因而经验很丰富，他们可以在服务进行SRE审核的时候提供帮助。


* 经验证明，工程师会绕过那些过于烦琐，或者附加值不高的流程—尤其是在产品上线的压力之下—整个发布流程可能被视为另外一个拦路虎。正是由于这样，LCE必须持续不断地优化整个发布的体验，在成本与收益上保持平衡。


* 在具体实践中，我们可以提出近乎无限的问题，这样会导致该列表无限增长。LCE需要精心挑选合理的问题，确保开发者负担在可控范围之内。为了控制该列表的增长，向列表中增加新的问题需要经过公司副总裁的批准。LCE遵守以下原则：
● 每一个问题的重要性必须非常高，理想情况下，都必须有之前发布的经验教训来证明。
● 每个指令必须非常具体、可行，开发者可以在合理的时间内完成。

* 针对新服务进行系统性的故障模式分析可以确保发布时服务的可靠性。在检查列表的这一部分中，我们可以检查每个组件以及每个组件的依赖组件来确定当它们发生故障时的影响范围。该服务是否能够承受单独物理机故障？单数据中心故障？网络故障？如何应对无效或者恶意输入，是否有针对拒绝服务攻击（DoS）的保护？如果某个依赖组件发生故障，该服务是否能够在降级模式下继续工作？该服务在启动时能否应对某个依赖组件不可用的情况？在运行时能否处理依赖不可用和自动恢复情况？

* 流程与自动化
Google鼓励工程师们使用标准工具来自动化一些常见流程。然而，自动化永远不是完美的，每个服务都有需要人工执行的流程：构建一个新版本，迁移服务到另外一个数据中心中，从备份中恢复数据等。为了保障可靠性，我们应该尽量减少流程中的单点故障源，包括人在内。
这些剩余的流程应该在发布之前文档化，确保在工程师还记得各种细节的时候就完全转移到文档中，这样才能在紧急情况下派上用场。流程文档应该做到能使任何一个团队成员都可以在紧急事故中处理问题。

* 市场宣传部门或PR部门经常会提出他们的要求，使得整个流程更复杂。例如，某个团队可能需要在主题演讲进行时启用某个功能，但需要在主题演讲之前屏蔽掉。
备用方案是发布计划的另外一个方面。如果没有在主题演讲中成功启用这个功能怎么处理？有的时候这些备用方案可能就是简单地准备另外一页幻灯片，“我们会在接下来的几天内发布该功能”，而不是“我们已经发布了这个功能”。


* 系统管理员常说的一句谚语是“永远不要在正在运行的系统上做改动”。任何改动都具有一定的危险性，而任何危险性都应该被最小化，这样才能保障系统的可靠性。在小型系统上测试成功的改变，在Google这种全球分布式、高度复制的系统中很难直接使用。
Google的发布很少是“立即可用”的—在某个具体时间后整个世界都可使用。Google研发了一系列发布模式，用它们逐渐地发布产品和服务可以降低风险（更多信息可参见附录B）。

* “金丝雀”测试是嵌入到很多Google内部自动化工具中的一个核心理念，包括那些修改配置文件的工具。负责安装新软件的工具通常都会对新启动的程序监控一段时间，保证服务没有崩溃或者返回异常。如果在校验期间出现问题，系统会自动回退。

* 最简单的客户端滥用行为是某个更新间隔的设置问题。一个每60s同步一次的新客户端，会比600s同步一次的旧客户端造成10倍的负载。重试逻辑也有一些常见问题会影响到用户触发的行为，或者客户端自动触发的行为。以一个目前处于过载状态的服务为例，该服务由于过载，某些请求会处理失败。如果客户端重试这些失败请求，会对已经过载的服务造成更大负载，于是会造成更多的重试，更多的负载。客户端这时应该降低重试的频率，一般需要增加指数型增长的重试延迟，同时仔细考虑哪些错误值得重试。例如，网络错误通常值得重试，但是4xx HTTP错误（这一般意味着客户端侧请求有问题）一般不应该重试。

* 在简化的模型中，假设物理机的CPU用量与某个服务的负载线性相关（例如，请求的数量或者处理的数据量等），一旦可用CPU耗尽，处理过程就会变慢。但是不幸的是，真实的服务很少按照理想模式运转。大部分服务在没有负载的情况下都会变慢，一般是由于各种各样的缓存因素，例如CPU缓存、JIT缓存，以及其他的数据缓存等。随着负载上升，经常有一段时间CPU用量和负载线性同步增长，响应时间基本保持固定。


* 到达某一点后，很多服务在过载之前会进入一个非线性转折点。在最简单的案例中，响应时间会上升，造成用户体验下降，但是不一定会造成故障（然而依赖服务的速度变慢可能会造成RPC超时，而造成某个用户可见的错误）。在极端情况下，服务会在过载情况下进入完全死锁状态。

* 如果底层基础设施（例如集群管理系统、存储、监控、负载均衡、数据传输）被开发团队不断修改，运行在之上的各服务的负责人需要投入大量精力不停地跟上这些改动的节奏。随着某个基础设施功能的废弃和被某个新功能所替换，服务负责人必须要不停地修改配置文件，或者重新构建可执行文件，也就是“逆水行舟，原地不动”。这个问题的解决方案是指定和执行某种策略，禁止基础设施工程师发布非向后兼容的功能改变，除非他们可以自动化地将客户端迁移到新功能上。在制作新功能的时候就创建这种自动化迁移工具可以减少服务负责人的迁移压力。

* [27]奥卡姆剃刀原理（Occam's Razor），可参见https://en.wikipedia.org/wiki/Occam%27s_razor。但是需要注意的是，系统中可能同时存在多个问题，尤其是有的时候是因为系统中存在一系列低危害性问题，联合起来，才可以解释系统目前的状态。而不是系统中存在一个非常罕见的问题，同时造成了所有的问题现象，参见https://en.wikipedia.org/wiki/Hickam%27s_dictum。

* [40]虽然带着一个未能确认的Bug上线并不是理想情况，很多时候消除所有已知Bug是不可行的。有的时候，我们只能依靠良好的工程师判断力来尽力消除风险。


* [46]如果你想收集事后总结，Etsy发布了Morgue（http://github.com/etsy/morgue）用来管理事后总结。


* [67]不是因为软件开发工程师不写这类工具，而是这类工具通常需要纵越不同的技术领域，以及同时横跨多个抽象层，这种类型的工具的开发团队经验比较少，系统管理员团队较多。

* [83]Jeff Dean的讲座“构建大型分布式系统的软件工程建议”是一个非常好的资源（参见文献[Dea07]）。


* [98]在实践中，编写代码对大部分SRE来说并不是什么难事，因为大部分SRE都是很有经验的软件开发者。这种要求也使得SRE非常难以招聘。从这个案例以及其他案例中可以了解为什么SRE坚持需要招聘熟练的软件开发工程师（参见文献[Jon15]）

* [99]在我们的经验里，云计算工程师经常拒绝针对生产环境中的数据删除设置报警，因为删除速度随着时间波动很大。然而，这种报警的关注点其实应该在全局范围，而非局部。比起针对每个用户的删除速度的报警，更有用的报警是全局汇总过后的删除速度，阈值可以是一个比较极端的数字（例如10倍于观察到的95% 值），这样的报警规则有利于检测到极端情况。


### 第28章 迅速培养SRE加入on-call

* 可以考虑在服务访问权限控制配置中实现一种分层模式。第一层访问权限允许学员只读访问组件的内部信息。接下来则允许修改生产环境状态。随着检查列表项目的完成，学员将会逐步拥有系统的更高权限。搜索SRE团队将这称为“升级”，[5]所有的学员最后都会被赋予系统最高权限。

* SRE是天生的问题解决者，所以我们应该给予他们一个难题去解决！ 在学习过程中，允许新成员向整个服务中加入一点点新东西是很有激励性的。这也是鼓励团队之间构建信任的好方法，因为老员工需要和新员工进行交流，了解新的组件或者新的流程。在Google内的标准做法是：所有的工程师都会被分配一个初始项目，给他们提供足够的基础设施知识，同时让他们可以为服务做一点小的但是有意义的贡献。让新的SRE成员将时间同时分配在学习和项目工作中可以给他们一种参与感和效率感，这比让他们专攻两者中的任意一个要好。

* 我们可以将SRE处理大规模系统紧急情况的方式理解为他们在实时展开的决策树中不断抉择的过程。在紧急情况处理的有限时间中，SRE只能在几百个可能的选择中选择几个动作去执行，缓解故障。因为时间通常是有限的，SRE必须能够有效地在决策树中进行抉择。这种能力的获得通常要靠经验积累，而经验只能通过有效和真实的实践获得。这种经验必须同时和一系列精心构造的假设结合，当这些假设被证实，或者证伪时，可以更进一步地减少决策空间。用另外一种说法，进行系统故障调试的过程很像是一种游戏：“下列哪一个东西与其他不同”？这里的“东西”可能是内核版本、CPU架构、二进制文件版本、地区性的流量区别，或者其他几百种因素。从架构层面来讲，该运维团队必须要保证这些因子是可控的，每个因子都应该是进行独立分析和比较过的。然而，我们也应该培训SRE新员工从一入职就成为好的数据分析者。

* “新SRE需要学习Google Maps生产服务的一部分时，与其让其他人传授知识给他，不如自己动手_利用反向工程手段自己了解服务，让其他人纠正他的错误和补充遗漏的部分。”这样做的结果是——这可能比我亲自授课效果还要好，虽然我已经为这个服务on-call超过5年了！”

* 在课程末尾，会给每人分配一项任务。每个学员需要和自己团队的资深SRE共同选择未来on-call系统的一部分。利用课程中学到的知识，该学员需要自行构建出整个技术栈的组件图，同时与资深SRE共同讨论。学员肯定会在这个过程中忽略一些小的但是很关键的细节问题，这些恰恰是最好的讨论话题。资深SRE在这个过程中很有可能也会学到一些知识，因为系统不断变化，任何人都可能出现理解上的偏差。整个SRE团队都应该抓住一切机会学习系统的新变化，而最好的方式恰恰是与团队中最新的成员，而不是最资深的成员交流。

* 如果某个较大的故障发生，书写事后总结比较必要时，on-call成员应该将学员加为共同作者。不要将整个书写过程全部交给学员，因为这可能会导致他误认为事后总结是一项琐事，专门交给新手来完成的：千万不要造成这种印象。


### 第29章 处理中断性任务

* 任何复杂系统都和创造这个系统的人一样，都不是完美的。在管理这些系统造成的运维负载时，一定要记住这一点。


* 从某种意义上讲，人类可以被称为不完美的机器。人会感觉无聊，人的处理器（指思维方式）工作原理不清楚，用户界面（指沟通方式）也不太友好，效率也不高。在工作安排中，能够认识到人的这些缺陷，取长补短是最好的。

* 主on-call工程师应该专注于on-call工作。如果目前紧急警报较少，那么一些可以随时放下的工单，或者其他中断性事务应该由on-call人员处理。当某个工程师on-call一周时，这一周他应该完全排除在项目进度之外。如果某个项目非常重要，不能等待一周，那么这个人就不应该参与on-call。管理层应该介入，安排其他人替代on-call。管理层不应该期望员工在on-call的同时还能在项目上有所进展（或者其他高上下文切换成本的活动）。

* 如果目前你是随机分配工单给团队成员，请立刻停止。这样做对团队的时间非常不尊重，和让成员尽可能不被打扰的目标背道而驰。工单处理应该由全职人员负责，同时保证占用合理的时间。如果团队目前的工单主oncall和副on-call都处理不完，那么需要重新架构整个工单的处理流程，保障任何时间都有两个全职员工处理工单。不要将复杂分散到整个团队中去。人不是机器，这样做只会干扰员工，降低他们的工作效率。

* 有的时候，某个中断性任务只有某个目前不在值班的成员能够妥善处理。虽然理想情况下，这种情况不应该发生，但是还是偶尔会发生，我们应该尽一切努力避免这种情况。有的时候员工在非值班时间也会处理工单，因为这样是显得很忙的好办法。这种行为是错误的。这意味着该成员效率不高，他们会影响对于工单数量的统计。如果某个人负责处理工单，但是其他两三个人也来参与，可能管理层就没法知道到底目前的工单数量水平是否可行。


### 第30章 通过嵌入SRE的方式帮助团队从运维过载中恢复

* Google的SRE团队的标准政策要求团队在项目研发和被动式工作之间均匀分配时间。在实际工作中，这种平衡可能会被每天工单数量的不断增加而被打乱，甚至持续数月之久。长时间承担非常繁重的Ops类型的工作是非常危险的，团队成员可能会劳累过度，不能在项目工作上取得任何进展。当一个团队被迫花费过多时间解决工单的时候，他们就会减少花在改进服务上的时间，服务的可扩展性和可靠性肯定会随之变差。解决这个问题的一种方式是给处于过载状态的团队临时调入一个SRE。调入该团队后，该SRE应该关注于改善这个团队的行事方式，而不是简单地帮助团队清理积压的工单。通过观察团队的日常工作，提出改善性意见，该SRE可以帮助团队用全新的视角来审视自己的日常工作。这往往是团队本身的成员做不到的。

* 在嵌入团队的过程中，你的主要工作是清楚地表达团队目前的流程和工作习惯对于该服务的可扩展性有利或者有弊的原因。你应该提醒该团队，日益增加的工单不应需要更多的SRE来处理。SRE模型的目标是仅仅在系统复杂度上升的时候才增加新人。你应该尝试引导团队建立健康的工作习惯，这样能够减少花费在工单上的时间。这与指出该服务目前还可以自动化或者进一步简化同样重要。

* 如果该服务中最主要的组件的商业价值很高但是部署规模实际很小（只需要很少的资源或者并不复杂），那么我们应该关注于改进团队现在的工作方式，找到那些阻碍他们改善服务可靠性的部分。时刻记住，你的工作是保证服务正常运转，而不是避免警报发生。

* 如同前文提到的那样，你可能会遇到“为什么是我？”这样的回复。这种回复恰恰说明了该团队相信事后总结过程是报复性的。这一态度来源于对坏苹果理论的认同：整个系统工作良好，如果我们摆脱所有的坏苹果以及它们所犯的错误，系统将会一直运行良好。坏苹果理论已经被实践证明是错误的，在很多行业中，包括那些对安全极为重视的航空行业中，都有明确的证据来证明，参见文献[Dek14]。你应该向团队成员指出这个理论的错误之处。说服其他工程师书写事后总结最有效的措辞是：“在任何一个交互关系错综复杂的系统中，错误是不可避免的。我相信，在当时你利用了最正确的信息做出了最正确的决定。我想要你写下在这段时间里每一个时间点你所想的事情，这样我们就能发现系统在哪里误导了你，以及过程中什么地方对人的认知能力要求过高。”

* 应该提出引导性问题，而非指责性的问题。当你和SRE团队交流时，试着用一种可以鼓励别人思考基本理念的方式来提出问题。这对于你建立这种模型而言是格外有价值的，因为，按照定义，处于Ops模型中的团队会天生拒绝这种逻辑推理。当你花费一定时间解释了对于不同政策问题的理论推理之后，其实就使得该团队更深刻地理解了SRE的哲学。


### 第31章 SRE与其他团队的沟通与协作

* 生产会议通常每周进行一次；鉴于SRE对于毫无意义的会议十分反感，这个频率似乎是合适的：有足够时间积累足够的素材使得会议有价值，同时又不会太频繁而总让人们找借口不参加。每次会议通常持续30到60分钟。如果会议过短，意味着某些东西没有充分讨论，或者意味着是服务内容太少。如果会议过长则意味着你可能陷入了细节讨论之中，又或者是待讨论的东西太多，应该按服务或者团队进一步拆分会议。

* 在两个SRE团队视频会议时，如果一个团队比另外一个大很多，我们建议从较小的团队一边选择主席。更大的一方会自然而然地安静下来，一些不平衡的团队规模所造成的不良影响（这些会由于视频会议的延迟而变得更糟）将得到改善。[14]我们不知道这是否有任何科学依据，但它确实有效。

* 这种分布式的模式还和SRE的团队是如何组织的类似。SRE团队最主要的目标是通过掌握先进技术来创造价值，而先进技术的掌握往往是困难的，因此我们试图掌握相关系统或基础设施的一部分子集以降低认知的难度。专业化是实现这一目标的一种方法，比如，团队X只会为产品Y服务。专业化有很多优势，因为它能够大幅提高技术熟练度。但同时专业化也是有弊端的，因为它会导致局部化，忽视大局。我们要有一个清晰的团队章程来定义一个团队将要做什么—更重要的是不会做什么—但这并不总是那么容易。

* SRE团队的成员有着各种各样的技能，从系统工程到软件工程，以及组织能力和管理能力都有。我们唯一可以确定的是：成功的协作需要团队内充分的多样性。有很多证据表明，多样化强的团队在各方面都更强（参见文献[Nel14]），组建这样一个多样化的团队意味着需要特别注意沟通，防止认知偏差的出现，我们就不在这里详细介绍了。正式地讲，SRE团队中有着“技术负责人”（TL）、“SRE经理”（SRM）和“项目经理”（也被称为PM、TPM、PGM）的角色。有的成员希望这些角色的责任被明确定义：因为他们据此可以迅速和安全地做出范围内的决策。另外一些成员则希望在一个更为动态的环境中工作，这样他们可以随时协商切换责任。普遍看来，越是动态的团队，团队成员的个人能力越强，整个团队适应新的情况的能力也越强。但是这样的团队需要在沟通上花费更多时间，因为什么东西都不太确定。

* SRE采用很多方法提高工作效率。一般来说，单人项目最终肯定会失败，除非此人个人能力超强或者待解决的问题是非常简单直接的。做成任何高价值的事情都需要很多人共同协作。正因为这样，SRE团队需要良好的协作能力。这里再提一下，有很多阅读材料都讨论了团队协作，这类资料的大部分也都适用于SRE。一般来说，在做本地团队边界之外的工作时要想成功就一定需要良好的沟通技巧。而与异地团队合作，或者与跨越时区的团队一起工作则需要极好的书面沟通能力或者是大量的旅行，这样才能建立人与人之间的高质量关系。虽然有的时候这种旅行可以推迟进行，但是最终来看仍是必要的。书面沟通能力再强，随着时间的推移，人们对你的印象也会逐渐淡薄成一个电子邮件地址，直到你再次出现进行直接沟通。

* 最终看来，对未来的一个共同愿景是项目合并成功的关键因素。这两个团队在开发过程中都发现了自己的价值，并且从对方的贡献中受益。这种势头一直保持到2014年年底，Viceroy被正式宣布为适用于所有SRE团队的通用监控解决方案。这个声明带有Google的一贯特色，它没有“要求”团队采用Viceroy：相反，它“建议”团队采用Viceroy而非另行开发监控台程序。

* 随着人们不停地加入和离开团队，我们发现，这种短期的贡献是有用的但也是昂贵的。其主要成本是所有权的稀释问题：一旦某个功能做完了，维护者离开团队之后，这个功能会随着时间的推移出现各种问题，最终一般会被丢弃。

* 动力十足的贡献者是有价值的，但不是所有的贡献成果都是同样宝贵的。确保项目的贡献者会实际投入时间，而不只是为了一些朦胧的自我实现的目标而加入（例如想要在一个闪亮的项目上带上自己的名字；想要在一个新的令人兴奋的项目上编码，却不承诺未来的维护）。有着特定目标的贡献者通常会更有动力，能够更好地维护他们的贡献。

* 显然，一般的软件工程最佳实践也适用于协作项目：每一个组件都应该有设计文档并应该在团队内部评审。这样，团队中的每个人都有机会及时了解变化，也有机会参与影响和改进设计。文档化是抵消物理和/或逻辑距离的主要技术之一，一定要多用。


### 第32章 SRE参与模式的演进历程

* 图32-1：一个典型的服务生命周期

* 检查列表中的项目可能包括：● 对于服务进行的更新是否立刻影响到系统中大比例的部分？是否合理？● 服务的依赖是否合理？例如，直接面向用户请求的服务不应该依赖于一个主要用于批处理的系统。● 该服务在连接关键的远程服务时，是否需要较高的网络服务质量（QoS）？● 该服务是否会将错误报告传送到中央日志记录系统进行分析？该服务是否报告所有的会造成降级回复，或者会造成最终用户请求失败的特殊情况？● 所有用户可见的请求失败情况都有度量和监控吗？报警策略是否合适？

* 一开始，莎士比亚搜索服务的研发人员对产品直接负责，包括对紧急事件进行响应。然而，随着服务用量的上升和服务收入的增长，该服务开始需要SRE的支持。由于该产品已经上线，所以SRE启动了PRR评审流程。评审中发现的一个问题是该服务的监控台页面漏掉了一些SLO指标，需要修复。在全部问题都被修复以后，SRE接过了该服务的on-call职责，但是同时还有两个开发者参与运维轮值。开发者同时也会参与每周一次的on-call会议，讨论上周遇到的问题以及如何处理即将到来的大规模维护活动或者集群离线情况。同时，有关服务的未来计划也会与SRE充分讨论，这样确保未来新的发布更平稳（然而墨菲定律总是寻找机会搞破坏）。

* SRE还可以帮助实现广泛使用的发布模式和控制系统。例如，SRE可以帮助实现一种“隐形发布”机制，将现有用户流量的一部分复制发送给新服务，不影响生产服务。新服务中的响应是“隐形”的，因为它们会被直接抛弃而不会实际显示给用户。这种“隐形发布”的做法可以让团队获得运维知识，在不影响现有用户的情况下解决问题，并减少在发布后遇到问题的风险。发布过程中的平稳有助于保持运维负载很低，同时可以维持在发布之后的发展势头。发布过程中出现的问题会很容易地导致对于源代码和生产环境的紧急更改，这会打乱研发团队对于未来功能的计划。

* 通用的控制接口使自动化和智能化达到一个以前不可能达到的水平。例如，SRE可以用一个统一的视图查看关于一次故障的全部相关信息，不用收集和分析来自不同数据源的原始数据（日志、监控数据，等等）。基于这些指导思想，在我们支持的每一个环境中（Java、C++、Go），建立了一系列SRE支持的平台和服务框架。利用这些框架构建的服务可以共享那些按SRE支持平台设计的代码，SRE和开发团队共同维护这些框架。这种变革使得产品研发团队可以利用符合SRE标准的框架来设计应用程序，然后再花时间按SRE规范改造程序。

* 最初的SRE参与模型体现了两种选项：完整的SRE支持，或者基本上没有SRE支持。[19]一个有着公共服务结构、惯例，以及软件基础设施的生产平台，使SRE团队可以为“平台”基础设施提供支持，而让研发团队为服务的功能性问题提供on-call支持—也就是应用程序代码中的错误。在这种模式下，SRE承担大部分基础设施服务的软件开发和维护的责任，特别是控制系统，如负载抛弃、过载保护、自动化、流量管理、日志和监控等。该模型体现了一个从最初以两种主要方法为主的服务管理方式的重大变革：它需要一个SRE与研发团队之间的新的关系模型，以及一个对于SRE所支持的服务进行管理的新人员配置模型。

* [20]新的服务管理模型以两种方式改变了SRE人员配置模型：（1）因为大量的服务采用通用技术，每个服务所需要的SRE人数也随之减少；（2）它促成了生产平台的建立，使得关注重点划分为由SRE支持的生产平台和由开发团队支持的服务的具体业务逻辑。这些平台团队根据所维护平台的需要而配置人员，而非基于所服务的数量来配置，同时可以在多个产品之间共享资源。


### 第33章 其他行业的实践经验

* 在任何事故和业务灾难发生之前，类似事故都曾经发生过好几次，只是没有造成任何后果。这些事故在发生的时候都被忽略了。潜伏的错误，加上某个适合的时机，就会导致事故的发生。

* 救生员行业也有一个根深蒂固的事后分析与计划文化。Mike Doherty 讽刺道：“如果救生员的脚踏入了水里，后续就要有很多报告要写。”在游泳池或者沙滩上出现的任何事故都需要一个详细的事后总结。

* 数据驱动决策要优于情感驱动的决策、直觉驱动的决策，以及资深人士的意见。

* 最终，某些行业，例如交易行业，将决策划分成更小的块来更好地管理风险。根据John Li的经验，交易行业有一个专门的风险控制部门，独立于交易部门之外，负责确保不会在追求利益的同时承受不必要的风险。这个风险控制部门负责监控交易场所内部，同时在交易发生意外的时候终止。如果某个系统发生异常情况，风险控制部门的第一反应是关掉这个系统。John Li提到：“如果我们不进行交易，那么我们就不会亏钱。虽然我们也没有赚钱，但是起码不会亏钱。”只有风险控制部门可以将系统重新启用，不论交易员是否错失了某种可以赚钱的机会。


### 附录B 生产环境运维过程中的最佳实践

* 配置文件，或者服务的任何输入都应该经过正确性和准确性检查再提供给服务。服务在接收到不合理的配置文件或者输入数据时，应该继续保持之前的状态正常工作，同时发出错误输入的警报。

* 服务在过载情况下应该仍然提供合理程度上的次优结果。例如，Google 搜索服务会在过载情况下只搜索一部分索引数据，同时中止某些即时提示功能等，从而能够提供足够优质的结果数据。搜索SRE会对集群进行超出集群容量的测试，确保它们可以在过载情况下正常工作。
当负载高到一定程度时，次优结果也不够的情况下，服务应该采用队列、动态超时等手段进行优雅地流量抛弃，参见第21章。其他的技术包括：将回复延迟发送，或者指定某些用户接受错误，从而保障其他客户的服务质量等。

* 重试可以将低错误率转化为高流量，从而导致连锁故障（参见第22章）。一旦负载超过容量，就通过丢弃一部分系统上游的流量（包括重试）来应对连锁故障。
每个发送RPC的客户端都应该实现指数型延迟重试（包括抖动），这样可以降低错误放大率。移动客户端问题尤其严重，因为可能有几百万个客户端，修复他们代码中的问题需要花费很多时间，甚至数周，还需要用户安装更新。

* 每次on-call轮值应该处理不超过两起事故（平均每12小时1个）：应对和修复事故都需要时间，书写事后总结以及处理Bug也需要时间。事故频繁会造成响应质量的下降，同时也意味着系统设计中存在一个或多个设计缺陷，监控系统过于敏感，之前书写在事后总结中的问题没有及时修复等。
具有讽刺意味的是，如果我们应用了这些最佳实践，SRE团队最终可能会由于系统事故出现的越来越少而失去熟练性，这可能会造成原本很小的事故需要花很长的时间来解决。通过进行周期性的假想灾难演习可熟悉应对紧急事故的文档与流程。

